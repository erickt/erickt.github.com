<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Rust | Tilting at Rabbit Holes]]></title>
  <link href="http://erickt.github.io/blog/categories/rust/atom.xml" rel="self"/>
  <link href="http://erickt.github.io/"/>
  <updated>2014-12-20T19:05:03-08:00</updated>
  <id>http://erickt.github.io/</id>
  <author>
    <name><![CDATA[Erick Tryzelaar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 3.1: Another Performance Digression]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/performance-digression/"/>
    <updated>2014-12-13T18:35:08-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/performance-digression</id>
    <content type="html"><![CDATA[<p>Wow, home stretch! Here&rsquo;s the rest of the series if you want to catch up:
<a href="http://erickt.github.io/blog/2014/10/28/serialization/">part 1</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.1</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.2</a>, and
<a href="http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization/">part 3</a>.</p>

<p>Overall <code>serde</code>&rsquo;s approach for serialization works out pretty well. One thing I
forgot to include in the last post was that I also have two benchmarks that are
not using <code>serde</code>, but are just safely reading and writing values.  Assuming I
haven&rsquo;t missed anything, they should be the upper limit in performance we can
get out of any serialization framework: Here&rsquo;s
<a href="https://github.com/erickt/rust-serde/blob/master/benches/bench_log.rs#L1021">serialization</a>:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library                        </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max without string escapes</strong> </td>
<td> <strong>353</strong>              </td>
</tr>
<tr>
<td> c++      </td>
<td> rapidjson                      </td>
<td> 304                  </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with string escape</strong>     </td>
<td> <strong>234</strong>              </td>
</tr>
<tr>
<td> rust     </td>
<td> serde::json                    </td>
<td> 201                  </td>
</tr>
<tr>
<td> rust     </td>
<td> serialize::json                </td>
<td> 147                  </td>
</tr>
<tr>
<td> go       </td>
<td> ffjson                         </td>
<td> 147                  </td>
</tr>
</tbody>
</table>


<p>So beyond optimizing string escaping, <code>serde::json</code> is only 14% slower than the
zero-cost version and 34% slower than <code>rapidjson</code>.</p>

<p><a href="https://github.com/erickt/rust-serde/blob/master/benches/bench_log.rs#L1613">Deserialization</a>,
on the other hand, still has a ways to go:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library                         </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> rust     </td>
<td> rapidjson (SAX)                 </td>
<td> 189                    </td>
</tr>
<tr>
<td> c++      </td>
<td> rapidjson (DOM)                 </td>
<td> 162                    </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with Iterator&lt;u8&gt;</strong> </td>
<td> <strong>152</strong>                </td>
</tr>
<tr>
<td> go       </td>
<td> ffjson                          </td>
<td> 95                     </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with Reader</strong>             </td>
<td> <strong>78</strong>                 </td>
</tr>
<tr>
<td> rust     </td>
<td> serde::json                     </td>
<td> 73                     </td>
</tr>
<tr>
<td> rust     </td>
<td> serialize::json                 </td>
<td> 24                     </td>
</tr>
</tbody>
</table>


<p>There are a couple interesting things here:</p>

<p>First, <code>serde::json</code> is built upon consuming from an <code>Iterator&lt;u8&gt;</code>, so we&rsquo;re
48% slower than our theoretical max, and 58% slower than <code>rapidjson</code>. It looks
like tagged tokens, while faster than the closures in <code>libserialize</code>, are still
pretty expensive.</p>

<p>Second, <code>ffjson</code> is beating us and they compile dramatically faster too. The
<a href="https://github.com/cloudflare/goser">goser</a> test suite takes about 0.54
seconds to compile, whereas mine takes about 30 seconds at <code>--opt-level=3</code>
(!!). Rust itself is only taking 1.5 seconds, the rest is spent in LLVM. With
no optimization, it compiles &ldquo;only&rdquo; in 5.6 seconds, and is 96% slower.</p>

<p>Third, <code>Reader</code> is a surprisingly expensive trait when dealing with a format
like JSON that need to read a byte at a time. It turns out we&rsquo;re not
<a href="https://github.com/rust-lang/rust/issues/19864">generating great code</a> for
types with padding. aatch has been working on fixing this though.</p>

<hr />

<p>Since I wrote that last section, I did a little more experimentation to try to
figure out why our serialization upper bound is 23% slower than rapidjson. And,
well, maybe I found it?</p>

<table>
<thead>
<tr>
<th>                                </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> serde::json with a MyMemWriter </td>
<td> 346                  </td>
</tr>
<tr>
<td> serde::json with a Vec<u8>     </td>
<td> 247                  </td>
</tr>
</tbody>
</table>


<p>All I did with <code>MyMemWriter</code> is copy the <code>Vec::&lt;u8&gt;</code> implementation of <code>Writer</code>
into the local codebase:</p>

<pre><code class="rust">struct MyMemWriter0 {
    buf: Vec&lt;u8&gt;,
}

impl MyMemWriter0 {
    pub fn with_capacity(cap: uint) -&gt; MyMemWriter0 {
        MyMemWriter0 {
            buf: Vec::with_capacity(cap)
        }
    }
}

impl Writer for MyMemWriter0 {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; io::IoResult&lt;()&gt; {
        self.buf.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_serializer_my_mem_writer0(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut wr = MyMemWriter0::with_capacity(1024);

    b.iter(|| {
        wr.buf.clear();

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}

#[bench]
fn bench_serializer_vec(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut wr = Vec::with_capacity(1024);

    b.iter(|| {
        wr.clear();

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}
</code></pre>

<p>Somehow it&rsquo;s not enough to just mark <code>Vec::write</code> as
<code>#[inline]</code>, having it in the same file gave LLVM enough information to
optimize it&rsquo;s overhead away. Even using <code>#[inline(always)]</code> on <code>Vec::write</code> and
<code>Vec::push_all</code> isn&rsquo;t able to get the same increase, so I&rsquo;m not sure how to
replicate this in the general case.</p>

<p>Also interesting is <code>bench_serializer_slice</code>, which uses <code>BufWriter</code>.</p>

<table>
<thead>
<tr>
<th>                                </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> serde::json with a BufWriter   </td>
<td> 342                  </td>
</tr>
</tbody>
</table>


<pre><code class="rust">#[bench]
fn bench_serializer_slice(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut buf = [0, .. 1024];

    b.iter(|| {
        for item in buf.iter_mut(){ *item = 0; }
        let mut wr = std::io::BufWriter::new(&amp;mut buf);

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}
</code></pre>

<hr />

<p>Another digression. Since I wrote the above, aatch has put out some PRs that
should help speed up enums.
<a href="https://github.com/rust-lang/rust/pull/19898">19898</a> and
<a href="https://github.com/rust-lang/rust/pull/20060">#20060</a> and was able to optimize
the padding out of enums and fix an issue with returns generating bad code. In
my <a href="https://github.com/rust-lang/rust/issues/19864">bug from earlier</a>
his patches were able to speed up my benchmark returning an
<code>Result&lt;(), IoError&gt;</code> from running at 40MB/s to 88MB/s. However, if we&rsquo;re able
to reduce <code>IoError</code> down to a word, we get the performance up to 730MB/s! We
also might get enum compression, so a type like <code>Result&lt;(), IoError&gt;</code> then
would speed up to 1200MB/s! I think going in this direction is going to really
help speed things up.</p>

<hr />

<p>That was taking a while, so until next time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 3: Introducing Serde]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization/"/>
    <updated>2014-12-13T14:40:18-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization</id>
    <content type="html"><![CDATA[<p>There&rsquo;s been a long digression over the past month
(<a href="http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/">possible kernel bugs</a>,
<a href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/">benchmarking Writers</a>,
and
<a href="https://github.com/rust-lang/rust/pull/19574">don&rsquo;t believe in magic, folks</a>), but I&rsquo;m back
into serialization. Woo! Here&rsquo;s
<a href="http://erickt.github.io/blog/2014/10/28/serialization/">part 1</a> and
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2</a>), Rust&rsquo;s
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.1</a>), Rust&rsquo;s
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.2</a>) if you need
to catch up.</p>

<p>So <code>libserialize</code> has some pretty serious downsides. It&rsquo;s slow, it&rsquo;s got this
weird recursive closure thing going on, and it can&rsquo;t even represent enum types
like a <code>serialize::json::Json</code>. We need a new solution, and while I was at it,
we ended up with two: <a href="https://github.com/erickt/rust-serde">serde</a> and
<a href="https://github.com/erickt/rust-serde/tree/master/serde2">serde2</a>. Both are
different approaches to trying to address these problems. The biggest one being
the type representation problem.</p>

<h2>Serde Version 1</h2>

<h3>Deserialization</h3>

<p>I want to start with deserialization first, as that&rsquo;s really the interesting
bit. To repeat myself a little bit from
<a href="https://erickt.github.io/blog/2014/10/28/serialization/">part 1</a>,
here is a generic json <code>Value</code> enum:</p>

<pre><code class="rust">pub enum Value {
    I64(i64),
    U64(u64),
    F64(f64),
    String(String),
    Boolean(bool),
    Array(Vec&lt;Value&gt;),
    Object(TreeMap&lt;String, Value&gt;),
    Null,
}
</code></pre>

<p>To deserialize a string like <code>[1, true]</code> into
<code>Array(vec![I64(1), Boolean(true)])</code>, we need to peek at one character ahead
(ignoring whitespace) in order to discover what is the type of the next value.
We then can use that knowledge to pick the right variant, and parse the next
value correctly. While I haven&rsquo;t formally studied this stuff, I believe this
can be more formally stated as <code>Value</code> requires at least a LL(1) grammar,
but since <code>libserialize</code> supports no lookahead, so at most it can handle LL(0)
grammars.</p>

<p>Since I was thinking of this problem in terms of grammars, I wanted to take a
page out of their book and implement generic deserialization in this style.
<code>serde::de::Deserializer</code>s are then an <code>Iterator&lt;serde::de::Token&gt;</code> lexer that
produces a token stream, and <code>serde::de::Deserialize</code>s are a parser that
consumes this stream to produce a value. Here&rsquo;s <code>serde::de::Token</code>, which can
represent nearly all the rust types:</p>

<pre><code class="rust">pub enum Token {
    Null,
    Bool(bool),
    Int(int),
    I8(i8),
    I16(i16),
    I32(i32),
    I64(i64),
    Uint(uint),
    U8(u8),
    U16(u16),
    U32(u32),
    U64(u64),
    F32(f32),
    F64(f64),
    Char(char),
    Str(&amp;'static str),
    String(String),

    Option(bool),     // true if the option has a value

    TupleStart(uint), // estimate of the number of values

    StructStart(
        &amp;'static str, // the struct name
        uint,         // estimate of the number of (string, value) pairs
    ),

    EnumStart(
        &amp;'static str, // the enum name
        &amp;'static str, // the variant name
        uint          // estimate of the number of values
    ),

    SeqStart(uint), // number of values

    MapStart(uint), // number of (value, value) pairs

    End,
}
</code></pre>

<p>The <code>serde::de::Deserialize</code> stream must generate tokens that follow this
grammar:</p>

<pre><code class="antlr">value ::= Null
        | Bool
        | Int
        | ...
        | option
        | tuple
        | struct
        | enum
        | sequence
        | map
        ;

option ::= Option value
         | Option
         ;

tuple := TupleStart value* End;

struct := StructStart (Str value)* End;

enum := EnumStart value* End;

sequence := SeqStart value* End;

map := MapStart (value value)* End;
</code></pre>

<p>For performance reasons, there is no separator in the compound grammar.</p>

<p>Finishing up this section are the actual traits, <code>Deserialize</code> and <code>Deserializer</code>:</p>

<pre><code class="rust">pub trait Deserialize&lt;D: Deserializer&lt;E&gt;, E&gt; {
    fn deserialize(d: &amp;mut D) -&gt; Result&lt;Self, E&gt; {
        let token = try!(d.expect_token());
        Deserialize::deserialize_token(d, token)
    }

    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Self, E&gt;;
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    /// Called when a `Deserialize` expected more tokens, but the
    /// `Deserializer` was empty.
    fn end_of_stream_error(&amp;mut self) -&gt; E;

    /// Called when a `Deserializer` was unable to properly parse the stream.
    fn syntax_error(&amp;mut self, token: Token, expected: &amp;'static [TokenKind]) -&gt; E;

    /// Called when a named structure or enum got a name that it didn't expect.
    fn unexpected_name_error(&amp;mut self, token: Token) -&gt; E;

    /// Called when a value was unable to be coerced into another value.
    fn conversion_error(&amp;mut self, token: Token) -&gt; E;

    /// Called when a `Deserialize` structure did not deserialize a field
    /// named `field`.
    fn missing_field&lt;
        T: Deserialize&lt;Self, E&gt;
    &gt;(&amp;mut self, field: &amp;'static str) -&gt; Result&lt;T, E&gt;;

    /// Called when a `Deserialize` has decided to not consume this token.
    fn ignore_field(&amp;mut self, _token: Token) -&gt; Result&lt;(), E&gt; {
        let _: IgnoreTokens = try!(Deserialize::deserialize(self));
        Ok(())
    }

    #[inline]
    fn expect_token(&amp;mut self) -&gt; Result&lt;Token, E&gt; {
        self.next().unwrap_or_else(|| Err(self.end_of_stream_error()))
    }

    ...
}
</code></pre>

<p>The <code>Deserialize</code> trait is kept pretty slim, and is how lookahead is
implemented. <code>Deserializer</code> is an enhanced <code>Iterator&lt;Result&lt;Token, E&gt;&gt;</code>, with
many helpful default methods. Here are them in action. First we&rsquo;ll start with
what&rsquo;s probably the simplest <code>Deserializer</code>, which just wraps a <code>Vec&lt;Token&gt;</code>:</p>

<pre><code class="rust">enum Error {
    EndOfStream,
    SyntaxError(Vec&lt;TokenKind&gt;),
    UnexpectedName,
    ConversionError,
    MissingField(&amp;'static str),
}

struct TokenDeserializer&lt;Iter&gt; {
    tokens: Iter,
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; TokenDeserializer&lt;Iter&gt; {
    fn new(tokens: Iter) -&gt; TokenDeserializer&lt;Iter&gt; {
        TokenDeserializer {
            tokens: tokens,
        }
    }
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; Iterator&lt;Result&lt;Token, Error&gt;&gt; for TokenDeserializer&lt;Iter&gt; {
    fn next(&amp;mut self) -&gt; option::Option&lt;Result&lt;Token, Error&gt;&gt; {
        self.tokens.next().map(|token| Ok(token))
    }
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; Deserializer&lt;Error&gt; for TokenDeserializer&lt;Iter&gt; {
    fn end_of_stream_error(&amp;mut self) -&gt; Error {
        Error::EndOfStream
    }

    fn syntax_error(&amp;mut self, _token: Token, expected: &amp;[TokenKind]) -&gt; Error {
        Error::SyntaxError(expected.to_vec())
    }

    fn unexpected_name_error(&amp;mut self, _token: Token) -&gt; Error {
        Error::UnexpectedName
    }

    fn conversion_error(&amp;mut self, _token: Token) -&gt; Error {
        Error::ConversionError
    }

    #[inline]
    fn missing_field&lt;
        T: Deserialize&lt;TokenDeserializer&lt;Iter&gt;, Error&gt;
    &gt;(&amp;mut self, field: &amp;'static str) -&gt; Result&lt;T, Error&gt; {
        Err(Error::MissingField(field))
    }
}
</code></pre>

<p>Overall it should be pretty straight forward. As usual, error handling makes
things a bit noisier, but hopefully it&rsquo;s not too onerous. Next is a
<code>Deserialize</code> for <code>bool</code>:</p>

<pre><code class="rust">impl&lt;D: Deserializer&lt;E&gt;, E&gt; Deserialize&lt;D, E&gt; for bool {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;bool, E&gt; {
        d.expect_bool(token)
    }
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    ...

    #[inline]
    fn expect_bool(&amp;mut self, token: Token) -&gt; Result&lt;bool, E&gt; {
        match token {
            Token::Bool(value) =&gt; Ok(value),
            token =&gt; {
                static EXPECTED_TOKENS: &amp;'static [TokenKind] = &amp;[
                    TokenKind::BoolKind,
                ];
                Err(self.syntax_error(token, EXPECTED_TOKENS))
            }
        }
    }


    ...
}
</code></pre>

<p>Simple! Sequences are a bit more tricky. Here&rsquo;s <code>Deserialize</code> a <code>Vec&lt;T&gt;</code>. We
use a helper adaptor <code>SeqDeserializer</code> to deserialize from all types that
implement <code>FromIterator</code>:</p>

<pre><code class="rust">impl&lt;
    D: Deserializer&lt;E&gt;,
    E,
    T: Deserialize&lt;D ,E&gt;
&gt; Deserialize&lt;D, E&gt; for Vec&lt;T&gt; {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Vec&lt;T&gt;, E&gt; {
        d.expect_seq(token)
    }
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    ...

    #[inline]
    fn expect_seq&lt;
        T: Deserialize&lt;Self, E&gt;,
        C: FromIterator&lt;T&gt;
    &gt;(&amp;mut self, token: Token) -&gt; Result&lt;C, E&gt; {
        let len = try!(self.expect_seq_start(token));
        let mut err = None;

        let collection: C = {
            let d = SeqDeserializer {
                d: self,
                len: len,
                err: &amp;mut err,
            };

            d.collect()
        };

        match err {
            Some(err) =&gt; Err(err),
            None =&gt; Ok(collection),
        }
    }

    ...
}

struct SeqDeserializer&lt;'a, D: 'a, E: 'a&gt; {
    d: &amp;'a mut D,
    len: uint,
    err: &amp;'a mut Option&lt;E&gt;,
}

impl&lt;
    'a,
    D: Deserializer&lt;E&gt;,
    E,
    T: Deserialize&lt;D, E&gt;
&gt; Iterator&lt;T&gt; for SeqDeserializer&lt;'a, D, E&gt; {
    #[inline]
    fn next(&amp;mut self) -&gt; option::Option&lt;T&gt; {
        match self.d.expect_seq_elt_or_end() {
            Ok(next) =&gt; {
                self.len -= 1;
                next
            }
            Err(err) =&gt; {
                *self.err = Some(err);
                None
            }
        }
    }

    #[inline]
    fn size_hint(&amp;self) -&gt; (uint, option::Option&lt;uint&gt;) {
        (self.len, Some(self.len))
    }
}
</code></pre>

<p>Last is a struct deserializer. This relies on a simple state machine in order
to deserialize from out of order maps:</p>

<pre><code class="rust">struct Foo {
    a: (),
    b: uint,
    c: TreeMap&lt;String, Option&lt;char&gt;&gt;,
}

impl&lt;
    D: Deserializer&lt;E&gt;,
    E
&gt; Deserialize&lt;D, E&gt; for Foo {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Foo, E&gt; {
        try!(d.expect_struct_start(token, "Foo"));

        let mut a = None;
        let mut b = None;
        let mut c = None;

        static FIELDS: &amp;'static [&amp;'static str] = &amp;["a", "b", "c"];

        loop {
            let idx = match try!(d.expect_struct_field_or_end(FIELDS)) {
                Some(idx) =&gt; idx,
                None =&gt; { break; }
            };

            match idx {
                Some(0) =&gt; { a = Some(try!(d.expect_struct_value())); }
                Some(1) =&gt; { b = Some(try!(d.expect_struct_value())); }
                Some(2) =&gt; { c = Some(try!(d.expect_struct_value())); }
                Some(_) =&gt; unreachable!(),
                None =&gt; { let _: IgnoreTokens = try!(Deserialize::deserialize(d)); }
            }
        }

        Ok(Foo { a: a.unwrap(), b: b.unwrap(), c: c.unwrap() })
    }
}
</code></pre>

<p>It&rsquo;s more complicated than <code>libserialize</code>&rsquo;s struct parsing, but it performs
much better because it can handle out of order maps without buffering tokens.</p>

<h3>Serialization</h3>

<p>Serialization&rsquo;s story is a much simpler one. Conceptually
<code>serde::ser::Serializer</code>/<code>serde::ser::Serialize</code> are inspired by the
deserialization story, but we don&rsquo;t need the tagged tokens because we already
know the types. Here are the traits:</p>

<pre><code class="rust">pub trait Serialize&lt;S: Serializer&lt;E&gt;, E&gt; {
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt;;
}

pub trait Serializer&lt;E&gt; {
    fn serialize_null(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_bool(&amp;mut self, v: bool) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_int(&amp;mut self, v: int) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i8(&amp;mut self, v: i8) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i16(&amp;mut self, v: i16) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i32(&amp;mut self, v: i32) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i64(&amp;mut self, v: i64) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_uint(&amp;mut self, v: uint) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u8(&amp;mut self, v: u8) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u16(&amp;mut self, v: u16) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u32(&amp;mut self, v: u32) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u64(&amp;mut self, v: u64) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_f32(&amp;mut self, v: f32) -&gt; Result&lt;(), E&gt; {
        self.serialize_f64(v as f64)
    }

    fn serialize_f64(&amp;mut self, v: f64) -&gt; Result&lt;(), E&gt;;

    fn serialize_char(&amp;mut self, v: char) -&gt; Result&lt;(), E&gt;;

    fn serialize_str(&amp;mut self, v: &amp;str) -&gt; Result&lt;(), E&gt;;

    fn serialize_tuple_start(&amp;mut self, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_tuple_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_tuple_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_struct_start(&amp;mut self, name: &amp;str, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_struct_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, name: &amp;str, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_struct_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_enum_start(&amp;mut self, name: &amp;str, variant: &amp;str, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_enum_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_enum_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_option&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;Option&lt;T&gt;) -&gt; Result&lt;(), E&gt;;

    fn serialize_seq&lt;
        T: Serialize&lt;Self, E&gt;,
        Iter: Iterator&lt;T&gt;
    &gt;(&amp;mut self, iter: Iter) -&gt; Result&lt;(), E&gt;;

    fn serialize_map&lt;
        K: Serialize&lt;Self, E&gt;,
        V: Serialize&lt;Self, E&gt;,
        Iter: Iterator&lt;(K, V)&gt;
    &gt;(&amp;mut self, iter: Iter) -&gt; Result&lt;(), E&gt;;
}
</code></pre>

<p>There are many default methods, so only a handful of implementations need to be
specified. Now lets look at how they are used. Here&rsquo;s a simple
<code>AssertSerializer</code> that I use in my test suite to make sure I&rsquo;m serializing
properly:</p>

<pre><code class="rust">struct AssertSerializer&lt;Iter&gt; {
    iter: Iter,
}

impl&lt;'a, Iter: Iterator&lt;Token&lt;'a&gt;&gt;&gt; AssertSerializer&lt;Iter&gt; {
    fn new(iter: Iter) -&gt; AssertSerializer&lt;Iter&gt; {
        AssertSerializer {
            iter: iter,
        }
    }

    fn serialize&lt;'b&gt;(&amp;mut self, token: Token&lt;'b&gt;) -&gt; Result&lt;(), Error&gt; {
        let t = self.iter.next().unwrap();

        assert_eq!(t, token);

        Ok(())
    }
}

impl&lt;'a, Iter: Iterator&lt;Token&lt;'a&gt;&gt;&gt; Serializer&lt;Error&gt; for AssertSerializer&lt;Iter&gt; {
    fn serialize_null(&amp;mut self) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Null)
    }
    fn serialize_bool(&amp;mut self, v: bool) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Bool(v))
    }
    fn serialize_int(&amp;mut self, v: int) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Int(v))
    }
    ...
}
</code></pre>

<p>Implementing <code>Serialize</code> for values follows the same pattern. Here&rsquo;s <code>bool</code>:</p>

<pre><code>impl&lt;S: Serializer&lt;E&gt;, E&gt; Serialize&lt;S, E&gt; for bool {
    #[inline]
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        s.serialize_bool(*self)
    }
}
</code></pre>

<p><code>Vec&lt;T&gt;</code>:</p>

<pre><code class="rust">impl&lt;
    S: Serializer&lt;E&gt;,
    E,
    T: Serialize&lt;S, E&gt;
&gt; Serialize&lt;S, E&gt; for Vec&lt;T&gt; {
    #[inline]
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        s.serialize_seq(self.iter())
    }
}

pub trait Serializer&lt;E&gt; {
    ...

    fn serialize_seq&lt;
        T: Serialize&lt;AssertSerializer&lt;Iter&gt;, Error&gt;,
        SeqIter: Iterator&lt;T&gt;
    &gt;(&amp;mut self, mut iter: SeqIter) -&gt; Result&lt;(), Error&gt; {
        let (len, _) = iter.size_hint();
        try!(self.serialize(Token::SeqStart(len)));
        for elt in iter {
            try!(elt.serialize(self));
        }
        self.serialize(Token::SeqEnd)
    }

    ...
}
</code></pre>

<p>And structs:</p>

<pre><code class="rust">struct Foo {
    a: (),
    b: uint,
    c: TreeMap&lt;String, Option&lt;char&gt;&gt;,
}

impl&lt;
  S: Serializer&lt;E&gt;,
  E
&gt; Serialize&lt;S, E&gt; for Foo {
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        try!(s.serialize_struct_start("Foo", 2u));
        try!(s.serialize_struct_elt("a", &amp;self.a));
        try!(s.serialize_struct_elt("b", &amp;self.b));
        try!(s.serialize_struct_elt("c", &amp;self.c));
        s.serialize_struct_end()
    }
}
</code></pre>

<p>Much simpler than deserialization.</p>

<h2>Performance</h2>

<p>So how does it perform? Here&rsquo;s the serialization benchmarks, with yet another
ordering. This time sorted by the performance:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library         </th>
<th> format                 </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (unpacked) </td>
<td> 4349                 </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto            </td>
<td> 3824.20              </td>
</tr>
<tr>
<td> Rust     </td>
<td> bincode         </td>
<td> Binary                 </td>
<td> 1020                 </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf    </td>
<td> Protocol Buffers       </td>
<td> 596.78               </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (packed)   </td>
<td> 583                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-msgpack    </td>
<td> MessagePack            </td>
<td> 397                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-protobuf   </td>
<td> Protocol Buffers       </td>
<td> 357                  </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON                   </td>
<td> 304                  </td>
</tr>
<tr>
<td> <strong>Rust</strong> </td>
<td> <strong>serde::json</strong> </td>
<td> <strong>JSON</strong>               </td>
<td> <strong>222</strong>              </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf      </td>
<td> Protocol Buffers       </td>
<td> 214.68               </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson          </td>
<td> JSON                   </td>
<td> 147.37               </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json </td>
<td> JSON                   </td>
<td> 147                  </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json   </td>
<td> JSON                   </td>
<td> 80.49                </td>
</tr>
</tbody>
</table>


<p><code>serde::json</code> is doing pretty good! It still has got a ways to go to catch up
to <a href="https://github.com/miloyip/rapidjson">rapidjson</a>, but it&rsquo;s pretty cool it&rsquo;s
beating <a href="https://github.com/golang/protobuf">goprotobuf</a> out of the box :)</p>

<p>Here are the deserialization numbers:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library         </th>
<th> format                  </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (unpacked)  </td>
<td> 2185                   </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto (zero copy) </td>
<td> 1407.95                </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto             </td>
<td> 711.77                 </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (packed)    </td>
<td> 351                    </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf    </td>
<td> Protocol Buffers        </td>
<td> 272.68                 </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON (sax)              </td>
<td> 189                    </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON (dom)              </td>
<td> 162                    </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-msgpack    </td>
<td> MessagePack             </td>
<td> 138                    </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-protobuf   </td>
<td> Protocol Buffers        </td>
<td> 129                    </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson          </td>
<td> JSON                    </td>
<td> 95.06                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> bincode         </td>
<td> Binary                  </td>
<td> 80                     </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf      </td>
<td> Protocol Buffers        </td>
<td> 79.78                  </td>
</tr>
<tr>
<td> <strong>Rust</strong> </td>
<td> <strong>serde::json</strong> </td>
<td> <strong>JSON</strong>                </td>
<td> <strong>67</strong>                 </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json </td>
<td> JSON                    </td>
<td> 24                     </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json   </td>
<td> JSON                    </td>
<td> 22.79                  </td>
</tr>
</tbody>
</table>


<p>Well on the plus side, <code>serde::json</code> nearly 3 times faster than
<code>libserialize::json</code>. On the downside rapidjson is nearly 3 times faster than
us in it&rsquo;s SAX style parsing. Even the newly added deserialization support in
<a href="https://github.com/pquerna/ffjson">ffjson</a> is 1.4 times faster than us. So we
got more work cut out for us!</p>

<p>Next time, serde2!</p>

<p>PS: I&rsquo;m definitely getting close to the end of my story, and while I have some
better numbers with serde2, nothing is quite putting me in the rapidjson
range. Anyone want to help optimize
<a href="https://github.com/erickt/rust-serde">serde</a>? I would greatly appreciate the help!</p>

<p>PPS: I&rsquo;ve gotten a number of requests for my
<a href="https://github.com/erickt/rust-serialization-benchmarks">serialization benchmarks</a>
to be ported over to other languages and libraries. Especially a C++ version
of Cap&#8217;n Proto. Unfortunately I don&rsquo;t really have the time to do it myself.
Would anyone be up for helping to implement it?</p>

<p>comments on <a href="https://www.reddit.com/r/rust/comments/2p85za/rewriting_rust_serialization_part_3_introducing/">reddit</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Rust to Make a Safer Interface for Yahoo's Fast MDBM Database]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/rust-and-mdbm/"/>
    <updated>2014-12-13T11:09:19-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/rust-and-mdbm</id>
    <content type="html"><![CDATA[<p>I&rsquo;m really supposed to be working on my
<a href="http://erickt.github.io/blog/categories/serialization/">serialization series</a>,
 but I just saw a neat new library that was open sourced by Yahoo a couple days
ago called <a href="https://github.com/yahoo/mdbm">MDBM</a> on
<a href="https://news.ycombinator.com/item?id=8732891">Hacker News</a>. I know nothing
about the library, but there are some really neat claims:</p>

<ol>
<li>It&rsquo;s supposed to be fast, and that&rsquo;s always nice.</li>
<li>It&rsquo;s supposed to have a really slim interface.</li>
<li>It&rsquo;s so fast because it&rsquo;s passing around naked pointers to mmapped files,
which is terribly unsafe. Unless you got rust which can prove that those
pointers won&rsquo;t escape :)</li>
</ol>


<p>So I wanted to see how easy it&rsquo;d be to make a Rust binding for the project.
If you want to follow along, first make sure you have
 <a href="http://www.rust-lang.org/install.html">rust installed</a>. Unfortunately it
looks like MDBM only supports Linux and FreeBSD, so I had to build out a Fedora
VM to test this out on. I <em>think</em> this is all you need to build it:</p>

<pre><code>% git clone https://github.com/yahoo/mdbm
% cd mdbm/redhat
% make
% rpm -Uvh ~/rpmbuild/RPMS/x86_64/mdbm-4.11.1-1.fc21.x86_64.rpm
% rpm -Uvh ~/rpmbuild/RPMS/x86_64/mdbm-devel-4.11.1-1.fc21.x86_64.rpm
</code></pre>

<p>Unfortunately it&rsquo;s only for linux, and I got a mac, but it turns out there&rsquo;s
plenty I can do to prep while VirtualBox and Fedora 21 download. Lets start out
by creating our project with <a href="https://crates.io">cargo</a>:</p>

<pre><code>% cargo new mdbm
% cd rust-mdbm
</code></pre>

<p>(Right now there&rsquo;s no way to have the name be different than the path, so edit
<code>Cargo.toml</code> to rename the project to <code>mdbm</code>. I filed
<a href="https://github.com/rust-lang/cargo/issues/1030">#1030</a> to get that
implemented).</p>

<p>By convention, we put bindgen packages into <code>$name-sys</code>, so make that crate as
well:</p>

<pre><code>% cargo new --no-git mdbm-sys
% cd mdbm-sys
</code></pre>

<p>We&rsquo;ve got a really cool tool called
<a href="https://github.com/crabtw/rust-bindgen">bindgen</a>, which uses clang to parse
header files and convert them into an unsafe rust interface. So lets check out
MDBM, and generate a crate to wrap it up in.</p>

<pre><code>% cd ../..
% git clone git@github.com:crabtw/rust-bindgen.git
% cd rust-bindgen
% cargo build
% cd ..
% git clone git@github.com:yahoo/mdbm.git
% cd rust-mdbm/mdbm-sys
% DYLD_LIBRARY_PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib \
  ~/rust/rust-bindgen/target/bindgen \
  -lmdbm \
  -o src/lib.rs \
  mdbm/include/mdbm.h
</code></pre>

<p>Pretty magical. Make sure it builds:</p>

<pre><code>% cargo build
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3:21: 3:25 error: failed to resolve. Maybe a missing `extern crate libc`?
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3 pub type __int8_t = ::libc::c_char;
                                                                  ^~~~
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3:21: 3:35 error: use of undeclared type name `libc::c_char`
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3 pub type __int8_t = ::libc::c_char;
                                                                  ^~~~~~~~~~~~~~
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:4:22: 4:26 error: failed to resolve. Maybe a missing `extern crate libc`?
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:4 pub type __uint8_t = ::libc::c_uchar;
                                                                   ^~~~
...
</code></pre>

<p>Nope! The problem is that we don&rsquo;t have the <code>libc</code> crate imported. We don&rsquo;t
have a convention yet for this, but I like to do is:</p>

<pre><code>mv src/lib.rs src/ffi.rs
</code></pre>

<p>And create a <code>src/lib.rs</code> that contains:</p>

<pre><code class="rust">#![allow(non_camel_case_types)]
#![allow(non_snake_case)]

extern crate libc;

type __builtin_va_list = libc::c_void;

include!("ffi.rs")
</code></pre>

<p>This lets me run bindgen later on without mucking up the library. This now
compiles. Next up is our high level interface. Add <code>mdbm-sys</code> to our high level
interface by adding this to the <code>rust-mdbm/Cargo.toml</code> file:</p>

<pre><code class="toml">[dependencies.mdbm-sys]
path = "mdbm-sys"
</code></pre>

<p>By now I got my VirtualBox setup working, so now to the actual code! Lets start
with a barebones wrapper around the database:</p>

<pre><code class="rust">pub struct MDBM {
    db: *mut mdbm_sys::MDBM,
}
</code></pre>

<p>Next is the constructor and destructor. I&rsquo;m hardcoding things for now and using
IoError, since MDBM appears to log everything to the ERRNO:</p>

<pre><code>impl MDBM {
    pub fn new(
        path: &amp;Path,
        flags: uint,
        mode: uint,
        psize: uint,
        presize: uint
    ) -&gt; Result&lt;MDBM, IoError&gt; {
        unsafe {
            let path = path.to_c_str();
            let db = mdbm_sys::mdbm_open(
                path.as_ptr(),
                flags as libc::c_int,
                mode as libc::c_int,
                psize as libc::c_int,
                presize as libc::c_int);

            if db.is_null() {
                Err(IoError::last_error())
            } else {
                Ok(MDBM { db: db })
            }
        }
    }

    ...
}

impl Drop for MDBM {
    fn drop(&amp;mut self) {
        unsafe {
            mdbm_sys::mdbm_sync(self.db);
            mdbm_sys::mdbm_close(self.db);
        }
    }
}
</code></pre>

<p>Pretty straightforward translation of the examples with some hardcoded values
to start out. Next up is a wrapper around MDBM&rsquo;s <code>datum</code> type, which is the
type used for both keys and values. <code>datum</code> is just a simple struct containing
a pointer and length, pretty much analogous to our <code>&amp;[u8]</code> slices. However our
slices are much more powerful because our type system can guarantee that in
safe Rust, these slices can never outlive where they are derived from:</p>

<pre><code class="rust">pub struct Datum&lt;'a&gt; {
    bytes: &amp;'a [u8],
}

impl&lt;'a&gt; Datum&lt;'a&gt; {
    pub fn new(bytes: &amp;[u8]) -&gt; Datum {
        Datum { bytes: bytes }
    }
}

fn to_raw_datum(datum: &amp;Datum) -&gt; mdbm_sys::datum {
    mdbm_sys::datum {
        dptr: datum.bytes.as_ptr() as *mut _,
        dsize: datum.bytes.len() as libc::c_int,
    }
}
</code></pre>

<p>And for convenience, lets add a <code>AsDatum</code> conversion method:</p>

<pre><code class="rust">pub trait AsDatum for Sized? {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt;;
}

impl&lt;'a, Sized? T: AsDatum&gt; AsDatum for &amp;'a T {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; { (**self).as_datum() }
}

impl AsDatum for [u8] {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; {
        Datum::new(self)
    }
}

impl AsDatum for str {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; {
        self.as_bytes().as_datum()
    }
}
</code></pre>

<p>And finally, we got setting and getting a key-value. Setting is pretty
straightforward. The only fun thing is using the <code>AsDatum</code> constraints so we
can do <code>db.set(&amp;"foo", &amp;"bar", 0)</code> instead of
<code>db.set(Datum::new(&amp;"foo".as_slice()), Datum::new("bar".as_slice()), 0)</code>.
we&rsquo;re copying into the database, we don&rsquo;t have to worry about lifetimes yet:</p>

<pre><code class="rust">impl MDBM {
    ...

    /// Set a key.
    pub fn set&lt;K, V&gt;(&amp;self, key: &amp;K, value: &amp;V, flags: int) -&gt; Result&lt;(), IoError&gt; where
        K: AsDatum,
        V: AsDatum,
    {
        unsafe {
            let rc = mdbm_sys::mdbm_store(
                self.db,
                to_raw_datum(&amp;key.as_datum()),
                to_raw_datum(&amp;value.as_datum()),
                flags as libc::c_int);

            if rc == -1 {
                Err(IoError::last_error())
            } else {
                Ok(())
            }
        }
    }

    ...
</code></pre>

<p>MDBM requires the database to be locked in order to get the keys. This os
where things get fun in order to prevent those interior pointers from escaping.
We&rsquo;ll create another wrapper type that manages the lock, and uses RAII to
unlock when we&rsquo;re done. We tie the lifetime of the <code>Lock</code> to the lifetime of
the database and key, which prevents it from outliving either object:</p>

<pre><code class="rust">impl MDBM {
    ...

    /// Lock a key.
    pub fn lock&lt;'a, K&gt;(&amp;'a self, key: &amp;'a K, flags: int) -&gt; Result&lt;Lock&lt;'a&gt;, IoError&gt; where
        K: AsDatum,
    {
        let rc = unsafe {
            mdbm_sys::mdbm_lock_smart(
                self.db,
                &amp;to_raw_datum(&amp;key.as_datum()),
                flags as libc::c_int)
        };

        if rc == 1 {
            Ok(Lock { db: self, key: key.as_datum() })
        } else {
            Err(IoError::last_error())
        }
    }

    ...
}

pub struct Lock&lt;'a&gt; {
    db: &amp;'a MDBM,
    key: Datum&lt;'a&gt;,
}

#[unsafe_destructor]
impl&lt;'a&gt; Drop for Lock&lt;'a&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            let rc = mdbm_sys::mdbm_unlock_smart(
                self.db.db,
                &amp;to_raw_datum(&amp;self.key),
                0);

            assert_eq!(rc, 1);
        }
    }
}
</code></pre>

<p>(Note that I&rsquo;ve heard <code>#[unsafe_destrutor]</code> as used here may become unnecessary
in 1.0).</p>

<p>Finally, let&rsquo;s get our value! Assuming the value exists, we tie the lifetime of
the <code>Lock</code> to the lifetime of the returned <code>&amp;[u8]</code>:</p>

<pre><code class="rust">impl&lt;'a&gt; Lock&lt;'a&gt; {
    /// Fetch a key.
    pub fn get&lt;'a&gt;(&amp;'a self) -&gt; Option&lt;&amp;'a [u8]&gt; {
        unsafe {
            let value = mdbm_sys::mdbm_fetch(
                self.db.db,
                to_raw_datum(&amp;self.key));

            if value.dptr.is_null() {
                None
            } else {
                // we want to constrain the ptr to our lifetime.
                let ptr: &amp;*const u8 = mem::transmute(&amp;value.dptr);
                Some(slice::from_raw_buf(ptr, value.dsize as uint))
            }
        }
    }
}
</code></pre>

<p>Now to verify it works:</p>

<pre><code class="rust">#[test]
fn test() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    db.set(&amp;"hello", &amp;"world", 0).unwrap();

    {
        // key needs to be an lvalue so the lock can hold a reference to
        // it.
        let key = "hello";

        // Lock the key. RAII will unlock it when we exit this scope.
        let value = db.lock(&amp;key, 0).unwrap();

        // Convert the value into a string. The lock is still live at this
        // point.
        let value = str::from_utf8(value.get().unwrap()).unwrap();
        assert_eq!(value, "world");
        println!("hello: {}", value);
    }
}
</code></pre>

<p>Which when run with <code>cargo test</code>, produces:</p>

<pre><code>   Compiling rust-mdbm v0.0.1 (file:///home/erickt/Projects/rust-mdbm)
     Running target/rust-mdbm-98b81ab156dc1e5f

running 1 test
test tests::test_set_get ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured

   Doc-tests rust-mdbm

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured
</code></pre>

<p>Next we want to make sure invalid behavior is a compile-time error. First, make
sure we don&rsquo;t leak the keys:</p>

<pre><code class="rust">#[test]
fn test_keys_cannot_escape() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    db.set(&amp;"hello", &amp;"world", 0).unwrap();

    let _ = {
        let key = vec![1];
        db.lock(&amp;key.as_slice(), 0).unwrap()
    };
}
</code></pre>

<p>Which errors with:</p>

<pre><code>src/lib.rs:217:22: 217:25 error: `key` does not live long enough
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
                                    ^~~
src/lib.rs:215:17: 218:10 note: reference must be valid for the expression at 215:16...
src/lib.rs:215         let _ = {
src/lib.rs:216             let key = vec![1];
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
src/lib.rs:218         };
src/lib.rs:215:17: 218:10 note: ...but borrowed value is only valid for the block at 215:16
src/lib.rs:215         let _ = {
src/lib.rs:216             let key = vec![1];
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
src/lib.rs:218         };
error: aborting due to previous error
</code></pre>

<p>And confirm the value doesn&rsquo;t leak either:</p>

<pre><code class="rust">#[test]
fn test_values_cannot_escape() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    let _ = {
        db.set(&amp;"hello", &amp;"world", 0).unwrap();

        let key = "hello";
        let value = db.lock(&amp;key, 0).unwrap();
        str::from_utf8(value.get().unwrap()).unwrap()
    };
}
</code></pre>

<p>Which errors with:</p>

<pre><code>src/lib.rs:237:34: 237:37 error: `key` does not live long enough
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
                                                ^~~
src/lib.rs:233:17: 239:10 note: reference must be valid for the expression at 233:16...
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:233:17: 239:10 note: ...but borrowed value is only valid for the block at 233:16
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:238:28: 238:33 error: `value` does not live long enough
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
                                          ^~~~~
src/lib.rs:233:17: 239:10 note: reference must be valid for the expression at 233:16...
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:233:17: 239:10 note: ...but borrowed value is only valid for the block at 233:16
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
                                                     ...
</code></pre>

<p>Success! Not too bad for 2 hours of work. Baring bugs, this <code>mdbm</code>
library should perform at roughly the same speed as the C library, but
eliminate many very painful bug opportunities that require tools like Valgrind
to debug.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking Is Confusing in Low Level Rust]]></title>
    <link href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/"/>
    <updated>2014-11-22T12:10:29-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing</id>
    <content type="html"><![CDATA[<p>Edit: you can find all the code in this post
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">here</a>,
and I filed <a href="https://github.com/rust-lang/rust/issues/19281">19281</a> for the
regression I mentioned at the end of the post.</p>

<hr />

<p>Low level benchmarking is confusing and non-intuitive.</p>

<p>The end.</p>

<hr />

<p>Or not. Whatever. So I&rsquo;m trying to get my
implement-<code>Reader</code>-and-<code>Writer</code>-for-<code>&amp;[u8]</code> type PR
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a> landed. But
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63925495">Steven Fackler</a>
obnixously and correctly pointed out that this won&rsquo;t play that nicely with the
new <code>Reader</code> and <code>Writer</code> implementation for <code>Vec&lt;u8&gt;</code>. Grumble grumble. And then
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63927659">Alex Crichton</a>
had the gall to mention that a <code>Writer</code> for <code>mut &amp;[u8]</code> also probably won&rsquo;t be
that common either. Sure, he&rsquo;s write and all, but but I got it working without
needing an index! That means that the <code>&amp;mut [u8]</code> <code>Writer</code> only needs 2
pointers instead of <code>BufWriter</code>&rsquo;s three, so it just has to be faster! Well,
doesn&rsquo;t it?</p>

<p>Stupid benchmarks.</p>

<p>I got to say it&rsquo;s pretty addicting writing micro-benchmarks. It&rsquo;s a lot of fun
seeing how sensitive low-level code can be to just the smallest of tweaks. It&rsquo;s
also really annoying when you write something you think is pretty neat, then
you find it&rsquo;s chock-full of false dependencies between cache lines, or other
mean things CPUs like to impose on poor programmers.</p>

<p>Anyway, to start lets look at what should be the fastest way to write to a
buffer. Completely unsafely with no checks.</p>

<pre><code class="rust">unsafe fn do_copy_nonoverlapping_memory(
  mut dst: *mut u8,
  src: *const u8,
  len: uint,
  batches: uint
) {
    for _ in range(0, batches) {
        ptr::copy_nonoverlapping_memory(dst, src, len);
        dst = dst.offset(len as int);
    }
}

#[test]
fn test_copy_nonoverlapping_memory() {
    let dst = &amp;mut [0_u8, .. BATCHES * SRC_LEN];
    let src = &amp;[1, .. SRC_LEN];

    unsafe {
        do_copy_nonoverlapping_memory(
            dst.as_mut_ptr(),
            src.as_ptr(),
            src.len(),
            BATCHES
        );
    }
    assert!(dst.iter().all(|c| *c == 1));
}
</code></pre>

<p>With <code>SRC_LEN=4</code> and <code>BATCHES=128</code>, we get this. For fun I added the new
<code>libtest</code> from <a href="https://github.com/rust-lang/rust/pull/19233">#19233</a> that will
hopefully land soon. I also added also ran variations that explicitly inlined
and not inlined the inner function:</p>

<pre><code>test bench_copy_nonoverlapping_memory               ... bench: 50 |   [-***#**------]                        | 200:        72 ns/iter (+/- 45)
test bench_copy_nonoverlapping_memory_inline_always ... bench: 50 |       [---***#****************----------]| 100:        65 ns/iter (+/- 39)
test bench_copy_nonoverlapping_memory_inline_never  ... bench: 500 |      [------********#*********-------] | 1000:       747 ns/iter (+/- 393)
</code></pre>

<p>So overall it does quite well. Now lets compare with the code I wrote:</p>

<pre><code class="rust">impl&lt;'a&gt; Writer for &amp;'a mut [u8] {
    #[inline]
    fn write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        if self.is_empty() { return Err(io::standard_error(io::EndOfFile)); }

        let dst_len = self.len();
        let src_len = src.len();

        let write_len = min(dst_len, src_len);

        slice::bytes::copy_memory(*self, src.slice_to(write_len));

        unsafe {
            *self = mem::transmute(raw::Slice {
                data: self.as_ptr().offset(write_len as int),
                len: dst_len - write_len,
            });
        }

        if src_len &gt; dst_len {
            Err(io::standard_error(io::ShortWrite(write_len)))
        } else {
            Ok(())
        }
    }
}
</code></pre>

<p>And. Well. It didn&rsquo;t do that well.</p>

<pre><code>test writer::bench_slice_writer                             ... bench: 500 |   [------**#**--]                      | 2000:       920 ns/iter (+/- 448)
test writer::bench_slice_writer_inline_always               ... bench: 600 | [-**#*****---]                         | 2000:       711 ns/iter (+/- 405)
test writer::bench_slice_writer_inline_never                ... bench: 600 |   [-***#******---]                     | 2000:       838 ns/iter (+/- 474)
</code></pre>

<p>Wow. That&rsquo;s pretty bad compared to the ideal.</p>

<p>Crud. So not only did I add an implementation that&rsquo;s probably going to not work
with <code>write!</code> and now it turns out the performance is pretty terrible. Inlining
isn&rsquo;t helping like it did in the unsafe case. So how&rsquo;s
<a href="https://github.com/rust-lang/rust/blob/master/src/libstd/io/mem.rs#L219">std::io::BufWriter</a>
compare?</p>

<pre><code class="rust">pub struct BufWriter&lt;'a&gt; {
    buf: &amp;'a mut [u8],
    pos: uint
}

impl&lt;'a&gt; Writer for BufWriter&lt;'a&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        // return an error if the entire write does not fit in the buffer
        let cap = if self.pos &gt;= self.buf.len() { 0 } else { self.buf.len() - self.pos };
        if buf.len() &gt; cap {
            return Err(IoError {
                kind: io::OtherIoError,
                desc: "Trying to write past end of buffer",
                detail: None
            })
        }

        slice::bytes::copy_memory(self.buf[mut self.pos..], buf);
        self.pos += buf.len();
        Ok(())
    }
}
</code></pre>

<p>Here&rsquo;s how it does:</p>

<pre><code>test writer::bench_std_buf_writer                           ... bench: 50 |     [------**************#******-------] | 100:        79 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_always             ... bench: 50 |    [-----************#*********-------]  | 100:        75 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_never              ... bench: 600 |   [#****-----]                         | 2000:       705 ns/iter (+/- 337)
</code></pre>

<p>That&rsquo;s just cruel. The optimization gods obviously hate me. So I started
playing with a lot of
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">variations</a>
(it&rsquo;s my yeah yeah it&rsquo;s my serialization benchmark suite, I&rsquo;m planning on
making it more general purpose. Besides it&rsquo;s my suite and I can do whatever I
want with it, so there):</p>

<ul>
<li>(BufWriter0): Turning this <code>Writer</code> into a struct wrapper shouldn&rsquo;t do anything, and it
didn&rsquo;t.</li>
<li>(BufWriter1): There&rsquo;s error handling, does removing it help? Nope!</li>
<li>(BufWriter5): There&rsquo;s an implied branch in <code>let write_len = min(dst_len, src_len)</code>. We can
turn that into the branch-predictor-friendly:</li>
</ul>


<pre><code class="rust">let x = (dst_len &lt; buf_len) as uint;
let write_len = dst_len * x + src_len * (1 - x);
</code></pre>

<p>Doesn&rsquo;t matter, still performs the same.</p>

<ul>
<li>(BufWriter2): Fine then, optimization gods! Lets remove the branch altogether and just
  always advance the slice <code>src.len()</code> bytes! Damn the safety! That, of course,
works. I can hear them giggle.</li>
<li>(BufWriter3): Maybe, just maybe there&rsquo;s something weird going on with
  inlining across crates? Lets copy <code>std::io::BufWriter</code> and make sure that
  it&rsquo;s still nearly optimal. It still is.</li>
<li>(BufWriter6): Technically the <code>min(dst_len, src_len)</code> is a bounds check, so
we could switch from the bounds checked <code>std.slice::bytes::copy_memory</code> to
  the unsafe <code>std::ptr::copy_nonoverlapping_memory</code>, but that also doesn&rsquo;t
  help.</li>
<li>(BufWriter7): Might as well and apply the last trick to <code>std::io::BufWriter</code>,
  and it does shave a couple nanoseconds off. It might be worth pushing it
  upstream:</li>
</ul>


<pre><code>test writer::bench_buf_writer_7                             ... bench: 50 |   [-*#********--------------------------]| 100:        55 ns/iter (+/- 44)
test writer::bench_buf_writer_7_inline_always               ... bench: 50 |     [---------********#*********-------] | 100:        76 ns/iter (+/- 39)
test writer::bench_buf_writer_7_inline_never                ... bench: 600 |   [-***#****----]                      | 2000:       828 ns/iter (+/- 417)
</code></pre>

<ul>
<li>(BufWriter4): While I&rsquo;m using one less <code>uint</code> than <code>std::io::BufWriter</code>, I&rsquo;m
  doing two writes to advance my slice, one to advance the pointer, and one to
  shrink the length. <code>std::io::BufWriter</code> only has to advance it&rsquo;s <code>pos</code> index.
  But in this case if instead of treating the slice as a <code>(ptr, length)</code>, we
  can convert it into a <code>(start_ptr, end_ptr)</code>, where <code>start_ptr=ptr</code>, and
  <code>end_ptr=ptr+length</code>. This works! Ish:</li>
</ul>


<pre><code>test writer::bench_buf_writer_4                             ... bench: 80 |   [--******#*******-----]                | 200:       109 ns/iter (+/- 59)
test writer::bench_buf_writer_4_inline_always               ... bench: 100 |     [------***#******---]               | 200:       133 ns/iter (+/- 44)
test writer::bench_buf_writer_4_inline_never                ... bench: 500 |      [-----***********#***********----]| 1000:       778 ns/iter (+/- 426)
</code></pre>

<p>I know when I&rsquo;m defeated. Oh well. I guess I can at least update
<code>std::io::BufWriter</code> to support the new error handling approach:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter10&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if self.pos == dst_len { return Err(io::standard_error(io::EndOfFile)); }

        let src_len = src.len();
        let cap = dst_len - self.pos;

        let write_len = min(cap, src_len);

        slice::bytes::copy_memory(self.dst[mut self.pos..], src[..write_len]);

        if src_len &gt; dst_len {
            return Err(io::standard_error(io::ShortWrite(write_len)));
        }

        self.pos += write_len;
        Ok(())
    }
}
</code></pre>

<p>How&rsquo;s it do?</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 600 | [----**#***--]                         | 2000:       841 ns/iter (+/- 413)
test writer::bench_buf_writer_10_inline_always              ... bench: 600 |  [----**#***--]                        | 2000:       872 ns/iter (+/- 387)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--******#**---]                     | 2000:       960 ns/iter (+/- 486)
</code></pre>

<p>Grumble grumble. It turns out that if we tweak the <code>copy_memory</code> line to:</p>

<pre><code class="rust">        slice::bytes::copy_memory(self.dst[mut self.pos..], src);
</code></pre>

<p>It shaves 674 nanoseconds off the run:</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 200 |[---***#************------------]        | 400:       230 ns/iter (+/- 147)
test writer::bench_buf_writer_10_inline_always              ... bench: 200 |   [-----********#*******------]         | 400:       280 ns/iter (+/- 128)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--***#****----]                     | 2000:       885 ns/iter (+/- 475)
</code></pre>

<p>But still no where near where we need to be. That suggests though that always
cutting down the <code>src</code>, which triggers another bounds check has some measurable
impact. So maybe I should only shrink the <code>src</code> slice when we know it needs to
be shrunk?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter11&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst = self.dst[mut self.pos..];
        let dst_len = dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);
            }

                self.pos += src_len;

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);
            }

                self.pos += dst_len;

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>Lets see how it failed this time&hellip;</p>

<pre><code>test writer::bench_buf_writer_11                            ... bench: 60 |      [------********#*********-----]     | 100:        79 ns/iter (+/- 28)
test writer::bench_buf_writer_11_inline_always              ... bench: 60 |[-------******#*************-----------]  | 100:        72 ns/iter (+/- 35)
test writer::bench_buf_writer_11_inline_never               ... bench: 600 |  [--***#***----]                       | 2000:       835 ns/iter (+/- 423)
</code></pre>

<p>No way. That actually worked?! That&rsquo;s awesome! I&rsquo;ll carve that out into another
PR. Maybe it&rsquo;ll work for my original version that doesn&rsquo;t use a <code>pos</code>?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter12&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(src_len as int),
                    len: dst_len - src_len,
                });
            }

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(dst_len as int),
                    len: 0,
                });
            }

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>And yep, just as fast!</p>

<pre><code>test writer::bench_buf_writer_12                            ... bench: 50 |[**#*----------------------------------]   | 80:        51 ns/iter (+/- 26)
test writer::bench_buf_writer_12_inline_always              ... bench: 50 |  [--------**********#********------------]| 90:        69 ns/iter (+/- 36)
test writer::bench_buf_writer_12_inline_never               ... bench: 800 |  [---**#***-]                          | 2000:      1000 ns/iter (+/- 263)
</code></pre>

<p>At this point, both solutions are approximately just as fast as the unsafe
<code>ptr::copy_nonoverlapping_memory</code>! So that&rsquo;s awesome. Now would anyone really
care enough about the extra <code>uint</code>?  There may be a few very specialized cases
where that extra <code>uint</code> could cause a problem, but I&rsquo;m not sure if it&rsquo;s worth
it. What do you all think?</p>

<hr />

<p>I thought that was good, but since I&rsquo;m already here, how&rsquo;s the new <code>Vec&lt;u8&gt;</code>
writer doing? Here&rsquo;s the driver:</p>

<pre><code class="rust">impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_std_vec_writer(b: &amp;mut test::Bencher) {
    let mut dst = Vec::with_capacity(BATCHES * SRC_LEN);
    let src = &amp;[1, .. SRC_LEN];

    b.iter(|| {
        dst.clear();

        do_std_writes(&amp;mut dst, src, BATCHES);
    })
}
</code></pre>

<p>And the results:</p>

<pre><code>test writer::bench_std_vec_writer                           ... bench: 1000 | [----*****#*****--------]             | 2000:      1248 ns/iter (+/- 588)
test writer::bench_std_vec_writer_inline_always             ... bench: 900 |   [----*#***--]                        | 2000:      1125 ns/iter (+/- 282)
test writer::bench_std_vec_writer_inline_never              ... bench: 1000 |  [----***#*****--------]              | 2000:      1227 ns/iter (+/- 516)
</code></pre>

<p>Wow. That&rsquo;s pretty terrible. Something weird must be going on with
<code>Vec::push_all</code>. (Maybe that&rsquo;s what caused my serialization benchmarks to slow
1/3?). Lets skip it:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for VecWriter1&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let src_len = src.len();

        self.dst.reserve(src_len);

        let dst = self.dst.as_mut_slice();

        unsafe {
            // we reserved enough room in `dst` to store `src`.
            ptr::copy_nonoverlapping_memory(
                dst.as_mut_ptr(),
                src.as_ptr(),
                src_len);
        }

        Ok(())
    }
}
</code></pre>

<p>And it looks a bit better, but not perfect:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 100 |         [------*********#*****--------] | 200:       160 ns/iter (+/- 68)
test writer::bench_vec_writer_1_inline_always               ... bench: 100 |     [--------****#**--]                 | 300:       182 ns/iter (+/- 79)
test writer::bench_vec_writer_1_inline_never                ... bench: 600 |   [---****#**--]                       | 2000:       952 ns/iter (+/- 399)
</code></pre>

<p>There&rsquo;s even less going on here than before. The only difference is that
reserve call. Commenting it out gets us back to <code>copy_nonoverlapping_memory</code>
territory:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 70 | [----------------*********#******-------]| 100:        89 ns/iter (+/- 27)
test writer::bench_vec_writer_1_inline_always               ... bench: 50 |   [-***#******--]                        | 200:        75 ns/iter (+/- 46)
test writer::bench_vec_writer_1_inline_never                ... bench: 500 |   [--***#***---]                       | 2000:       775 ns/iter (+/- 433)
</code></pre>

<p>Unfortunately it&rsquo;s getting pretty late, so rather than wait until the next time
to dive into this, I&rsquo;ll leave it up to you all. Does anyone know why <code>reserve</code>
is causing so much trouble here?</p>

<hr />

<p>PS: While I was working on this, I saw
<a href="https://github.com/erickt/rust-serialization-benchmarks/pull/2">stevencheg</a>
submitted a patch to speed up the protocol buffer support. But when I ran the
tests, everything was about 40% slower than the last benchmark
<a href="http://erickt.github.io/blog/2014/11/13/benchmarks-2/">post</a>! Something
happened with Rust&rsquo;s performance over these past couple weeks!</p>

<pre><code>test goser::bincode::bench_decoder                          ... bench:      7682 ns/iter (+/- 3680) = 52 MB/s
test goser::bincode::bench_encoder                          ... bench:       516 ns/iter (+/- 265) = 775 MB/s
test goser::bincode::bench_populate                         ... bench:      1504 ns/iter (+/- 324)
test goser::capnp::bench_deserialize                        ... bench:       251 ns/iter (+/- 140) = 1784 MB/s
test goser::capnp::bench_deserialize_packed_unbuffered      ... bench:      1344 ns/iter (+/- 533) = 250 MB/s
test goser::capnp::bench_populate                           ... bench:       663 ns/iter (+/- 236)
test goser::capnp::bench_serialize                          ... bench:       144 ns/iter (+/- 37) = 3111 MB/s
test goser::capnp::bench_serialize_packed_unbuffered        ... bench:       913 ns/iter (+/- 436) = 369 MB/s
test goser::msgpack::bench_decoder                          ... bench:      3411 ns/iter (+/- 1837) = 84 MB/s
test goser::msgpack::bench_encoder                          ... bench:       961 ns/iter (+/- 477) = 298 MB/s
test goser::msgpack::bench_populate                         ... bench:      1564 ns/iter (+/- 453)
test goser::protobuf::bench_decoder                         ... bench:      3116 ns/iter (+/- 1485) = 91 MB/s
test goser::protobuf::bench_encoder                         ... bench:      1220 ns/iter (+/- 482) = 234 MB/s
test goser::protobuf::bench_populate                        ... bench:       942 ns/iter (+/- 836)
test goser::serialize_json::bench_decoder                   ... bench:     31934 ns/iter (+/- 16186) = 18 MB/s
test goser::serialize_json::bench_encoder                   ... bench:      8481 ns/iter (+/- 3392) = 71 MB/s
test goser::serialize_json::bench_populate                  ... bench:      1471 ns/iter (+/- 426)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Chasing an EPROTOTYPE Through Rust, Sendto, and the OSX Kernel With C-Reduce]]></title>
    <link href="http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/"/>
    <updated>2014-11-19T07:35:04-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug</id>
    <content type="html"><![CDATA[<p>A slight diversion from my serialization series. Last week, strcat submitted
<a href="https://github.com/rust-lang/rust/pull/18885">#18885</a> pull request, which adds
support for using a <code>Vec</code> as a <code>Writer</code>. Over the weekend I submitted
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a>, which allows a <code>&amp;[u8]</code>
to be used as a <code>Reader</code>. Overall a pretty simple change. However, when I was
running the test suite, I found that the <code>std::io::net::tcp::write_close_ip4()</code>
test was occasionally failing:</p>

<pre><code class="rust">    #[test]
    fn write_close_ip4() {
        let addr = next_test_ip4();
        let mut acceptor = TcpListener::bind(addr).listen();

        spawn(proc() {
            let _stream = TcpStream::connect(addr);
            // Close
        });

        let mut stream = acceptor.accept();
        let buf = [0];
        loop {
            match stream.write(buf) {
                Ok(..) =&gt; {}
                Err(e) =&gt; {
                    assert!(e.kind == ConnectionReset ||
                            e.kind == BrokenPipe ||
                            e.kind == ConnectionAborted,
                            "unknown error: {}", e);
                    break;
                }
            }
        }
    }
</code></pre>

<p>The <code>write</code> would succeed a few times, then occasionally error with the
<code>unknown error: Protocol wrong type for socket</code>, or the <code>EPROTOTYPE</code> errno.
This is a really surprising error. As far as I know, the only functions that
return that errno are <code>socket</code>, <code>socketpair</code>, and <code>connect</code>. I searched
everywhere, but I couldn&rsquo;t find any documentation suggesting that <code>write</code> would
ever produce that error.</p>

<p>I wasn&rsquo;t the only one who ran into it. bjz opened
<a href="https://github.com/rust-lang/rust/issues/18900">#18900</a> describing the same
problem. One interesting thing to note is they were also on OSX Yosemite.  So I
took a little bit of time to extract out that test from the Rust test suite
into this <a href="https://gist.github.com/erickt/ac1f35e20834aa5d1972">gist</a> and got
someone on #rust-internals to run it on linux for me with this little driver
script:</p>

<pre><code class="sh">#!/bin/sh

set -e

rustc test.rs

for i in $(seq 1 1000); do
  ./test tcp::test::write_close_ip4
done
</code></pre>

<p>and it didn&rsquo;t error out. So it seems to be system dependent. Further
experimentation showed that if we introduced sleeps or a mutex synchronization
appeared to fix the problem as well. At this point I wasn&rsquo;t sure if this was a
non-issue, a bug in our code, or a bug in the OS. One things for sure though,
if there is a bug, it could be scattered somewhere across the Rust codebase,
which just <code>std::io</code> alone is 20 files at around 12522 lines. It&rsquo;d be a pain to
cut that down to a self contained test case.</p>

<p>Fortunately we&rsquo;ve got <a href="http://embed.cs.utah.edu/creduce/">C-Reduce</a> to help us
out. Back in May <a href="http://www.cs.utah.edu/~regehr/">Professor John Regehr</a> from
the University of Utah came to our
<a href="http://www.meetup.com/Rust-Bay-Area/events/169434302/">Bay Area Rust meetup</a>
 to talk about compiler testing and fuzzing. We recorded the talk, so if you
want to watch it, you can find it
<a href="https://air.mozilla.org/rust-meetup-may-2014/">here</a>. One of the things he
talked about was the tool C-Reduce his research group developed to
automatically cut
out unnecessary lines of code that can still reproduce a bug you&rsquo;re looking
for. While it&rsquo;s written to target C files, it turns out Rust is syntatically
close enough to C that it works out pretty well for it too. All you need is a
single source file and driver script that&rsquo;ll report if the compiled source file
reproduced the bug.</p>

<p>Aside 1: By the way, one of the other things I did this weekend was I put
together a Homebrew
<a href="https://github.com/Homebrew/homebrew/pull/34220">pull request for C-Reduce</a>.
It hasn&rsquo;t landed yet, but you want to use it, you can do:</p>

<pre><code>% brew install https://raw.githubusercontent.com/erickt/homebrew/delta-and-creduce/Library/Formula/delta.rb
% brew install https://raw.githubusercontent.com/erickt/homebrew/delta-and-creduce/Library/Formula/creduce.rb
</code></pre>

<p>Hopefully it&rsquo;ll land soon so you&rsquo;ll be able to do:</p>

<pre><code>% brew install creduce
</code></pre>

<p>Anyway, back to the story. So we&rsquo;ve got a rather large code base to cover, and
while C-reduce does a pretty good job of trimming away lines, just pointing it
at the entire <code>std</code> module is a bit too much for it to handle in a reasonable
amount of time. It probably needs some more semantic information about Rust to
do a better job of cutting out broad swaths of code.</p>

<p>So I needed to do at least a rough pass to slim down the files. I figured the
problem was probably contained in <code>std::io</code> or <code>std::sys</code>, so I wrote a simple
<code>test.rs</code> that explicitly included those modules as part of the crate (see
this <a href="https://gist.github.com/erickt/ac1f35e20834aa5d1972">gist</a> if you are
interested), and used the pretty printer to merge it into one file:</p>

<pre><code>% rustc --pretty normal test.rs &gt; test.rs
</code></pre>

<p>Aside 2: Our pretty printer still has some bugs in it, which I filed:
<a href="https://github.com/rust-lang/rust/issues/19075">19075</a> and
<a href="https://github.com/rust-lang/rust/issues/19077">19077</a>.  Fortunately it was
pretty simple to fix those cases by hand in the generated <code>std.rs</code>, and odds
are good they&rsquo;ll be fixed by the time you might use this process.</p>

<p>Next up, we need a driver script. We can adapt our one from before. The only
special consideration is that we need to make sure to only exit with a return
code of 0 if the version of <code>std.rs</code> we&rsquo;re compiling errors with <code>EPROTOTYPE</code>:</p>

<pre><code class="bash">#!/bin/sh

rustc \
  -A unused_imports \
  -A unused_unsafe \
  -A non_camel_case_types \
  -A unused_variables \
  --test \
  -o test \
  test.rs &gt;/dev/null 2&gt;&amp;1
if [ "$?" -ne "0" ]; then
  exit 1
fi

root=`dirname $0`

for i in $(seq 1 1000); do
  $root/timeout.sh 5 ./test --nocapture tcp::test::write_close_ip4 2&gt;&amp;1 | tee log
  grep "Protocol wrong type for socket" log
  RET="$?"
  if [ "$RET" -eq "0" ]; then
          exit 0
  elif [ "$RET" -eq "143" ]; then
          exit 1
  fi
  echo
done

exit 1
</code></pre>

<p>I used the helper script
<a href="http://www.ict.griffith.edu.au/anthony/software/timeout.sh">timeout.sh</a> to
time out tests in case C-Reduce accidentally made us an infinite loop.</p>

<p>Finally we&rsquo;re ready to start running C-Reduce:</p>

<pre><code>% creduce ./driver.sh test.rs
===&lt; 30598 &gt;===
running 8 interestingness test(s) in parallel
===&lt; pass_blank :: 0 &gt;===
(0.0 %, 156170 bytes)
===&lt; pass_clang_binsrch :: replace-function-def-with-decl &gt;===
===&lt; pass_clang_binsrch :: remove-unused-function &gt;===
===&lt; pass_lines :: 0 &gt;===
(-0.6 %, 157231 bytes)
(1.2 %, 154323 bytes)
(3.7 %, 150455 bytes)
(4.6 %, 149074 bytes)
(5.5 %, 147639 bytes)
(6.4 %, 146172 bytes)
(7.3 %, 144881 bytes)
(7.7 %, 144187 bytes)
(9.1 %, 141936 bytes)
(9.9 %, 140706 bytes)
(10.3 %, 140104 bytes)
(10.4 %, 139998 bytes)
...
</code></pre>

<p>I let it run in the background for sometime in the background while I did some
other things. When I got back, C-Reduce automatically reduced the file from
153KB to a slim 22KB. I then reran rust with the lints enabled to manually cut
out the dead code C-Reduce failed to remove, and flattened away some
unnecessary structs and methods. I was finally left with:</p>

<pre><code class="rust">extern crate libc;

use std::io::net::ip::Ipv4Addr;
use std::io::net::ip::SocketAddr;
use std::io::net::ip::ToSocketAddr;
use std::io::net::ip;
use std::io::test::next_test_ip4;
use std::mem;
use std::num::Int;
use std::os::errno;
use std::os::error_string;
use std::ptr;

fn bind(addr: ip::SocketAddr) -&gt; libc::c_int {
    unsafe {
        let fd = match libc::socket(libc::AF_INET, libc::SOCK_STREAM, 0) {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        };

        let mut storage = mem::zeroed();
        let len = addr_to_sockaddr(addr, &amp;mut storage);
        let addrp = &amp;storage as *const _ as *const libc::sockaddr;
        match libc::bind(fd, addrp, len) {
            -1 =&gt; panic!(),
            _ =&gt; fd,
        }
    }
}

fn listen(fd: libc::c_int, backlog: int) {
    unsafe {
        match libc::listen(fd, backlog as libc::c_int) {
            -1 =&gt; panic!(),
            _ =&gt; {}
        }
    }
}

fn accept(fd: libc::c_int) -&gt; libc::c_int {
    unsafe {
        let x = libc::accept(fd, ptr::null_mut(), ptr::null_mut());
        match x {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        }
    }
}

fn connect(addr: SocketAddr) -&gt; libc::c_int {
    unsafe {
        let fd = match libc::socket(libc::AF_INET, libc::SOCK_STREAM, 0) {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        };

        let mut storage = mem::zeroed();
        let len = addr_to_sockaddr(addr, &amp;mut storage);
        let addrp = &amp;storage as *const _ as *const libc::sockaddr;
        let x = libc::connect(fd, addrp, len);
        fd
    }
}

fn write(fd: libc::c_int, buf: &amp;[u8]) -&gt; Result&lt;(), uint&gt; {
    unsafe {
        let len = buf.len();
        let ret = libc::send(fd, buf.as_ptr() as *const _, len as libc::size_t, 0) as i64;
        if ret &lt; 0 {
            Err(errno())
        } else {
            Ok(())
        }
    }
}

fn close(fd: libc::c_int) {
    unsafe {
        let x = libc::close(fd);
        assert_eq!(x, 0);
    }
}

fn addr_to_sockaddr(addr: SocketAddr, storage: &amp;mut libc::sockaddr_storage) -&gt; libc::socklen_t {
    let inaddr = match addr.ip {
        Ipv4Addr(a, b, c, d) =&gt; {
            let ip = a as u32 &lt;&lt; 24 | b as u32 &lt;&lt; 16 | c as u32 &lt;&lt; 8 | d as u32 &lt;&lt; 0;
            libc::in_addr {
                s_addr: Int::from_be(ip),
            }
        }
        _ =&gt; panic!(),
    };
    unsafe {
        let storage = storage as *mut _ as *mut libc::sockaddr_in;
        (*storage).sin_family = libc::AF_INET as libc::sa_family_t;
        (*storage).sin_port = addr.port.to_be();
        (*storage).sin_addr = inaddr;
        let len = mem::size_of::&lt;libc::sockaddr_in&gt;();
        len as libc::socklen_t
    }
}

fn main() {
    let addr = next_test_ip4();

    let listener = bind(addr);
    listen(listener, 128);

    spawn(proc() {
        let addresses = addr.to_socket_addr_all().unwrap();
        for addr in addresses.into_iter() {
            let fd = connect(addr);
            close(fd);
            return;
        }
    });

    let stream = accept(listener);
    loop {
        let x = write(stream, [0]);
        match x {
            Ok(..) =&gt; { }
            Err(e) =&gt; {
                let e = e as i32;
                assert!(
                    e == libc::ECONNREFUSED ||
                    e == libc::EPIPE ||
                    e == libc::ECONNABORTED,
                    "unknown error: {} {}", e, error_string(e as uint));
                break;
            }
        }
    }

    close(stream);
    close(listener);
}
</code></pre>

<p>This snippet reproduces the same <code>EPROTOTYPE</code> that we started with at the top
of the post. Pretty cool that we got here without much effort?</p>

<p>Now At this point you might say to yourself that couldn&rsquo;t I have extracted this
out myself?  And yeah, you would be right. This is a pretty much a c-in-rust
implementation of this bug. But what&rsquo;s great about using C-Reduce here is that
I only had to make some very rough guesses about what files were and were not
important to include in my <code>test.rs</code>. Eventually when we get some rust plugins
written for C-Reduce I probably could just point it at the whole <code>libstd</code> let
C-Reduce do it&rsquo;s thing. Doing this by hand can be a pretty long and painful
manual process, especially if we&rsquo;re trying to debug a codegen or runtime bug.
In the past I&rsquo;ve spent hours reducing some codegen bugs down into a small
snippet that C-Reduce was also able to produce in a couple minutes.</p>

<p>The last step with this code was to eliminate Rust from the picture, and
translate this code into C:</p>

<pre><code class="c">#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;strings.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;pthread.h&gt;
#include &lt;errno.h&gt;

int do_server() {
  int fd;
  struct sockaddr_in server_addr;

  fd = socket(AF_INET, SOCK_STREAM, 0);

  if (fd == -1) {
    perror("error socket server");
    exit(1);
  }

  bzero((char*) &amp;server_addr, sizeof(server_addr));
  server_addr.sin_family = AF_INET;
  server_addr.sin_addr.s_addr = INADDR_ANY;
  server_addr.sin_port = htons(9600);

  if (bind(fd, (struct sockaddr *) &amp;server_addr, sizeof(server_addr)) &lt; 0) {
    perror("error binding");
    exit(1);
  }

  return fd;
}

void* do_child_thread(void* unused) {
  struct sockaddr_in client_addr;
  int fd;

  fd = socket(AF_INET, SOCK_STREAM, 0);

  if (fd == -1) {
    perror("error socket client");
    exit(1);
  }

  bzero((char*) &amp;client_addr, sizeof(client_addr));
  client_addr.sin_family = AF_INET;
  client_addr.sin_addr.s_addr = INADDR_ANY;
  client_addr.sin_port = htons(9600);

  if (connect(fd, (struct sockaddr *) &amp;client_addr, sizeof(client_addr)) &lt; 0) {
    perror("error connect");
    exit(1);
  }

  fprintf(stderr, "closing client socket\n");

  if (close(fd) &lt; 0) {
    perror("error close client socket");
    exit(1);
  }

  fprintf(stderr, "closed client socket\n");

  return NULL;
}

int main(int argc, char** argv) {
  int server_fd, client_fd;
  socklen_t client_len;
  struct sockaddr_in client_addr;
  char buf[] = { 'a', '\n' };
  pthread_t child_thread;
  int rc;

  signal(SIGPIPE, SIG_IGN);

  server_fd = do_server();
  rc = listen(server_fd, 5);
  if (rc &lt; 0) {
    perror("error listen");
    return 1;
  }

  rc = pthread_create(&amp;child_thread, NULL, do_child_thread, NULL);
  if (rc != 0) {
    perror("error pthread_create");
    return 1;
  }

  client_len = sizeof(client_addr);
  client_fd = accept(server_fd, (struct sockaddr *) &amp;client_addr, &amp;client_len);
  if (client_fd &lt; 0) {
    perror("error accept");
    return 1;
  }

  while (1) {
    fprintf(stderr, "before send\n");
    rc = send(client_fd, buf, sizeof(buf), 0);
    fprintf(stderr, "after send: %d\n", rc);

    if (rc &lt; 0) {
      if (errno == EPIPE) {
        break;
      } else {
        int so_type;
        socklen_t so_len = sizeof(so_type);
        getsockopt(client_fd, SOL_SOCKET, SO_TYPE, &amp;so_type, &amp;so_len);
        fprintf(stderr, "type: %d %d\n", so_type, SOCK_STREAM);

        perror("error send");
        return 1;
      }
    }
  }

  fprintf(stderr, "before server closing client fd\n");
  if (close(client_fd) &lt; 0) {
    perror("error close client");
    return 1;
  }
  fprintf(stderr, "after server closing client fd\n");


  fprintf(stderr, "before server closing fd\n");
  if (close(server_fd) &lt; 0) {
    perror("error close server");
    return 1;
  }
  fprintf(stderr, "after server closing fd\n");

  rc = pthread_join(child_thread, NULL);
  if (rc != 0 &amp;&amp; rc != ESRCH) {
    fprintf(stderr, "error pthread_join: %d\n", rc);
    return 1;
  }

  return 0;
}
</code></pre>

<p>This also produces <code>EPROTOTYPE</code>, so we can eliminate Rust altogther. But lets
keep digging. What exactly is producing this error? If I was on Linux, I&rsquo;d use
<code>strace</code>, but that program isn&rsquo;t on Macs. There&rsquo;s a similar tool called
<code>dtruss</code>, but that seemed to slow things down enough that the <code>EPROTOTYPE</code>
never happened. Fortunately though there is another program called <code>errinfo</code>,
that just prints the <code>errno</code> along with every syscall. In one terminal I ran
<code>while ./test; do sleep 0.1; done</code>. In the other:</p>

<pre><code>% sudo errinfo -n test
...
           a.out           stat64    0
           a.out             open    0
           a.out psynch_mutexwait    0
           a.out   write_nocancel    0
           a.out           sendto    0
           a.out   write_nocancel    0
           a.out   write_nocancel    0
           a.out           sendto   41  Protocol wrong type for socket
           a.out   write_nocancel    0
           a.out       getsockopt    0
...
</code></pre>

<p>Right there we see our <code>sendto</code> syscall is actually returning the <code>EPROTOTYPE</code>.
This <code>errno</code> then is definitely being created inside the OSX kernel, not in any
userspace code. Fortunately, most of the Apple kernel, XNU, is open sourced, so
we can dig down to what&rsquo;ll be the my last layer. You can find the tarballs at
<a href="http://www.opensource.apple.com/.">http://www.opensource.apple.com/.</a> But I&rsquo;d rather use the
<a href="https://github.com/opensource-apple/xnu">unoffical GitHub repository</a>. Using GitHub&rsquo;s
search tools, We can find all 17 instances of
<a href="https://github.com/opensource-apple/xnu/search?l=c&amp;q=EPROTOTYPE&amp;utf8=%E2%9C%93">EPROTOTYPE</a>
in the codebase. Now I don&rsquo;t know the XNU codebase, but there are still some
really interesting things we can find. The first is in
<a href="https://github.com/opensource-apple/xnu/blob/bb7368935f659ada117c0889612e379c97eb83b3/bsd/kern/uipc_usrreq.c#L408">bsd/kern/uipc_usrreq.c</a>:</p>

<pre><code class="c">/*
 * Returns:  0      Success
 *    EINVAL
 *    EOPNOTSUPP
 *    EPIPE
 *    ENOTCONN
 *    EISCONN
 *  unp_internalize:EINVAL
 *  unp_internalize:EBADF
 *  unp_connect:EAFNOSUPPORT  Address family not supported
 *  unp_connect:EINVAL        Invalid argument
 *  unp_connect:ENOTSOCK      Not a socket
 *  unp_connect:ECONNREFUSED  Connection refused
 *  unp_connect:EISCONN       Socket is connected
 *  unp_connect:EPROTOTYPE    Protocol wrong type for socket
 *  unp_connect:???
 *  sbappendaddr:ENOBUFS      [5th argument, contents modified]
 *  sbappendaddr:???          [whatever a filter author chooses]
 */
static int
uipc_send(struct socket *so, int flags, struct mbuf *m, struct sockaddr *nam,
    struct mbuf *control, proc_t p)
{
...
</code></pre>

<p>Hey look at that! There&rsquo;s handler for the <code>send</code> syscall (although for IPC, not
TCP) that actually documents that it can return <code>EPROTOTYPE</code>! While it doesn&rsquo;t
explain exactly how this can happen, the fact it mentions <code>unp_connect</code> hints
that <code>uipc_send</code> may trigger a connect, and that&rsquo;s exactly what we find a
couple lines into the function:</p>

<pre><code class="c">...
    /* Connect if not connected yet. */
    /*
     * Note: A better implementation would complain
     * if not equal to the peer's address.
     */
    if ((so-&gt;so_state &amp; SS_ISCONNECTED) == 0) {
      if (nam) {
        error = unp_connect(so, nam, p);
        if (error)
          break;  /* XXX */
      } else {
        error = ENOTCONN;
        break;
      }
    }
...
</code></pre>

<p>The fact that the comment says the socket might not be connected yet when we&rsquo;re
doing a <code>send</code> hints that Apple may have introduced some level of asynchrony
and preemption to sockets. So if we trigger the actual connect here, it could
then return <code>EPROTOTYPE</code>, which makes sense. Unfortunately that&rsquo;s still not
quite the behavior we&rsquo;re seeing. We&rsquo;re not getting <code>EPROTOTYPE</code> on our first
write, but after we&rsquo;ve done a couple.</p>

<p>I believe we find that behavior in the actual TCP syscall file,
<a href="https://github.com/opensource-apple/xnu/blob/bb7368935f659ada117c0889612e379c97eb83b3/bsd/netinet/tcp_usrreq.c#L914-L948">bsd/netinet/tcp_usrreq.c</a>:</p>

<pre><code class="c">static int
tcp_usr_send(struct socket *so, int flags, struct mbuf *m,
     struct sockaddr *nam, struct mbuf *control, struct proc *p)
{
  int error = 0;
  struct inpcb *inp = sotoinpcb(so);
  struct tcpcb *tp;
  uint32_t msgpri = MSG_PRI_DEFAULT;
#if INET6
  int isipv6;
#endif
  TCPDEBUG0;

  if (inp == NULL || inp-&gt;inp_state == INPCB_STATE_DEAD
#if NECP
    || (necp_socket_should_use_flow_divert(inp))
#endif /* NECP */
    ) {
    /*
     * OOPS! we lost a race, the TCP session got reset after
     * we checked SS_CANTSENDMORE, eg: while doing uiomove or a
     * network interrupt in the non-splnet() section of sosend().
     */
    if (m != NULL)
      m_freem(m);
    if (control != NULL) {
      m_freem(control);
      control = NULL;
    }

    if (inp == NULL)
      error = ECONNRESET;  /* XXX EPIPE? */
    else
      error = EPROTOTYPE;
    tp = NULL;
    TCPDEBUG1();
    goto out;
  }
...
</code></pre>

<p>I believe that comment explains everything we&rsquo;re seeing. If we trigger a <code>send</code>
while the kernel is in the middle of tearing down the socket, it returns
<code>EPROTOTYPE</code>. This then looks to be an error we could retry. Once the socket is
fully torn down, it should eventually return the proper <code>EPIPE</code>. This is also
pretty easy to test. So I modified the inner loop of our C test:</p>

<pre><code>  while (1) {
    fprintf(stderr, "before send\n");
    rc = send(client_fd, buf, sizeof(buf), 0);
    fprintf(stderr, "after send: %d\n", rc);

    if (rc &lt; 0) {
      if (errno == EPIPE) {
        break;
      } else if (errno == EPROTOTYPE) {
        continue;
      } else {
        int so_type;
        socklen_t so_len = sizeof(so_type);
        getsockopt(client_fd, SOL_SOCKET, SO_TYPE, &amp;so_type, &amp;so_len);
        fprintf(stderr, "type: %d %d\n", so_type, SOCK_STREAM);

        perror("error send");
        return 1;
      }
    }
  }
</code></pre>

<p>And yep, it exits cleanly. After all of this, I think it&rsquo;s pretty clear at this
point that there&rsquo;s no weird kernel corruption bug going on, just a poorly
documented edge case. But it sure was fun chasing this condition through the
system.</p>

<p>To prevent anyone else from tripping over this edge case, I filed a Apple Radar
ticket (number #19012087 for any Apple employees reading this). Hopefully if
anyone runs into this mysterious <code>EPROTOTYPE</code> it&rsquo;ll be documented for them, or
at least there&rsquo;s a chance they&rsquo;ll stumble over this blog post and save
themselves a weekend diving through the OS.</p>
]]></content>
  </entry>
  
</feed>
