<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Rust | Tilting at Rabbit Holes]]></title>
  <link href="http://erickt.github.io/blog/categories/rust/atom.xml" rel="self"/>
  <link href="http://erickt.github.io/"/>
  <updated>2014-11-24T14:00:31-08:00</updated>
  <id>http://erickt.github.io/</id>
  <author>
    <name><![CDATA[Erick Tryzelaar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking Is Confusing in Low Level Rust]]></title>
    <link href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/"/>
    <updated>2014-11-22T12:10:29-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing</id>
    <content type="html"><![CDATA[<p>Edit: you can find all the code in this post
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">here</a>,
and I filed <a href="https://github.com/rust-lang/rust/issues/19281">19281</a> for the
regression I mentioned at the end of the post.</p>

<hr />

<p>Low level benchmarking is confusing and non-intuitive.</p>

<p>The end.</p>

<hr />

<p>Or not. Whatever. So I&rsquo;m trying to get my
implement-<code>Reader</code>-and-<code>Writer</code>-for-<code>&amp;[u8]</code> type PR
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a> landed. But
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63925495">Steven Fackler</a>
obnixously and correctly pointed out that this won&rsquo;t play that nicely with the
new <code>Reader</code> and <code>Writer</code> implementation for <code>Vec&lt;u8&gt;</code>. Grumble grumble. And then
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63927659">Alex Crichton</a>
had the gall to mention that a <code>Writer</code> for <code>mut &amp;[u8]</code> also probably won&rsquo;t be
that common either. Sure, he&rsquo;s write and all, but but I got it working without
needing an index! That means that the <code>&amp;mut [u8]</code> <code>Writer</code> only needs 2
pointers instead of <code>BufWriter</code>&rsquo;s three, so it just has to be faster! Well,
doesn&rsquo;t it?</p>

<p>Stupid benchmarks.</p>

<p>I got to say it&rsquo;s pretty addicting writing micro-benchmarks. It&rsquo;s a lot of fun
seeing how sensitive low-level code can be to just the smallest of tweaks. It&rsquo;s
also really annoying when you write something you think is pretty neat, then
you find it&rsquo;s chock-full of false dependencies between cache lines, or other
mean things CPUs like to impose on poor programmers.</p>

<p>Anyway, to start lets look at what should be the fastest way to write to a
buffer. Completely unsafely with no checks.</p>

<pre><code class="rust">unsafe fn do_copy_nonoverlapping_memory(
  mut dst: *mut u8,
  src: *const u8,
  len: uint,
  batches: uint
) {
    for _ in range(0, batches) {
        ptr::copy_nonoverlapping_memory(dst, src, len);
        dst = dst.offset(len as int);
    }
}

#[test]
fn test_copy_nonoverlapping_memory() {
    let dst = &amp;mut [0_u8, .. BATCHES * SRC_LEN];
    let src = &amp;[1, .. SRC_LEN];

    unsafe {
        do_copy_nonoverlapping_memory(
            dst.as_mut_ptr(),
            src.as_ptr(),
            src.len(),
            BATCHES
        );
    }
    assert!(dst.iter().all(|c| *c == 1));
}
</code></pre>

<p>With <code>SRC_LEN=4</code> and <code>BATCHES=128</code>, we get this. For fun I added the new
<code>libtest</code> from <a href="https://github.com/rust-lang/rust/pull/19233">#19233</a> that will
hopefully land soon. I also added also ran variations that explicitly inlined
and not inlined the inner function:</p>

<pre><code>test bench_copy_nonoverlapping_memory               ... bench: 50 |   [-***#**------]                        | 200:        72 ns/iter (+/- 45)
test bench_copy_nonoverlapping_memory_inline_always ... bench: 50 |       [---***#****************----------]| 100:        65 ns/iter (+/- 39)
test bench_copy_nonoverlapping_memory_inline_never  ... bench: 500 |      [------********#*********-------] | 1000:       747 ns/iter (+/- 393)
</code></pre>

<p>So overall it does quite well. Now lets compare with the code I wrote:</p>

<pre><code class="rust">impl&lt;'a&gt; Writer for &amp;'a mut [u8] {
    #[inline]
    fn write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        if self.is_empty() { return Err(io::standard_error(io::EndOfFile)); }

        let dst_len = self.len();
        let src_len = src.len();

        let write_len = min(dst_len, src_len);

        slice::bytes::copy_memory(*self, src.slice_to(write_len));

        unsafe {
            *self = mem::transmute(raw::Slice {
                data: self.as_ptr().offset(write_len as int),
                len: dst_len - write_len,
            });
        }

        if src_len &gt; dst_len {
            Err(io::standard_error(io::ShortWrite(write_len)))
        } else {
            Ok(())
        }
    }
}
</code></pre>

<p>And. Well. It didn&rsquo;t do that well.</p>

<pre><code>test writer::bench_slice_writer                             ... bench: 500 |   [------**#**--]                      | 2000:       920 ns/iter (+/- 448)
test writer::bench_slice_writer_inline_always               ... bench: 600 | [-**#*****---]                         | 2000:       711 ns/iter (+/- 405)
test writer::bench_slice_writer_inline_never                ... bench: 600 |   [-***#******---]                     | 2000:       838 ns/iter (+/- 474)
</code></pre>

<p>Wow. That&rsquo;s pretty bad compared to the ideal.</p>

<p>Crud. So not only did I add an implementation that&rsquo;s probably going to not work
with <code>write!</code> and now it turns out the performance is pretty terrible. Inlining
isn&rsquo;t helping like it did in the unsafe case. So how&rsquo;s
<a href="https://github.com/rust-lang/rust/blob/master/src/libstd/io/mem.rs#L219">std::io::BufWriter</a>
compare?</p>

<pre><code class="rust">pub struct BufWriter&lt;'a&gt; {
    buf: &amp;'a mut [u8],
    pos: uint
}

impl&lt;'a&gt; Writer for BufWriter&lt;'a&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        // return an error if the entire write does not fit in the buffer
        let cap = if self.pos &gt;= self.buf.len() { 0 } else { self.buf.len() - self.pos };
        if buf.len() &gt; cap {
            return Err(IoError {
                kind: io::OtherIoError,
                desc: "Trying to write past end of buffer",
                detail: None
            })
        }

        slice::bytes::copy_memory(self.buf[mut self.pos..], buf);
        self.pos += buf.len();
        Ok(())
    }
}
</code></pre>

<p>Here&rsquo;s how it does:</p>

<pre><code>test writer::bench_std_buf_writer                           ... bench: 50 |     [------**************#******-------] | 100:        79 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_always             ... bench: 50 |    [-----************#*********-------]  | 100:        75 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_never              ... bench: 600 |   [#****-----]                         | 2000:       705 ns/iter (+/- 337)
</code></pre>

<p>That&rsquo;s just cruel. The optimization gods obviously hate me. So I started
playing with a lot of
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">variations</a>
(it&rsquo;s my yeah yeah it&rsquo;s my serialization benchmark suite, I&rsquo;m planning on
making it more general purpose. Besides it&rsquo;s my suite and I can do whatever I
want with it, so there):</p>

<ul>
<li>(BufWriter0): Turning this <code>Writer</code> into a struct wrapper shouldn&rsquo;t do anything, and it
didn&rsquo;t.</li>
<li>(BufWriter1): There&rsquo;s error handling, does removing it help? Nope!</li>
<li>(BufWriter5): There&rsquo;s an implied branch in <code>let write_len = min(dst_len, src_len)</code>. We can
turn that into the branch-predictor-friendly:</li>
</ul>


<pre><code class="rust">let x = (dst_len &lt; buf_len) as uint;
let write_len = dst_len * x + src_len * (1 - x);
</code></pre>

<p>Doesn&rsquo;t matter, still performs the same.</p>

<ul>
<li>(BufWriter2): Fine then, optimization gods! Lets remove the branch altogether and just
  always advance the slice <code>src.len()</code> bytes! Damn the safety! That, of course,
works. I can hear them giggle.</li>
<li>(BufWriter3): Maybe, just maybe there&rsquo;s something weird going on with
  inlining across crates? Lets copy <code>std::io::BufWriter</code> and make sure that
  it&rsquo;s still nearly optimal. It still is.</li>
<li>(BufWriter6): Technically the <code>min(dst_len, src_len)</code> is a bounds check, so
we could switch from the bounds checked <code>std.slice::bytes::copy_memory</code> to
  the unsafe <code>std::ptr::copy_nonoverlapping_memory</code>, but that also doesn&rsquo;t
  help.</li>
<li>(BufWriter7): Might as well and apply the last trick to <code>std::io::BufWriter</code>,
  and it does shave a couple nanoseconds off. It might be worth pushing it
  upstream:</li>
</ul>


<pre><code>test writer::bench_buf_writer_7                             ... bench: 50 |   [-*#********--------------------------]| 100:        55 ns/iter (+/- 44)
test writer::bench_buf_writer_7_inline_always               ... bench: 50 |     [---------********#*********-------] | 100:        76 ns/iter (+/- 39)
test writer::bench_buf_writer_7_inline_never                ... bench: 600 |   [-***#****----]                      | 2000:       828 ns/iter (+/- 417)
</code></pre>

<ul>
<li>(BufWriter4): While I&rsquo;m using one less <code>uint</code> than <code>std::io::BufWriter</code>, I&rsquo;m
  doing two writes to advance my slice, one to advance the pointer, and one to
  shrink the length. <code>std::io::BufWriter</code> only has to advance it&rsquo;s <code>pos</code> index.
  But in this case if instead of treating the slice as a <code>(ptr, length)</code>, we
  can convert it into a <code>(start_ptr, end_ptr)</code>, where <code>start_ptr=ptr</code>, and
  <code>end_ptr=ptr+length</code>. This works! Ish:</li>
</ul>


<pre><code>test writer::bench_buf_writer_4                             ... bench: 80 |   [--******#*******-----]                | 200:       109 ns/iter (+/- 59)
test writer::bench_buf_writer_4_inline_always               ... bench: 100 |     [------***#******---]               | 200:       133 ns/iter (+/- 44)
test writer::bench_buf_writer_4_inline_never                ... bench: 500 |      [-----***********#***********----]| 1000:       778 ns/iter (+/- 426)
</code></pre>

<p>I know when I&rsquo;m defeated. Oh well. I guess I can at least update
<code>std::io::BufWriter</code> to support the new error handling approach:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter10&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if self.pos == dst_len { return Err(io::standard_error(io::EndOfFile)); }

        let src_len = src.len();
        let cap = dst_len - self.pos;

        let write_len = min(cap, src_len);

        slice::bytes::copy_memory(self.dst[mut self.pos..], src[..write_len]);

        if src_len &gt; dst_len {
            return Err(io::standard_error(io::ShortWrite(write_len)));
        }

        self.pos += write_len;
        Ok(())
    }
}
</code></pre>

<p>How&rsquo;s it do?</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 600 | [----**#***--]                         | 2000:       841 ns/iter (+/- 413)
test writer::bench_buf_writer_10_inline_always              ... bench: 600 |  [----**#***--]                        | 2000:       872 ns/iter (+/- 387)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--******#**---]                     | 2000:       960 ns/iter (+/- 486)
</code></pre>

<p>Grumble grumble. It turns out that if we tweak the <code>copy_memory</code> line to:</p>

<pre><code class="rust">        slice::bytes::copy_memory(self.dst[mut self.pos..], src);
</code></pre>

<p>It shaves 674 nanoseconds off the run:</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 200 |[---***#************------------]        | 400:       230 ns/iter (+/- 147)
test writer::bench_buf_writer_10_inline_always              ... bench: 200 |   [-----********#*******------]         | 400:       280 ns/iter (+/- 128)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--***#****----]                     | 2000:       885 ns/iter (+/- 475)
</code></pre>

<p>But still no where near where we need to be. That suggests though that always
cutting down the <code>src</code>, which triggers another bounds check has some measurable
impact. So maybe I should only shrink the <code>src</code> slice when we know it needs to
be shrunk?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter11&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst = self.dst[mut self.pos..];
        let dst_len = dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);
            }

                self.pos += src_len;

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);
            }

                self.pos += dst_len;

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>Lets see how it failed this time&hellip;</p>

<pre><code>test writer::bench_buf_writer_11                            ... bench: 60 |      [------********#*********-----]     | 100:        79 ns/iter (+/- 28)
test writer::bench_buf_writer_11_inline_always              ... bench: 60 |[-------******#*************-----------]  | 100:        72 ns/iter (+/- 35)
test writer::bench_buf_writer_11_inline_never               ... bench: 600 |  [--***#***----]                       | 2000:       835 ns/iter (+/- 423)
</code></pre>

<p>No way. That actually worked?! That&rsquo;s awesome! I&rsquo;ll carve that out into another
PR. Maybe it&rsquo;ll work for my original version that doesn&rsquo;t use a <code>pos</code>?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter12&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(src_len as int),
                    len: dst_len - src_len,
                });
            }

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(dst_len as int),
                    len: 0,
                });
            }

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>And yep, just as fast!</p>

<pre><code>test writer::bench_buf_writer_12                            ... bench: 50 |[**#*----------------------------------]   | 80:        51 ns/iter (+/- 26)
test writer::bench_buf_writer_12_inline_always              ... bench: 50 |  [--------**********#********------------]| 90:        69 ns/iter (+/- 36)
test writer::bench_buf_writer_12_inline_never               ... bench: 800 |  [---**#***-]                          | 2000:      1000 ns/iter (+/- 263)
</code></pre>

<p>At this point, both solutions are approximately just as fast as the unsafe
<code>ptr::copy_nonoverlapping_memory</code>! So that&rsquo;s awesome. Now would anyone really
care enough about the extra <code>uint</code>?  There may be a few very specialized cases
where that extra <code>uint</code> could cause a problem, but I&rsquo;m not sure if it&rsquo;s worth
it. What do you all think?</p>

<hr />

<p>I thought that was good, but since I&rsquo;m already here, how&rsquo;s the new <code>Vec&lt;u8&gt;</code>
writer doing? Here&rsquo;s the driver:</p>

<pre><code class="rust">impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_std_vec_writer(b: &amp;mut test::Bencher) {
    let mut dst = Vec::with_capacity(BATCHES * SRC_LEN);
    let src = &amp;[1, .. SRC_LEN];

    b.iter(|| {
        dst.clear();

        do_std_writes(&amp;mut dst, src, BATCHES);
    })
}
</code></pre>

<p>And the results:</p>

<pre><code>test writer::bench_std_vec_writer                           ... bench: 1000 | [----*****#*****--------]             | 2000:      1248 ns/iter (+/- 588)
test writer::bench_std_vec_writer_inline_always             ... bench: 900 |   [----*#***--]                        | 2000:      1125 ns/iter (+/- 282)
test writer::bench_std_vec_writer_inline_never              ... bench: 1000 |  [----***#*****--------]              | 2000:      1227 ns/iter (+/- 516)
</code></pre>

<p>Wow. That&rsquo;s pretty terrible. Something weird must be going on with
<code>Vec::push_all</code>. (Maybe that&rsquo;s what caused my serialization benchmarks to slow
1/3?). Lets skip it:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for VecWriter1&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let src_len = src.len();

        self.dst.reserve(src_len);

        let dst = self.dst.as_mut_slice();

        unsafe {
            // we reserved enough room in `dst` to store `src`.
            ptr::copy_nonoverlapping_memory(
                dst.as_mut_ptr(),
                src.as_ptr(),
                src_len);
        }

        Ok(())
    }
}
</code></pre>

<p>And it looks a bit better, but not perfect:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 100 |         [------*********#*****--------] | 200:       160 ns/iter (+/- 68)
test writer::bench_vec_writer_1_inline_always               ... bench: 100 |     [--------****#**--]                 | 300:       182 ns/iter (+/- 79)
test writer::bench_vec_writer_1_inline_never                ... bench: 600 |   [---****#**--]                       | 2000:       952 ns/iter (+/- 399)
</code></pre>

<p>There&rsquo;s even less going on here than before. The only difference is that
reserve call. Commenting it out gets us back to <code>copy_nonoverlapping_memory</code>
territory:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 70 | [----------------*********#******-------]| 100:        89 ns/iter (+/- 27)
test writer::bench_vec_writer_1_inline_always               ... bench: 50 |   [-***#******--]                        | 200:        75 ns/iter (+/- 46)
test writer::bench_vec_writer_1_inline_never                ... bench: 500 |   [--***#***---]                       | 2000:       775 ns/iter (+/- 433)
</code></pre>

<p>Unfortunately it&rsquo;s getting pretty late, so rather than wait until the next time
to dive into this, I&rsquo;ll leave it up to you all. Does anyone know why <code>reserve</code>
is causing so much trouble here?</p>

<hr />

<p>PS: While I was working on this, I saw
<a href="https://github.com/erickt/rust-serialization-benchmarks/pull/2">stevencheg</a>
submitted a patch to speed up the protocol buffer support. But when I ran the
tests, everything was about 40% slower than the last benchmark
<a href="http://erickt.github.io/blog/2014/11/13/benchmarks-2/">post</a>! Something
happened with Rust&rsquo;s performance over these past couple weeks!</p>

<pre><code>test goser::bincode::bench_decoder                          ... bench:      7682 ns/iter (+/- 3680) = 52 MB/s
test goser::bincode::bench_encoder                          ... bench:       516 ns/iter (+/- 265) = 775 MB/s
test goser::bincode::bench_populate                         ... bench:      1504 ns/iter (+/- 324)
test goser::capnp::bench_deserialize                        ... bench:       251 ns/iter (+/- 140) = 1784 MB/s
test goser::capnp::bench_deserialize_packed_unbuffered      ... bench:      1344 ns/iter (+/- 533) = 250 MB/s
test goser::capnp::bench_populate                           ... bench:       663 ns/iter (+/- 236)
test goser::capnp::bench_serialize                          ... bench:       144 ns/iter (+/- 37) = 3111 MB/s
test goser::capnp::bench_serialize_packed_unbuffered        ... bench:       913 ns/iter (+/- 436) = 369 MB/s
test goser::msgpack::bench_decoder                          ... bench:      3411 ns/iter (+/- 1837) = 84 MB/s
test goser::msgpack::bench_encoder                          ... bench:       961 ns/iter (+/- 477) = 298 MB/s
test goser::msgpack::bench_populate                         ... bench:      1564 ns/iter (+/- 453)
test goser::protobuf::bench_decoder                         ... bench:      3116 ns/iter (+/- 1485) = 91 MB/s
test goser::protobuf::bench_encoder                         ... bench:      1220 ns/iter (+/- 482) = 234 MB/s
test goser::protobuf::bench_populate                        ... bench:       942 ns/iter (+/- 836)
test goser::serialize_json::bench_decoder                   ... bench:     31934 ns/iter (+/- 16186) = 18 MB/s
test goser::serialize_json::bench_encoder                   ... bench:      8481 ns/iter (+/- 3392) = 71 MB/s
test goser::serialize_json::bench_populate                  ... bench:      1471 ns/iter (+/- 426)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Chasing an EPROTOTYPE Through Rust, Sendto, and the OSX Kernel With C-Reduce]]></title>
    <link href="http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/"/>
    <updated>2014-11-19T07:35:04-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug</id>
    <content type="html"><![CDATA[<p>A slight diversion from my serialization series. Last week, strcat submitted
<a href="https://github.com/rust-lang/rust/pull/18885">#18885</a> pull request, which adds
support for using a <code>Vec</code> as a <code>Writer</code>. Over the weekend I submitted
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a>, which allows a <code>&amp;[u8]</code>
to be used as a <code>Reader</code>. Overall a pretty simple change. However, when I was
running the test suite, I found that the <code>std::io::net::tcp::write_close_ip4()</code>
test was occasionally failing:</p>

<pre><code class="rust">    #[test]
    fn write_close_ip4() {
        let addr = next_test_ip4();
        let mut acceptor = TcpListener::bind(addr).listen();

        spawn(proc() {
            let _stream = TcpStream::connect(addr);
            // Close
        });

        let mut stream = acceptor.accept();
        let buf = [0];
        loop {
            match stream.write(buf) {
                Ok(..) =&gt; {}
                Err(e) =&gt; {
                    assert!(e.kind == ConnectionReset ||
                            e.kind == BrokenPipe ||
                            e.kind == ConnectionAborted,
                            "unknown error: {}", e);
                    break;
                }
            }
        }
    }
</code></pre>

<p>The <code>write</code> would succeed a few times, then occasionally error with the
<code>unknown error: Protocol wrong type for socket</code>, or the <code>EPROTOTYPE</code> errno.
This is a really surprising error. As far as I know, the only functions that
return that errno are <code>socket</code>, <code>socketpair</code>, and <code>connect</code>. I searched
everywhere, but I couldn&rsquo;t find any documentation suggesting that <code>write</code> would
ever produce that error.</p>

<p>I wasn&rsquo;t the only one who ran into it. bjz opened
<a href="https://github.com/rust-lang/rust/issues/18900">#18900</a> describing the same
problem. One interesting thing to note is they were also on OSX Yosemite.  So I
took a little bit of time to extract out that test from the Rust test suite
into this <a href="https://gist.github.com/erickt/ac1f35e20834aa5d1972">gist</a> and got
someone on #rust-internals to run it on linux for me with this little driver
script:</p>

<pre><code class="sh">#!/bin/sh

set -e

rustc test.rs

for i in $(seq 1 1000); do
  ./test tcp::test::write_close_ip4
done
</code></pre>

<p>and it didn&rsquo;t error out. So it seems to be system dependent. Further
experimentation showed that if we introduced sleeps or a mutex synchronization
appeared to fix the problem as well. At this point I wasn&rsquo;t sure if this was a
non-issue, a bug in our code, or a bug in the OS. One things for sure though,
if there is a bug, it could be scattered somewhere across the Rust codebase,
which just <code>std::io</code> alone is 20 files at around 12522 lines. It&rsquo;d be a pain to
cut that down to a self contained test case.</p>

<p>Fortunately we&rsquo;ve got <a href="http://embed.cs.utah.edu/creduce/">C-Reduce</a> to help us
out. Back in May <a href="http://www.cs.utah.edu/~regehr/">Professor John Regehr</a> from
the University of Utah came to our
<a href="http://www.meetup.com/Rust-Bay-Area/events/169434302/">Bay Area Rust meetup</a>
 to talk about compiler testing and fuzzing. We recorded the talk, so if you
want to watch it, you can find it
<a href="https://air.mozilla.org/rust-meetup-may-2014/">here</a>. One of the things he
talked about was the tool C-Reduce his research group developed to
automatically cut
out unnecessary lines of code that can still reproduce a bug you&rsquo;re looking
for. While it&rsquo;s written to target C files, it turns out Rust is syntatically
close enough to C that it works out pretty well for it too. All you need is a
single source file and driver script that&rsquo;ll report if the compiled source file
reproduced the bug.</p>

<p>Aside 1: By the way, one of the other things I did this weekend was I put
together a Homebrew
<a href="https://github.com/Homebrew/homebrew/pull/34220">pull request for C-Reduce</a>.
It hasn&rsquo;t landed yet, but you want to use it, you can do:</p>

<pre><code>% brew install https://raw.githubusercontent.com/erickt/homebrew/delta-and-creduce/Library/Formula/delta.rb
% brew install https://raw.githubusercontent.com/erickt/homebrew/delta-and-creduce/Library/Formula/creduce.rb
</code></pre>

<p>Hopefully it&rsquo;ll land soon so you&rsquo;ll be able to do:</p>

<pre><code>% brew install creduce
</code></pre>

<p>Anyway, back to the story. So we&rsquo;ve got a rather large code base to cover, and
while C-reduce does a pretty good job of trimming away lines, just pointing it
at the entire <code>std</code> module is a bit too much for it to handle in a reasonable
amount of time. It probably needs some more semantic information about Rust to
do a better job of cutting out broad swaths of code.</p>

<p>So I needed to do at least a rough pass to slim down the files. I figured the
problem was probably contained in <code>std::io</code> or <code>std::sys</code>, so I wrote a simple
<code>test.rs</code> that explicitly included those modules as part of the crate (see
this <a href="https://gist.github.com/erickt/ac1f35e20834aa5d1972">gist</a> if you are
interested), and used the pretty printer to merge it into one file:</p>

<pre><code>% rustc --pretty normal test.rs &gt; test.rs
</code></pre>

<p>Aside 2: Our pretty printer still has some bugs in it, which I filed:
<a href="https://github.com/rust-lang/rust/issues/19075">19075</a> and
<a href="https://github.com/rust-lang/rust/issues/19077">19077</a>.  Fortunately it was
pretty simple to fix those cases by hand in the generated <code>std.rs</code>, and odds
are good they&rsquo;ll be fixed by the time you might use this process.</p>

<p>Next up, we need a driver script. We can adapt our one from before. The only
special consideration is that we need to make sure to only exit with a return
code of 0 if the version of <code>std.rs</code> we&rsquo;re compiling errors with <code>EPROTOTYPE</code>:</p>

<pre><code class="bash">#!/bin/sh

rustc \
  -A unused_imports \
  -A unused_unsafe \
  -A non_camel_case_types \
  -A unused_variables \
  --test \
  -o test \
  test.rs &gt;/dev/null 2&gt;&amp;1
if [ "$?" -ne "0" ]; then
  exit 1
fi

root=`dirname $0`

for i in $(seq 1 1000); do
  $root/timeout.sh 5 ./test --nocapture tcp::test::write_close_ip4 2&gt;&amp;1 | tee log
  grep "Protocol wrong type for socket" log
  RET="$?"
  if [ "$RET" -eq "0" ]; then
          exit 0
  elif [ "$RET" -eq "143" ]; then
          exit 1
  fi
  echo
done

exit 1
</code></pre>

<p>I used the helper script
<a href="http://www.ict.griffith.edu.au/anthony/software/timeout.sh">timeout.sh</a> to
time out tests in case C-Reduce accidentally made us an infinite loop.</p>

<p>Finally we&rsquo;re ready to start running C-Reduce:</p>

<pre><code>% creduce ./driver.sh test.rs
===&lt; 30598 &gt;===
running 8 interestingness test(s) in parallel
===&lt; pass_blank :: 0 &gt;===
(0.0 %, 156170 bytes)
===&lt; pass_clang_binsrch :: replace-function-def-with-decl &gt;===
===&lt; pass_clang_binsrch :: remove-unused-function &gt;===
===&lt; pass_lines :: 0 &gt;===
(-0.6 %, 157231 bytes)
(1.2 %, 154323 bytes)
(3.7 %, 150455 bytes)
(4.6 %, 149074 bytes)
(5.5 %, 147639 bytes)
(6.4 %, 146172 bytes)
(7.3 %, 144881 bytes)
(7.7 %, 144187 bytes)
(9.1 %, 141936 bytes)
(9.9 %, 140706 bytes)
(10.3 %, 140104 bytes)
(10.4 %, 139998 bytes)
...
</code></pre>

<p>I let it run in the background for sometime in the background while I did some
other things. When I got back, C-Reduce automatically reduced the file from
153KB to a slim 22KB. I then reran rust with the lints enabled to manually cut
out the dead code C-Reduce failed to remove, and flattened away some
unnecessary structs and methods. I was finally left with:</p>

<pre><code class="rust">extern crate libc;

use std::io::net::ip::Ipv4Addr;
use std::io::net::ip::SocketAddr;
use std::io::net::ip::ToSocketAddr;
use std::io::net::ip;
use std::io::test::next_test_ip4;
use std::mem;
use std::num::Int;
use std::os::errno;
use std::os::error_string;
use std::ptr;

fn bind(addr: ip::SocketAddr) -&gt; libc::c_int {
    unsafe {
        let fd = match libc::socket(libc::AF_INET, libc::SOCK_STREAM, 0) {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        };

        let mut storage = mem::zeroed();
        let len = addr_to_sockaddr(addr, &amp;mut storage);
        let addrp = &amp;storage as *const _ as *const libc::sockaddr;
        match libc::bind(fd, addrp, len) {
            -1 =&gt; panic!(),
            _ =&gt; fd,
        }
    }
}

fn listen(fd: libc::c_int, backlog: int) {
    unsafe {
        match libc::listen(fd, backlog as libc::c_int) {
            -1 =&gt; panic!(),
            _ =&gt; {}
        }
    }
}

fn accept(fd: libc::c_int) -&gt; libc::c_int {
    unsafe {
        let x = libc::accept(fd, ptr::null_mut(), ptr::null_mut());
        match x {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        }
    }
}

fn connect(addr: SocketAddr) -&gt; libc::c_int {
    unsafe {
        let fd = match libc::socket(libc::AF_INET, libc::SOCK_STREAM, 0) {
            -1 =&gt; panic!(),
            fd =&gt; fd,
        };

        let mut storage = mem::zeroed();
        let len = addr_to_sockaddr(addr, &amp;mut storage);
        let addrp = &amp;storage as *const _ as *const libc::sockaddr;
        let x = libc::connect(fd, addrp, len);
        fd
    }
}

fn write(fd: libc::c_int, buf: &amp;[u8]) -&gt; Result&lt;(), uint&gt; {
    unsafe {
        let len = buf.len();
        let ret = libc::send(fd, buf.as_ptr() as *const _, len as libc::size_t, 0) as i64;
        if ret &lt; 0 {
            Err(errno())
        } else {
            Ok(())
        }
    }
}

fn close(fd: libc::c_int) {
    unsafe {
        let x = libc::close(fd);
        assert_eq!(x, 0);
    }
}

fn addr_to_sockaddr(addr: SocketAddr, storage: &amp;mut libc::sockaddr_storage) -&gt; libc::socklen_t {
    let inaddr = match addr.ip {
        Ipv4Addr(a, b, c, d) =&gt; {
            let ip = a as u32 &lt;&lt; 24 | b as u32 &lt;&lt; 16 | c as u32 &lt;&lt; 8 | d as u32 &lt;&lt; 0;
            libc::in_addr {
                s_addr: Int::from_be(ip),
            }
        }
        _ =&gt; panic!(),
    };
    unsafe {
        let storage = storage as *mut _ as *mut libc::sockaddr_in;
        (*storage).sin_family = libc::AF_INET as libc::sa_family_t;
        (*storage).sin_port = addr.port.to_be();
        (*storage).sin_addr = inaddr;
        let len = mem::size_of::&lt;libc::sockaddr_in&gt;();
        len as libc::socklen_t
    }
}

fn main() {
    let addr = next_test_ip4();

    let listener = bind(addr);
    listen(listener, 128);

    spawn(proc() {
        let addresses = addr.to_socket_addr_all().unwrap();
        for addr in addresses.into_iter() {
            let fd = connect(addr);
            close(fd);
            return;
        }
    });

    let stream = accept(listener);
    loop {
        let x = write(stream, [0]);
        match x {
            Ok(..) =&gt; { }
            Err(e) =&gt; {
                let e = e as i32;
                assert!(
                    e == libc::ECONNREFUSED ||
                    e == libc::EPIPE ||
                    e == libc::ECONNABORTED,
                    "unknown error: {} {}", e, error_string(e as uint));
                break;
            }
        }
    }

    close(stream);
    close(listener);
}
</code></pre>

<p>This snippet reproduces the same <code>EPROTOTYPE</code> that we started with at the top
of the post. Pretty cool that we got here without much effort?</p>

<p>Now At this point you might say to yourself that couldn&rsquo;t I have extracted this
out myself?  And yeah, you would be right. This is a pretty much a c-in-rust
implementation of this bug. But what&rsquo;s great about using C-Reduce here is that
I only had to make some very rough guesses about what files were and were not
important to include in my <code>test.rs</code>. Eventually when we get some rust plugins
written for C-Reduce I probably could just point it at the whole <code>libstd</code> let
C-Reduce do it&rsquo;s thing. Doing this by hand can be a pretty long and painful
manual process, especially if we&rsquo;re trying to debug a codegen or runtime bug.
In the past I&rsquo;ve spent hours reducing some codegen bugs down into a small
snippet that C-Reduce was also able to produce in a couple minutes.</p>

<p>The last step with this code was to eliminate Rust from the picture, and
translate this code into C:</p>

<pre><code class="c">#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;strings.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;pthread.h&gt;
#include &lt;errno.h&gt;

int do_server() {
  int fd;
  struct sockaddr_in server_addr;

  fd = socket(AF_INET, SOCK_STREAM, 0);

  if (fd == -1) {
    perror("error socket server");
    exit(1);
  }

  bzero((char*) &amp;server_addr, sizeof(server_addr));
  server_addr.sin_family = AF_INET;
  server_addr.sin_addr.s_addr = INADDR_ANY;
  server_addr.sin_port = htons(9600);

  if (bind(fd, (struct sockaddr *) &amp;server_addr, sizeof(server_addr)) &lt; 0) {
    perror("error binding");
    exit(1);
  }

  return fd;
}

void* do_child_thread(void* unused) {
  struct sockaddr_in client_addr;
  int fd;

  fd = socket(AF_INET, SOCK_STREAM, 0);

  if (fd == -1) {
    perror("error socket client");
    exit(1);
  }

  bzero((char*) &amp;client_addr, sizeof(client_addr));
  client_addr.sin_family = AF_INET;
  client_addr.sin_addr.s_addr = INADDR_ANY;
  client_addr.sin_port = htons(9600);

  if (connect(fd, (struct sockaddr *) &amp;client_addr, sizeof(client_addr)) &lt; 0) {
    perror("error connect");
    exit(1);
  }

  fprintf(stderr, "closing client socket\n");

  if (close(fd) &lt; 0) {
    perror("error close client socket");
    exit(1);
  }

  fprintf(stderr, "closed client socket\n");

  return NULL;
}

int main(int argc, char** argv) {
  int server_fd, client_fd;
  socklen_t client_len;
  struct sockaddr_in client_addr;
  char buf[] = { 'a', '\n' };
  pthread_t child_thread;
  int rc;

  signal(SIGPIPE, SIG_IGN);

  server_fd = do_server();
  rc = listen(server_fd, 5);
  if (rc &lt; 0) {
    perror("error listen");
    return 1;
  }

  rc = pthread_create(&amp;child_thread, NULL, do_child_thread, NULL);
  if (rc != 0) {
    perror("error pthread_create");
    return 1;
  }

  client_len = sizeof(client_addr);
  client_fd = accept(server_fd, (struct sockaddr *) &amp;client_addr, &amp;client_len);
  if (client_fd &lt; 0) {
    perror("error accept");
    return 1;
  }

  while (1) {
    fprintf(stderr, "before send\n");
    rc = send(client_fd, buf, sizeof(buf), 0);
    fprintf(stderr, "after send: %d\n", rc);

    if (rc &lt; 0) {
      if (errno == EPIPE) {
        break;
      } else {
        int so_type;
        socklen_t so_len = sizeof(so_type);
        getsockopt(client_fd, SOL_SOCKET, SO_TYPE, &amp;so_type, &amp;so_len);
        fprintf(stderr, "type: %d %d\n", so_type, SOCK_STREAM);

        perror("error send");
        return 1;
      }
    }
  }

  fprintf(stderr, "before server closing client fd\n");
  if (close(client_fd) &lt; 0) {
    perror("error close client");
    return 1;
  }
  fprintf(stderr, "after server closing client fd\n");


  fprintf(stderr, "before server closing fd\n");
  if (close(server_fd) &lt; 0) {
    perror("error close server");
    return 1;
  }
  fprintf(stderr, "after server closing fd\n");

  rc = pthread_join(child_thread, NULL);
  if (rc != 0 &amp;&amp; rc != ESRCH) {
    fprintf(stderr, "error pthread_join: %d\n", rc);
    return 1;
  }

  return 0;
}
</code></pre>

<p>This also produces <code>EPROTOTYPE</code>, so we can eliminate Rust altogther. But lets
keep digging. What exactly is producing this error? If I was on Linux, I&rsquo;d use
<code>strace</code>, but that program isn&rsquo;t on Macs. There&rsquo;s a similar tool called
<code>dtruss</code>, but that seemed to slow things down enough that the <code>EPROTOTYPE</code>
never happened. Fortunately though there is another program called <code>errinfo</code>,
that just prints the <code>errno</code> along with every syscall. In one terminal I ran
<code>while ./test; do sleep 0.1; done</code>. In the other:</p>

<pre><code>% sudo errinfo -n test
...
           a.out           stat64    0
           a.out             open    0
           a.out psynch_mutexwait    0
           a.out   write_nocancel    0
           a.out           sendto    0
           a.out   write_nocancel    0
           a.out   write_nocancel    0
           a.out           sendto   41  Protocol wrong type for socket
           a.out   write_nocancel    0
           a.out       getsockopt    0
...
</code></pre>

<p>Right there we see our <code>sendto</code> syscall is actually returning the <code>EPROTOTYPE</code>.
This <code>errno</code> then is definitely being created inside the OSX kernel, not in any
userspace code. Fortunately, most of the Apple kernel, XNU, is open sourced, so
we can dig down to what&rsquo;ll be the my last layer. You can find the tarballs at
<a href="http://www.opensource.apple.com/.">http://www.opensource.apple.com/.</a> But I&rsquo;d rather use the
<a href="https://github.com/opensource-apple/xnu">unoffical GitHub repository</a>. Using GitHub&rsquo;s
search tools, We can find all 17 instances of
<a href="https://github.com/opensource-apple/xnu/search?l=c&amp;q=EPROTOTYPE&amp;utf8=%E2%9C%93">EPROTOTYPE</a>
in the codebase. Now I don&rsquo;t know the XNU codebase, but there are still some
really interesting things we can find. The first is in
<a href="https://github.com/opensource-apple/xnu/blob/bb7368935f659ada117c0889612e379c97eb83b3/bsd/kern/uipc_usrreq.c#L408">bsd/kern/uipc_usrreq.c</a>:</p>

<pre><code class="c">/*
 * Returns:  0      Success
 *    EINVAL
 *    EOPNOTSUPP
 *    EPIPE
 *    ENOTCONN
 *    EISCONN
 *  unp_internalize:EINVAL
 *  unp_internalize:EBADF
 *  unp_connect:EAFNOSUPPORT  Address family not supported
 *  unp_connect:EINVAL        Invalid argument
 *  unp_connect:ENOTSOCK      Not a socket
 *  unp_connect:ECONNREFUSED  Connection refused
 *  unp_connect:EISCONN       Socket is connected
 *  unp_connect:EPROTOTYPE    Protocol wrong type for socket
 *  unp_connect:???
 *  sbappendaddr:ENOBUFS      [5th argument, contents modified]
 *  sbappendaddr:???          [whatever a filter author chooses]
 */
static int
uipc_send(struct socket *so, int flags, struct mbuf *m, struct sockaddr *nam,
    struct mbuf *control, proc_t p)
{
...
</code></pre>

<p>Hey look at that! There&rsquo;s handler for the <code>send</code> syscall (although for IPC, not
TCP) that actually documents that it can return <code>EPROTOTYPE</code>! While it doesn&rsquo;t
explain exactly how this can happen, the fact it mentions <code>unp_connect</code> hints
that <code>uipc_send</code> may trigger a connect, and that&rsquo;s exactly what we find a
couple lines into the function:</p>

<pre><code class="c">...
    /* Connect if not connected yet. */
    /*
     * Note: A better implementation would complain
     * if not equal to the peer's address.
     */
    if ((so-&gt;so_state &amp; SS_ISCONNECTED) == 0) {
      if (nam) {
        error = unp_connect(so, nam, p);
        if (error)
          break;  /* XXX */
      } else {
        error = ENOTCONN;
        break;
      }
    }
...
</code></pre>

<p>The fact that the comment says the socket might not be connected yet when we&rsquo;re
doing a <code>send</code> hints that Apple may have introduced some level of asynchrony
and preemption to sockets. So if we trigger the actual connect here, it could
then return <code>EPROTOTYPE</code>, which makes sense. Unfortunately that&rsquo;s still not
quite the behavior we&rsquo;re seeing. We&rsquo;re not getting <code>EPROTOTYPE</code> on our first
write, but after we&rsquo;ve done a couple.</p>

<p>I believe we find that behavior in the actual TCP syscall file,
<a href="https://github.com/opensource-apple/xnu/blob/bb7368935f659ada117c0889612e379c97eb83b3/bsd/netinet/tcp_usrreq.c#L914-L948">bsd/netinet/tcp_usrreq.c</a>:</p>

<pre><code class="c">static int
tcp_usr_send(struct socket *so, int flags, struct mbuf *m,
     struct sockaddr *nam, struct mbuf *control, struct proc *p)
{
  int error = 0;
  struct inpcb *inp = sotoinpcb(so);
  struct tcpcb *tp;
  uint32_t msgpri = MSG_PRI_DEFAULT;
#if INET6
  int isipv6;
#endif
  TCPDEBUG0;

  if (inp == NULL || inp-&gt;inp_state == INPCB_STATE_DEAD
#if NECP
    || (necp_socket_should_use_flow_divert(inp))
#endif /* NECP */
    ) {
    /*
     * OOPS! we lost a race, the TCP session got reset after
     * we checked SS_CANTSENDMORE, eg: while doing uiomove or a
     * network interrupt in the non-splnet() section of sosend().
     */
    if (m != NULL)
      m_freem(m);
    if (control != NULL) {
      m_freem(control);
      control = NULL;
    }

    if (inp == NULL)
      error = ECONNRESET;  /* XXX EPIPE? */
    else
      error = EPROTOTYPE;
    tp = NULL;
    TCPDEBUG1();
    goto out;
  }
...
</code></pre>

<p>I believe that comment explains everything we&rsquo;re seeing. If we trigger a <code>send</code>
while the kernel is in the middle of tearing down the socket, it returns
<code>EPROTOTYPE</code>. This then looks to be an error we could retry. Once the socket is
fully torn down, it should eventually return the proper <code>EPIPE</code>. This is also
pretty easy to test. So I modified the inner loop of our C test:</p>

<pre><code>  while (1) {
    fprintf(stderr, "before send\n");
    rc = send(client_fd, buf, sizeof(buf), 0);
    fprintf(stderr, "after send: %d\n", rc);

    if (rc &lt; 0) {
      if (errno == EPIPE) {
        break;
      } else if (errno == EPROTOTYPE) {
        continue;
      } else {
        int so_type;
        socklen_t so_len = sizeof(so_type);
        getsockopt(client_fd, SOL_SOCKET, SO_TYPE, &amp;so_type, &amp;so_len);
        fprintf(stderr, "type: %d %d\n", so_type, SOCK_STREAM);

        perror("error send");
        return 1;
      }
    }
  }
</code></pre>

<p>And yep, it exits cleanly. After all of this, I think it&rsquo;s pretty clear at this
point that there&rsquo;s no weird kernel corruption bug going on, just a poorly
documented edge case. But it sure was fun chasing this condition through the
system.</p>

<p>To prevent anyone else from tripping over this edge case, I filed a Apple Radar
ticket (number #19012087 for any Apple employees reading this). Hopefully if
anyone runs into this mysterious <code>EPROTOTYPE</code> it&rsquo;ll be documented for them, or
at least there&rsquo;s a chance they&rsquo;ll stumble over this blog post and save
themselves a weekend diving through the OS.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 2.2: More Benchmarks]]></title>
    <link href="http://erickt.github.io/blog/2014/11/13/benchmarks-2/"/>
    <updated>2014-11-13T09:07:36-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/13/benchmarks-2</id>
    <content type="html"><![CDATA[<p>Back to the benchmarks! I got some great comments on
<a href="https://www.reddit.com/r/rust/comments/2lzc9n/rust_serialization_part_21_now_with_more/">reddit</a>,
So I wanted to do another post to update my numbers. Here&rsquo;s what I changed:</p>

<ul>
<li>I wasn&rsquo;t consistent on whether or not the serialization benchmarks included
Some tests are including the allocation of a buffer to write into. I&rsquo;ve
  changed it so most are reusing one, which speeds everything up (especially
capnproto-rust!). This does depend on
<a href="https://github.com/rust-lang/rust/pull/18885">#18885</a> landing though.</li>
<li>I&rsquo;ve added <a href="https://github.com/TyOverby/bincode">bincode</a>, which serializes
values as raw bytes. Quite speedy too! Not nearly as fast as Cap&#8217;n Proto though.</li>
<li>I&rsquo;ve changed <code>C++</code> and <code>Rust</code> JSON tests to serialize enums as uints.</li>
<li>I added the time it takes to create the populate the structures. I&rsquo;m betting
  the reason the Rust numbers are so high is that we&rsquo;re allocating strings. Not
sure if the other languages are able to avoid that allocation.</li>
</ul>


<hr />

<p>JSON:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library         </th>
<th> population (ns) </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> serialize::json </td>
<td> 1127            </td>
<td> 117                  </td>
<td> 26                     </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson (dom) </td>
<td> 546             </td>
<td> 281                  </td>
<td> 144                    </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson (dom) </td>
<td> 546             </td>
<td> 281                  </td>
<td> 181                    </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json   </td>
<td> 343             </td>
<td> 63.99                </td>
<td> 22.46                  </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson          </td>
<td> 343             </td>
<td> 144.60               </td>
<td> (not supported)        </td>
</tr>
</tbody>
</table>


<hr />

<p>Cap&#8217;n Proto:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library                   </th>
<th> population (ns) </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> capnproto-rust (unpacked) </td>
<td> 325             </td>
<td> 4977                 </td>
<td> 2251                   </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust (packed)   </td>
<td> 325             </td>
<td> 398                  </td>
<td> 246                    </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto              </td>
<td> 2368            </td>
<td> 2226.71              </td>
<td> 450                    </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto (zero copy)  </td>
<td> 2368            </td>
<td> 2226.71              </td>
<td> 1393.3                 </td>
</tr>
</tbody>
</table>


<hr />

<p>Protocol Buffers:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library       </th>
<th> population (ns) </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> rust-protobuf </td>
<td> 1041            </td>
<td> 370                  </td>
<td> 118                    </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf    </td>
<td> 1133            </td>
<td> 138.27               </td>
<td> 91.18                  </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf  </td>
<td> 343             </td>
<td> 472.69               </td>
<td> 295.33                 </td>
</tr>
</tbody>
</table>


<hr />

<p>Misc:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library      </th>
<th> population (ns) </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> rust-msgpack </td>
<td> 1143            </td>
<td> 454                  </td>
<td> 144                    </td>
</tr>
<tr>
<td> Rust     </td>
<td> bincode      </td>
<td> 1143            </td>
<td> 1149                 </td>
<td> 82                     </td>
</tr>
</tbody>
</table>


<p>Anyone want to add more C/Go/Rust/Java/etc benchmarks?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 2.1: Benchmarks]]></title>
    <link href="http://erickt.github.io/blog/2014/11/11/benchmarks/"/>
    <updated>2014-11-11T08:11:34-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/11/benchmarks</id>
    <content type="html"><![CDATA[<p>After <a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2</a> I received
a couple requests to add in a couple other rust serialization libraries. So one
thing led to another, and now I&rsquo;ve got a benchmark suite I&rsquo;m calling
<a href="https://github.com/erickt/rust-serialization-benchmarks">rust-serialization-benchmarks</a>.
Really creative name, eh? This includes all the other benchmarks I referred to
previously, as well as <a href="https://github.com/dwrensha/capnproto-rust">capnproto</a>,
<a href="https://github.com/mneumann/rust-msgpack">msgpack</a>, and
<a href="https://github.com/stepancheg/rust-protobuf">protobuf</a>.</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library             </th>
<th> format                  </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s)   </th>
</tr>
</thead>
<tbody>
<tr>
<td> C++      </td>
<td> rapidjson           </td>
<td> JSON (dom)              </td>
<td> 233                  </td>
<td> 102                      </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson           </td>
<td> JSON (sax)              </td>
<td> 233                  </td>
<td> 124                      </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json       </td>
<td> JSON                    </td>
<td> 54.93                </td>
<td> 16.72                    </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson              </td>
<td> JSON                    </td>
<td> 126.40               </td>
<td> (not supported)          </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf          </td>
<td> Protocol Buffers        </td>
<td> 138.27               </td>
<td> 91.18                    </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf        </td>
<td> Protocol Buffers        </td>
<td> 472.69               </td>
<td> 295.33                   </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto        </td>
<td> Cap&#8217;n Proto             </td>
<td> 2226.71              </td>
<td> 450                      </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto        </td>
<td> Cap&#8217;n Proto (zero copy) </td>
<td> 2226.71              </td>
<td> 1393.3                   </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json     </td>
<td> JSON                    </td>
<td> 89                   </td>
<td> 18                       </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-msgpack        </td>
<td> MessagePack             </td>
<td> 160                  </td>
<td> 52                       </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-protobuf       </td>
<td> Protocol Buffers        </td>
<td> 177                  </td>
<td> 70                       </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust      </td>
<td> Cap&#8217;n Proto (unpacked)  </td>
<td> 1729                 </td>
<td> 1276                     </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust      </td>
<td> Cap&#8217;n Proto (packed)    </td>
<td> 398                  </td>
<td> 246                      </td>
</tr>
</tbody>
</table>


<p>I upgraded to OS X Yosemite, so I think that brought these numbers down overall
from the last post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 2: Performance]]></title>
    <link href="http://erickt.github.io/blog/2014/11/03/performance/"/>
    <updated>2014-11-03T06:38:38-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/03/performance</id>
    <content type="html"><![CDATA[<p>As I said in the <a href="http://erickt.github.io/blog/2014/10/28/serialization/">last post</a>,
Rust&rsquo;s <code>serialize</code> library, specifically <code>serialize::json</code> is pretty slow.
Back when I started this project a number of months ago, I wanted to benchmark
to see how we compared to some other languages. There are a bunch of JSON
benchmarks, but the one I chose was Cloudflare&rsquo;s Go language.
<a href="https://github.com/cloudflare/goser">Goser</a>, mainly because it was using a
complex real world log structure, and they did the hard work of implementing
benchmarks for <a href="http://golang.org/pkg/encoding/json">encoding/json</a>,
<a href="http://code.google.com/p/goprotobuf/">goprotobuf</a>,
<a href="http://code.google.com/p/gogoprotobuf/">gogoprotobuf</a>, and
<a href="https://github.com/glycerine/go-capnproto">go-capnproto</a>. I also included the
Go <a href="https://github.com/pquerna/ffjson">ffjson</a> and C++
<a href="https://github.com/erickt/rapidjson/blob/master/log.cc">rapidjson</a>, which
both claim to be the fastest JSON libraries for those languages. Here are the
results I got:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library             </th>
<th> format           </th>
<th> serialization (MB/s) </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> C++      </td>
<td> rapidjson           </td>
<td> JSON             </td>
<td> 294                  </td>
<td> 164 (DOM) / 192 (SAX)  </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json       </td>
<td> JSON             </td>
<td> 71.47                </td>
<td> 25.09                  </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson              </td>
<td> JSON             </td>
<td> 156.67               </td>
<td> (not supported)        </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf          </td>
<td> Protocol Buffers </td>
<td> 148.78               </td>
<td> 99.57                  </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf        </td>
<td> Protocol Buffers </td>
<td> 519.48               </td>
<td> 319.40                 </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto        </td>
<td> Cap&#8217;n Proto      </td>
<td> 3419.54              </td>
<td> 665.35                 </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json     </td>
<td> JSON             </td>
<td> 40-ish               </td>
<td> 10-ish                 </td>
</tr>
</tbody>
</table>


<p>Notes:</p>

<ul>
<li><code>rapidjson</code> supports both DOM-style and SAX-style deserializing. DOM-style
means deserializing into a generic object, then from there into the final
object, SAX-style means a callback approach where a callback handler is
called for each JSON token.</li>
<li>Go&rsquo;s <code>encoding/json</code> uses reflection to serialize arbitrary values. <code>ffjson</code>
uses code generation to get it&rsquo;s serialization sped, but it doesn&rsquo;t implement
deserialization.</li>
<li>both <code>goprotobuf</code> and <code>gogoprotobuf</code> use code generation, but gogoprotobuf
uses Protocol Buffer&rsquo;s extension support to do cheaper serialization.</li>
<li>Cap&#8217;n Proto doesn&rsquo;t really do serialization, but lays the serialized data out
just like it is in memory so it has nearly zero serialization speed.</li>
<li>The Rust numbers are from a couple months ago and I couldn&rsquo;t track down
the exact numbers.</li>
</ul>


<p>So. Yikes. Not only are we no where near <code>rapidjson</code>, we were being soundly
beaten by Go&rsquo;s reflection-based framework <code>encoding/json</code>.  Even worse, our
compile time was at least 10 times theirs. So, not pretty at all.</p>

<p>But that was a couple months ago. Between then and now, Patrick Walton, Luqman
Aden, myself, and probably lots others found and fixed a number of bugs across
<code>serialize::json</code>, <code>std::io</code>, generic function calls, and more. All this work
got us to more than double our performance:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library           </th>
<th> format               </th>
<th> serialization (MB/s)   </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> serialize::json   </td>
<td> JSON                 </td>
<td> 117                    </td>
<td> 25                     </td>
</tr>
</tbody>
</table>


<p>We&rsquo;re (kind of) beating Go! At least the builtin reflection-based solution.
Better, but not great. I think our challenge is those dang closures. While LLVM
can optimize simple closures, it seems to have a lot of trouble with all these
recursive closure calls. While having finished unboxed closures might finally
let us break through this performance bottleneck, it&rsquo;s not guaranteed.</p>

<p>All in all, this, and the representational problems from
<a href="http://erickt.github.io/blog/2014/10/28/serialization/">post 1</a> make it pretty
obvious we got some fundamental issues here and we need to use an alternative
solution. Next post I&rsquo;ll start getting into the details of the design of
<a href="https://github.com/erickt/rust-serde">serde</a>.</p>
]]></content>
  </entry>
  
</feed>
