<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Rust | Tilting at Rabbit Holes]]></title>
  <link href="http://erickt.github.io/blog/categories/rust/atom.xml" rel="self"/>
  <link href="http://erickt.github.io/"/>
  <updated>2015-02-09T21:13:16-08:00</updated>
  <id>http://erickt.github.io/</id>
  <author>
    <name><![CDATA[Erick Tryzelaar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Syntex: Syntax Extensions for Rust 1.0]]></title>
    <link href="http://erickt.github.io/blog/2015/02/09/syntex-syntex-extensions-for-rust-1-dot-0/"/>
    <updated>2015-02-09T19:37:32-08:00</updated>
    <id>http://erickt.github.io/blog/2015/02/09/syntex-syntex-extensions-for-rust-1-dot-0</id>
    <content type="html"><![CDATA[<p>I use and love syntax extensions, and I&rsquo;m planning on using them to simplify
down how one interacts with a system like
<a href="https://github.com/erickt/rust-serde">serde</a>. Unfortunately though, to write
them you need to use Rust&rsquo;s <code>libsyntax</code>, which is not going to be exposed in
Rust 1.0 because we&rsquo;re not ready to stablize it&rsquo;s API. That would really hamper
future development of the compiler.</p>

<p>It would be so nice though, writing this for every type we want to serialize:</p>

<pre><code class="rust">#[derive_deserialize]
struct Point {
    x: int,
    y: int,
}
</code></pre>

<p>instead of:</p>

<pre><code class="rust">impl &lt;D: Deserializer&lt;E&gt;, E&gt; Deserialize&lt;D, E&gt; for Point {
    fn deserialize_token(state: &amp;mut D, token: Token) -&gt; Result&lt;Point, E&gt; {
        try!(state.expect_struct_start(token, "Point"));

        let mut x = None;
        let mut y = None;

        loop {
            let idx = match try!(state.expect_struct_field_or_end(&amp;["x", "y"])) {
                Some(idx) =&gt; idx,
                None =&gt; { break ; }
            };

            match idx {
                Some(0us) =&gt; { x = Some(try!(state.expect_struct_value())); }
                Some(1us) =&gt; { y = Some(try!(state.expect_struct_value())); }
                None | Some(_) =&gt; { panic!() }
            }
        }

        let x = match x {
            Some(x) =&gt; x,
            None =&gt; try!(state.missing_field("x")),
        };

        let y = match y {
            Some(y) =&gt; y,
            None =&gt; try!(state.missing_field("y")),
        };

        Ok(Point{x: x, y: y,})
    }
}
</code></pre>

<p>So I want to announce my plan on how to deal with this (and also publically
announce that everyone can blame me if this turns out to hurt Rust&rsquo;s future
development). I&rsquo;ve started <a href="https://github.com/erickt/rust-syntex">syntex</a>, a
library that enables code generation using an unofficial tracking fork of
<code>libsyntax</code>. In order to deal with the fact that <code>libsyntax</code> might make
breaking changes between minor Rust releases, we will just release a minor or
major release of <code>syntex</code>, depending on if there were any breaking changes in
<code>libsyntax</code>.  Ideally <code>syntex</code> will allow the community to experiment with
different code generation approaches to see what would be worth merging
upstream. Or even better, what hooks are needed in the compiler so it doesn&rsquo;t
have to think about syntax extensions at all.</p>

<p>I&rsquo;ve got the basic version working right now. Here&rsquo;s a simple
<code>hello_world_macros</code> syntax extension (you can see the actual code
<a href="https://github.com/erickt/rust-syntex/tree/master/hello_world">here</a>).
First, the <code>hello_world_macros/Cargo.toml</code>:</p>

<pre><code class="toml">[package]
name = "hello_world_macros"
version = "0.2.0"
authors = [ "erick.tryzelaar@gmail.com" ]

[dependencies]
syntex = "*"
syntex_syntax = "*"
</code></pre>

<p>The <code>syntex_syntax</code> is the crate for my fork of <code>libsyntax</code>, and <code>syntex</code>
provides some helper functions to ease registering syntax extensions.</p>

<p>Then the <code>src/lib.rs</code>, which declares a macro <code>hello_world</code> that just produces a
<code>"hello world"</code> string:</p>

<pre><code class="rust">extern crate syntex;
extern crate syntex_syntax;

use syntex::Registry;

use syntex_syntax::ast::TokenTree;
use syntex_syntax::codemap::Span;
use syntex_syntax::ext::base::{ExtCtxt, MacExpr, MacResult, TTMacroExpander};
use syntex_syntax::ext::build::AstBuilder;
use syntex_syntax::parse::token::InternedString;

fn expand_hello_world&lt;'cx&gt;(
    cx: &amp;'cx mut ExtCtxt,
    sp: Span,
    tts: &amp;[TokenTree]
) -&gt; Box&lt;MacResult + 'cx&gt; {
    let expr = cx.expr_str(sp, InternedString::new("hello world"));

    MacExpr::new(expr)
}

pub fn register(registry: &amp;mut Registry) {
    registry.register_fn("hello_world", expand_hello_world);
}
</code></pre>

<p>Now to use it. This is a little more complicated because we have to do code
generation, but Cargo helps with that. Our strategy is use a <code>build.rs</code> script
to do code generation in a <code>main.rss</code> file, and then use the <code>include!()</code> macro
to include it into our dummy <code>main.rs</code> file. Here&rsquo;s the <code>Cargo.toml</code> we need:</p>

<pre><code>[package]
name = "hello_world"
version = "0.2.0"
authors = [ "erick.tryzelaar@gmail.com" ]
build = "build.rs"

[build-dependencies]
syntex = "*"
syntex_syntax = "*"

[build-dependencies.hello_world_macros]
path = "hello_world_macros"
</code></pre>

<p>Here&rsquo;s the <code>build.rs</code>, which actually performs the code generation:</p>

<pre><code class="rust">extern crate syntex;
extern crate hello_world_macros;

use std::os;

fn main() {
    let mut registry = syntex::Registry::new();
    hello_world_macros::register(&amp;mut registry);

    registry.expand(
        "hello_world",
        &amp;Path::new("src/main.rss"),
        &amp;Path::new(os::getenv("OUT_DIR").unwrap()).join("main.rs"));
}
</code></pre>

<p>Our <code>main.rs</code> driver script:</p>

<pre><code class="rust">// Include the real main
include!(concat!(env!("OUT_DIR"), "/main.rs"));
</code></pre>

<p>And finally the <code>main.rss</code>:</p>

<pre><code class="rust">fn main() {
    let s = hello_world!();
    println!("{}", s);
}
</code></pre>

<p>One limitiation you can see above is that we unfortunately can&rsquo;t compose our
macros with the Rust macros is that <code>syntex</code> currently has no awareness of the
Rust macros, and since macros are parsed outside-in, we have to leave the
tokens inside a macro like <code>println!()</code> untouched.</p>

<hr />

<p>That&rsquo;s <code>syntex</code>. There is a bunch of more work left to be done in <code>syntex</code>
to make it really useable. There&rsquo;s also a lot of work in Rust and Cargo that
would help it be really effective:</p>

<ul>
<li>We need a way to inform Rust that this block of code is actually coming from
a different file than the one it&rsquo;s processing. This is roughly equivalent to
  the <a href="https://gcc.gnu.org/onlinedocs/cpp/Line-Control.html">#line</a> macros in
<code>C</code>.</li>
<li>We could upstream the &ldquo;ignore unknown macros&rdquo; patch to minimize
changes to <code>libsyntax</code>.</li>
<li>It would be nice if we could allow <code>#[path]</code> to reference an environment
variable. This would be a little cleaner than using <code>include!(...)</code>.</li>
<li>We need a way to extract macros from a crate.</li>
</ul>


<p>On Cargo&rsquo;s side:</p>

<ul>
<li>It would be nice if Cargo could be told to use a generated file as the
<code>main.rs</code>/<code>lib.rs</code>/etc.</li>
<li><code>Cargo.toml</code> could grow a plugin mechanism to remove the need to write a
<code>build.rs</code> script.</li>
</ul>


<p>I&rsquo;m sure there&rsquo;s plenty more that needs to get done! So, please help out!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 3.1: Another Performance Digression]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/performance-digression/"/>
    <updated>2014-12-13T18:35:08-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/performance-digression</id>
    <content type="html"><![CDATA[<p>Wow, home stretch! Here&rsquo;s the rest of the series if you want to catch up:
<a href="http://erickt.github.io/blog/2014/10/28/serialization/">part 1</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.1</a>,
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.2</a>, and
<a href="http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization/">part 3</a>.</p>

<p>Overall <code>serde</code>&rsquo;s approach for serialization works out pretty well. One thing I
forgot to include in the last post was that I also have two benchmarks that are
not using <code>serde</code>, but are just safely reading and writing values.  Assuming I
haven&rsquo;t missed anything, they should be the upper limit in performance we can
get out of any serialization framework: Here&rsquo;s
<a href="https://github.com/erickt/rust-serde/blob/master/benches/bench_log.rs#L1021">serialization</a>:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library                        </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max without string escapes</strong> </td>
<td> <strong>353</strong>              </td>
</tr>
<tr>
<td> c++      </td>
<td> rapidjson                      </td>
<td> 304                  </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with string escape</strong>     </td>
<td> <strong>234</strong>              </td>
</tr>
<tr>
<td> rust     </td>
<td> serde::json                    </td>
<td> 201                  </td>
</tr>
<tr>
<td> rust     </td>
<td> serialize::json                </td>
<td> 147                  </td>
</tr>
<tr>
<td> go       </td>
<td> ffjson                         </td>
<td> 147                  </td>
</tr>
</tbody>
</table>


<p>So beyond optimizing string escaping, <code>serde::json</code> is only 14% slower than the
zero-cost version and 34% slower than <code>rapidjson</code>.</p>

<p><a href="https://github.com/erickt/rust-serde/blob/master/benches/bench_log.rs#L1613">Deserialization</a>,
on the other hand, still has a ways to go:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library                         </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> rust     </td>
<td> rapidjson (SAX)                 </td>
<td> 189                    </td>
</tr>
<tr>
<td> c++      </td>
<td> rapidjson (DOM)                 </td>
<td> 162                    </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with Iterator&lt;u8&gt;</strong> </td>
<td> <strong>152</strong>                </td>
</tr>
<tr>
<td> go       </td>
<td> ffjson                          </td>
<td> 95                     </td>
</tr>
<tr>
<td> <strong>rust</strong> </td>
<td> <strong>max with Reader</strong>             </td>
<td> <strong>78</strong>                 </td>
</tr>
<tr>
<td> rust     </td>
<td> serde::json                     </td>
<td> 73                     </td>
</tr>
<tr>
<td> rust     </td>
<td> serialize::json                 </td>
<td> 24                     </td>
</tr>
</tbody>
</table>


<p>There are a couple interesting things here:</p>

<p>First, <code>serde::json</code> is built upon consuming from an <code>Iterator&lt;u8&gt;</code>, so we&rsquo;re
48% slower than our theoretical max, and 58% slower than <code>rapidjson</code>. It looks
like tagged tokens, while faster than the closures in <code>libserialize</code>, are still
pretty expensive.</p>

<p>Second, <code>ffjson</code> is beating us and they compile dramatically faster too. The
<a href="https://github.com/cloudflare/goser">goser</a> test suite takes about 0.54
seconds to compile, whereas mine takes about 30 seconds at <code>--opt-level=3</code>
(!!). Rust itself is only taking 1.5 seconds, the rest is spent in LLVM. With
no optimization, it compiles &ldquo;only&rdquo; in 5.6 seconds, and is 96% slower.</p>

<p>Third, <code>Reader</code> is a surprisingly expensive trait when dealing with a format
like JSON that need to read a byte at a time. It turns out we&rsquo;re not
<a href="https://github.com/rust-lang/rust/issues/19864">generating great code</a> for
types with padding. aatch has been working on fixing this though.</p>

<hr />

<p>Since I wrote that last section, I did a little more experimentation to try to
figure out why our serialization upper bound is 23% slower than rapidjson. And,
well, maybe I found it?</p>

<table>
<thead>
<tr>
<th>                                </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> serde::json with a MyMemWriter </td>
<td> 346                  </td>
</tr>
<tr>
<td> serde::json with a Vec<u8>     </td>
<td> 247                  </td>
</tr>
</tbody>
</table>


<p>All I did with <code>MyMemWriter</code> is copy the <code>Vec::&lt;u8&gt;</code> implementation of <code>Writer</code>
into the local codebase:</p>

<pre><code class="rust">struct MyMemWriter0 {
    buf: Vec&lt;u8&gt;,
}

impl MyMemWriter0 {
    pub fn with_capacity(cap: uint) -&gt; MyMemWriter0 {
        MyMemWriter0 {
            buf: Vec::with_capacity(cap)
        }
    }
}

impl Writer for MyMemWriter0 {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; io::IoResult&lt;()&gt; {
        self.buf.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_serializer_my_mem_writer0(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut wr = MyMemWriter0::with_capacity(1024);

    b.iter(|| {
        wr.buf.clear();

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}

#[bench]
fn bench_serializer_vec(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut wr = Vec::with_capacity(1024);

    b.iter(|| {
        wr.clear();

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}
</code></pre>

<p>Somehow it&rsquo;s not enough to just mark <code>Vec::write</code> as
<code>#[inline]</code>, having it in the same file gave LLVM enough information to
optimize it&rsquo;s overhead away. Even using <code>#[inline(always)]</code> on <code>Vec::write</code> and
<code>Vec::push_all</code> isn&rsquo;t able to get the same increase, so I&rsquo;m not sure how to
replicate this in the general case.</p>

<p>Also interesting is <code>bench_serializer_slice</code>, which uses <code>BufWriter</code>.</p>

<table>
<thead>
<tr>
<th>                                </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> serde::json with a BufWriter   </td>
<td> 342                  </td>
</tr>
</tbody>
</table>


<pre><code class="rust">#[bench]
fn bench_serializer_slice(b: &amp;mut Bencher) {
    let log = Log::new();
    let json = json::to_vec(&amp;log);
    b.bytes = json.len() as u64;

    let mut buf = [0, .. 1024];

    b.iter(|| {
        for item in buf.iter_mut(){ *item = 0; }
        let mut wr = std::io::BufWriter::new(&amp;mut buf);

        let mut serializer = json::Serializer::new(wr.by_ref());
        log.serialize(&amp;mut serializer).unwrap();
        let _json = serializer.unwrap();
    });
}
</code></pre>

<hr />

<p>Another digression. Since I wrote the above, aatch has put out some PRs that
should help speed up enums.
<a href="https://github.com/rust-lang/rust/pull/19898">19898</a> and
<a href="https://github.com/rust-lang/rust/pull/20060">#20060</a> and was able to optimize
the padding out of enums and fix an issue with returns generating bad code. In
my <a href="https://github.com/rust-lang/rust/issues/19864">bug from earlier</a>
his patches were able to speed up my benchmark returning an
<code>Result&lt;(), IoError&gt;</code> from running at 40MB/s to 88MB/s. However, if we&rsquo;re able
to reduce <code>IoError</code> down to a word, we get the performance up to 730MB/s! We
also might get enum compression, so a type like <code>Result&lt;(), IoError&gt;</code> then
would speed up to 1200MB/s! I think going in this direction is going to really
help speed things up.</p>

<hr />

<p>That was taking a while, so until next time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rewriting Rust Serialization, Part 3: Introducing Serde]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization/"/>
    <updated>2014-12-13T14:40:18-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/rewriting-rust-serialization</id>
    <content type="html"><![CDATA[<p>There&rsquo;s been a long digression over the past month
(<a href="http://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/">possible kernel bugs</a>,
<a href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/">benchmarking Writers</a>,
and
<a href="https://github.com/rust-lang/rust/pull/19574">don&rsquo;t believe in magic, folks</a>), but I&rsquo;m back
into serialization. Woo! Here&rsquo;s
<a href="http://erickt.github.io/blog/2014/10/28/serialization/">part 1</a> and
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2</a>), Rust&rsquo;s
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.1</a>), Rust&rsquo;s
<a href="http://erickt.github.io/blog/2014/11/03/performance/">part 2.2</a>) if you need
to catch up.</p>

<p>So <code>libserialize</code> has some pretty serious downsides. It&rsquo;s slow, it&rsquo;s got this
weird recursive closure thing going on, and it can&rsquo;t even represent enum types
like a <code>serialize::json::Json</code>. We need a new solution, and while I was at it,
we ended up with two: <a href="https://github.com/erickt/rust-serde">serde</a> and
<a href="https://github.com/erickt/rust-serde/tree/master/serde2">serde2</a>. Both are
different approaches to trying to address these problems. The biggest one being
the type representation problem.</p>

<h2>Serde Version 1</h2>

<h3>Deserialization</h3>

<p>I want to start with deserialization first, as that&rsquo;s really the interesting
bit. To repeat myself a little bit from
<a href="https://erickt.github.io/blog/2014/10/28/serialization/">part 1</a>,
here is a generic json <code>Value</code> enum:</p>

<pre><code class="rust">pub enum Value {
    I64(i64),
    U64(u64),
    F64(f64),
    String(String),
    Boolean(bool),
    Array(Vec&lt;Value&gt;),
    Object(TreeMap&lt;String, Value&gt;),
    Null,
}
</code></pre>

<p>To deserialize a string like <code>[1, true]</code> into
<code>Array(vec![I64(1), Boolean(true)])</code>, we need to peek at one character ahead
(ignoring whitespace) in order to discover what is the type of the next value.
We then can use that knowledge to pick the right variant, and parse the next
value correctly. While I haven&rsquo;t formally studied this stuff, I believe this
can be more formally stated as <code>Value</code> requires at least a LL(1) grammar,
but since <code>libserialize</code> supports no lookahead, so at most it can handle LL(0)
grammars.</p>

<p>Since I was thinking of this problem in terms of grammars, I wanted to take a
page out of their book and implement generic deserialization in this style.
<code>serde::de::Deserializer</code>s are then an <code>Iterator&lt;serde::de::Token&gt;</code> lexer that
produces a token stream, and <code>serde::de::Deserialize</code>s are a parser that
consumes this stream to produce a value. Here&rsquo;s <code>serde::de::Token</code>, which can
represent nearly all the rust types:</p>

<pre><code class="rust">pub enum Token {
    Null,
    Bool(bool),
    Int(int),
    I8(i8),
    I16(i16),
    I32(i32),
    I64(i64),
    Uint(uint),
    U8(u8),
    U16(u16),
    U32(u32),
    U64(u64),
    F32(f32),
    F64(f64),
    Char(char),
    Str(&amp;'static str),
    String(String),

    Option(bool),     // true if the option has a value

    TupleStart(uint), // estimate of the number of values

    StructStart(
        &amp;'static str, // the struct name
        uint,         // estimate of the number of (string, value) pairs
    ),

    EnumStart(
        &amp;'static str, // the enum name
        &amp;'static str, // the variant name
        uint          // estimate of the number of values
    ),

    SeqStart(uint), // number of values

    MapStart(uint), // number of (value, value) pairs

    End,
}
</code></pre>

<p>The <code>serde::de::Deserialize</code> stream must generate tokens that follow this
grammar:</p>

<pre><code class="antlr">value ::= Null
        | Bool
        | Int
        | ...
        | option
        | tuple
        | struct
        | enum
        | sequence
        | map
        ;

option ::= Option value
         | Option
         ;

tuple := TupleStart value* End;

struct := StructStart (Str value)* End;

enum := EnumStart value* End;

sequence := SeqStart value* End;

map := MapStart (value value)* End;
</code></pre>

<p>For performance reasons, there is no separator in the compound grammar.</p>

<p>Finishing up this section are the actual traits, <code>Deserialize</code> and <code>Deserializer</code>:</p>

<pre><code class="rust">pub trait Deserialize&lt;D: Deserializer&lt;E&gt;, E&gt; {
    fn deserialize(d: &amp;mut D) -&gt; Result&lt;Self, E&gt; {
        let token = try!(d.expect_token());
        Deserialize::deserialize_token(d, token)
    }

    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Self, E&gt;;
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    /// Called when a `Deserialize` expected more tokens, but the
    /// `Deserializer` was empty.
    fn end_of_stream_error(&amp;mut self) -&gt; E;

    /// Called when a `Deserializer` was unable to properly parse the stream.
    fn syntax_error(&amp;mut self, token: Token, expected: &amp;'static [TokenKind]) -&gt; E;

    /// Called when a named structure or enum got a name that it didn't expect.
    fn unexpected_name_error(&amp;mut self, token: Token) -&gt; E;

    /// Called when a value was unable to be coerced into another value.
    fn conversion_error(&amp;mut self, token: Token) -&gt; E;

    /// Called when a `Deserialize` structure did not deserialize a field
    /// named `field`.
    fn missing_field&lt;
        T: Deserialize&lt;Self, E&gt;
    &gt;(&amp;mut self, field: &amp;'static str) -&gt; Result&lt;T, E&gt;;

    /// Called when a `Deserialize` has decided to not consume this token.
    fn ignore_field(&amp;mut self, _token: Token) -&gt; Result&lt;(), E&gt; {
        let _: IgnoreTokens = try!(Deserialize::deserialize(self));
        Ok(())
    }

    #[inline]
    fn expect_token(&amp;mut self) -&gt; Result&lt;Token, E&gt; {
        self.next().unwrap_or_else(|| Err(self.end_of_stream_error()))
    }

    ...
}
</code></pre>

<p>The <code>Deserialize</code> trait is kept pretty slim, and is how lookahead is
implemented. <code>Deserializer</code> is an enhanced <code>Iterator&lt;Result&lt;Token, E&gt;&gt;</code>, with
many helpful default methods. Here are them in action. First we&rsquo;ll start with
what&rsquo;s probably the simplest <code>Deserializer</code>, which just wraps a <code>Vec&lt;Token&gt;</code>:</p>

<pre><code class="rust">enum Error {
    EndOfStream,
    SyntaxError(Vec&lt;TokenKind&gt;),
    UnexpectedName,
    ConversionError,
    MissingField(&amp;'static str),
}

struct TokenDeserializer&lt;Iter&gt; {
    tokens: Iter,
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; TokenDeserializer&lt;Iter&gt; {
    fn new(tokens: Iter) -&gt; TokenDeserializer&lt;Iter&gt; {
        TokenDeserializer {
            tokens: tokens,
        }
    }
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; Iterator&lt;Result&lt;Token, Error&gt;&gt; for TokenDeserializer&lt;Iter&gt; {
    fn next(&amp;mut self) -&gt; option::Option&lt;Result&lt;Token, Error&gt;&gt; {
        self.tokens.next().map(|token| Ok(token))
    }
}

impl&lt;Iter: Iterator&lt;Token&gt;&gt; Deserializer&lt;Error&gt; for TokenDeserializer&lt;Iter&gt; {
    fn end_of_stream_error(&amp;mut self) -&gt; Error {
        Error::EndOfStream
    }

    fn syntax_error(&amp;mut self, _token: Token, expected: &amp;[TokenKind]) -&gt; Error {
        Error::SyntaxError(expected.to_vec())
    }

    fn unexpected_name_error(&amp;mut self, _token: Token) -&gt; Error {
        Error::UnexpectedName
    }

    fn conversion_error(&amp;mut self, _token: Token) -&gt; Error {
        Error::ConversionError
    }

    #[inline]
    fn missing_field&lt;
        T: Deserialize&lt;TokenDeserializer&lt;Iter&gt;, Error&gt;
    &gt;(&amp;mut self, field: &amp;'static str) -&gt; Result&lt;T, Error&gt; {
        Err(Error::MissingField(field))
    }
}
</code></pre>

<p>Overall it should be pretty straight forward. As usual, error handling makes
things a bit noisier, but hopefully it&rsquo;s not too onerous. Next is a
<code>Deserialize</code> for <code>bool</code>:</p>

<pre><code class="rust">impl&lt;D: Deserializer&lt;E&gt;, E&gt; Deserialize&lt;D, E&gt; for bool {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;bool, E&gt; {
        d.expect_bool(token)
    }
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    ...

    #[inline]
    fn expect_bool(&amp;mut self, token: Token) -&gt; Result&lt;bool, E&gt; {
        match token {
            Token::Bool(value) =&gt; Ok(value),
            token =&gt; {
                static EXPECTED_TOKENS: &amp;'static [TokenKind] = &amp;[
                    TokenKind::BoolKind,
                ];
                Err(self.syntax_error(token, EXPECTED_TOKENS))
            }
        }
    }


    ...
}
</code></pre>

<p>Simple! Sequences are a bit more tricky. Here&rsquo;s <code>Deserialize</code> a <code>Vec&lt;T&gt;</code>. We
use a helper adaptor <code>SeqDeserializer</code> to deserialize from all types that
implement <code>FromIterator</code>:</p>

<pre><code class="rust">impl&lt;
    D: Deserializer&lt;E&gt;,
    E,
    T: Deserialize&lt;D ,E&gt;
&gt; Deserialize&lt;D, E&gt; for Vec&lt;T&gt; {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Vec&lt;T&gt;, E&gt; {
        d.expect_seq(token)
    }
}

pub trait Deserializer&lt;E&gt;: Iterator&lt;Result&lt;Token, E&gt;&gt; {
    ...

    #[inline]
    fn expect_seq&lt;
        T: Deserialize&lt;Self, E&gt;,
        C: FromIterator&lt;T&gt;
    &gt;(&amp;mut self, token: Token) -&gt; Result&lt;C, E&gt; {
        let len = try!(self.expect_seq_start(token));
        let mut err = None;

        let collection: C = {
            let d = SeqDeserializer {
                d: self,
                len: len,
                err: &amp;mut err,
            };

            d.collect()
        };

        match err {
            Some(err) =&gt; Err(err),
            None =&gt; Ok(collection),
        }
    }

    ...
}

struct SeqDeserializer&lt;'a, D: 'a, E: 'a&gt; {
    d: &amp;'a mut D,
    len: uint,
    err: &amp;'a mut Option&lt;E&gt;,
}

impl&lt;
    'a,
    D: Deserializer&lt;E&gt;,
    E,
    T: Deserialize&lt;D, E&gt;
&gt; Iterator&lt;T&gt; for SeqDeserializer&lt;'a, D, E&gt; {
    #[inline]
    fn next(&amp;mut self) -&gt; option::Option&lt;T&gt; {
        match self.d.expect_seq_elt_or_end() {
            Ok(next) =&gt; {
                self.len -= 1;
                next
            }
            Err(err) =&gt; {
                *self.err = Some(err);
                None
            }
        }
    }

    #[inline]
    fn size_hint(&amp;self) -&gt; (uint, option::Option&lt;uint&gt;) {
        (self.len, Some(self.len))
    }
}
</code></pre>

<p>Last is a struct deserializer. This relies on a simple state machine in order
to deserialize from out of order maps:</p>

<pre><code class="rust">struct Foo {
    a: (),
    b: uint,
    c: TreeMap&lt;String, Option&lt;char&gt;&gt;,
}

impl&lt;
    D: Deserializer&lt;E&gt;,
    E
&gt; Deserialize&lt;D, E&gt; for Foo {
    #[inline]
    fn deserialize_token(d: &amp;mut D, token: Token) -&gt; Result&lt;Foo, E&gt; {
        try!(d.expect_struct_start(token, "Foo"));

        let mut a = None;
        let mut b = None;
        let mut c = None;

        static FIELDS: &amp;'static [&amp;'static str] = &amp;["a", "b", "c"];

        loop {
            let idx = match try!(d.expect_struct_field_or_end(FIELDS)) {
                Some(idx) =&gt; idx,
                None =&gt; { break; }
            };

            match idx {
                Some(0) =&gt; { a = Some(try!(d.expect_struct_value())); }
                Some(1) =&gt; { b = Some(try!(d.expect_struct_value())); }
                Some(2) =&gt; { c = Some(try!(d.expect_struct_value())); }
                Some(_) =&gt; unreachable!(),
                None =&gt; { let _: IgnoreTokens = try!(Deserialize::deserialize(d)); }
            }
        }

        Ok(Foo { a: a.unwrap(), b: b.unwrap(), c: c.unwrap() })
    }
}
</code></pre>

<p>It&rsquo;s more complicated than <code>libserialize</code>&rsquo;s struct parsing, but it performs
much better because it can handle out of order maps without buffering tokens.</p>

<h3>Serialization</h3>

<p>Serialization&rsquo;s story is a much simpler one. Conceptually
<code>serde::ser::Serializer</code>/<code>serde::ser::Serialize</code> are inspired by the
deserialization story, but we don&rsquo;t need the tagged tokens because we already
know the types. Here are the traits:</p>

<pre><code class="rust">pub trait Serialize&lt;S: Serializer&lt;E&gt;, E&gt; {
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt;;
}

pub trait Serializer&lt;E&gt; {
    fn serialize_null(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_bool(&amp;mut self, v: bool) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_int(&amp;mut self, v: int) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i8(&amp;mut self, v: i8) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i16(&amp;mut self, v: i16) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i32(&amp;mut self, v: i32) -&gt; Result&lt;(), E&gt; {
        self.serialize_i64(v as i64)
    }

    #[inline]
    fn serialize_i64(&amp;mut self, v: i64) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_uint(&amp;mut self, v: uint) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u8(&amp;mut self, v: u8) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u16(&amp;mut self, v: u16) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u32(&amp;mut self, v: u32) -&gt; Result&lt;(), E&gt; {
        self.serialize_u64(v as u64)
    }

    #[inline]
    fn serialize_u64(&amp;mut self, v: u64) -&gt; Result&lt;(), E&gt;;

    #[inline]
    fn serialize_f32(&amp;mut self, v: f32) -&gt; Result&lt;(), E&gt; {
        self.serialize_f64(v as f64)
    }

    fn serialize_f64(&amp;mut self, v: f64) -&gt; Result&lt;(), E&gt;;

    fn serialize_char(&amp;mut self, v: char) -&gt; Result&lt;(), E&gt;;

    fn serialize_str(&amp;mut self, v: &amp;str) -&gt; Result&lt;(), E&gt;;

    fn serialize_tuple_start(&amp;mut self, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_tuple_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_tuple_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_struct_start(&amp;mut self, name: &amp;str, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_struct_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, name: &amp;str, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_struct_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_enum_start(&amp;mut self, name: &amp;str, variant: &amp;str, len: uint) -&gt; Result&lt;(), E&gt;;
    fn serialize_enum_elt&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;T) -&gt; Result&lt;(), E&gt;;
    fn serialize_enum_end(&amp;mut self) -&gt; Result&lt;(), E&gt;;

    fn serialize_option&lt;
        T: Serialize&lt;Self, E&gt;
    &gt;(&amp;mut self, v: &amp;Option&lt;T&gt;) -&gt; Result&lt;(), E&gt;;

    fn serialize_seq&lt;
        T: Serialize&lt;Self, E&gt;,
        Iter: Iterator&lt;T&gt;
    &gt;(&amp;mut self, iter: Iter) -&gt; Result&lt;(), E&gt;;

    fn serialize_map&lt;
        K: Serialize&lt;Self, E&gt;,
        V: Serialize&lt;Self, E&gt;,
        Iter: Iterator&lt;(K, V)&gt;
    &gt;(&amp;mut self, iter: Iter) -&gt; Result&lt;(), E&gt;;
}
</code></pre>

<p>There are many default methods, so only a handful of implementations need to be
specified. Now lets look at how they are used. Here&rsquo;s a simple
<code>AssertSerializer</code> that I use in my test suite to make sure I&rsquo;m serializing
properly:</p>

<pre><code class="rust">struct AssertSerializer&lt;Iter&gt; {
    iter: Iter,
}

impl&lt;'a, Iter: Iterator&lt;Token&lt;'a&gt;&gt;&gt; AssertSerializer&lt;Iter&gt; {
    fn new(iter: Iter) -&gt; AssertSerializer&lt;Iter&gt; {
        AssertSerializer {
            iter: iter,
        }
    }

    fn serialize&lt;'b&gt;(&amp;mut self, token: Token&lt;'b&gt;) -&gt; Result&lt;(), Error&gt; {
        let t = self.iter.next().unwrap();

        assert_eq!(t, token);

        Ok(())
    }
}

impl&lt;'a, Iter: Iterator&lt;Token&lt;'a&gt;&gt;&gt; Serializer&lt;Error&gt; for AssertSerializer&lt;Iter&gt; {
    fn serialize_null(&amp;mut self) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Null)
    }
    fn serialize_bool(&amp;mut self, v: bool) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Bool(v))
    }
    fn serialize_int(&amp;mut self, v: int) -&gt; Result&lt;(), Error&gt; {
        self.serialize(Token::Int(v))
    }
    ...
}
</code></pre>

<p>Implementing <code>Serialize</code> for values follows the same pattern. Here&rsquo;s <code>bool</code>:</p>

<pre><code>impl&lt;S: Serializer&lt;E&gt;, E&gt; Serialize&lt;S, E&gt; for bool {
    #[inline]
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        s.serialize_bool(*self)
    }
}
</code></pre>

<p><code>Vec&lt;T&gt;</code>:</p>

<pre><code class="rust">impl&lt;
    S: Serializer&lt;E&gt;,
    E,
    T: Serialize&lt;S, E&gt;
&gt; Serialize&lt;S, E&gt; for Vec&lt;T&gt; {
    #[inline]
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        s.serialize_seq(self.iter())
    }
}

pub trait Serializer&lt;E&gt; {
    ...

    fn serialize_seq&lt;
        T: Serialize&lt;AssertSerializer&lt;Iter&gt;, Error&gt;,
        SeqIter: Iterator&lt;T&gt;
    &gt;(&amp;mut self, mut iter: SeqIter) -&gt; Result&lt;(), Error&gt; {
        let (len, _) = iter.size_hint();
        try!(self.serialize(Token::SeqStart(len)));
        for elt in iter {
            try!(elt.serialize(self));
        }
        self.serialize(Token::SeqEnd)
    }

    ...
}
</code></pre>

<p>And structs:</p>

<pre><code class="rust">struct Foo {
    a: (),
    b: uint,
    c: TreeMap&lt;String, Option&lt;char&gt;&gt;,
}

impl&lt;
  S: Serializer&lt;E&gt;,
  E
&gt; Serialize&lt;S, E&gt; for Foo {
    fn serialize(&amp;self, s: &amp;mut S) -&gt; Result&lt;(), E&gt; {
        try!(s.serialize_struct_start("Foo", 2u));
        try!(s.serialize_struct_elt("a", &amp;self.a));
        try!(s.serialize_struct_elt("b", &amp;self.b));
        try!(s.serialize_struct_elt("c", &amp;self.c));
        s.serialize_struct_end()
    }
}
</code></pre>

<p>Much simpler than deserialization.</p>

<h2>Performance</h2>

<p>So how does it perform? Here&rsquo;s the serialization benchmarks, with yet another
ordering. This time sorted by the performance:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library         </th>
<th> format                 </th>
<th> serialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (unpacked) </td>
<td> 4349                 </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto            </td>
<td> 3824.20              </td>
</tr>
<tr>
<td> Rust     </td>
<td> bincode         </td>
<td> Binary                 </td>
<td> 1020                 </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf    </td>
<td> Protocol Buffers       </td>
<td> 596.78               </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (packed)   </td>
<td> 583                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-msgpack    </td>
<td> MessagePack            </td>
<td> 397                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-protobuf   </td>
<td> Protocol Buffers       </td>
<td> 357                  </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON                   </td>
<td> 304                  </td>
</tr>
<tr>
<td> <strong>Rust</strong> </td>
<td> <strong>serde::json</strong> </td>
<td> <strong>JSON</strong>               </td>
<td> <strong>222</strong>              </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf      </td>
<td> Protocol Buffers       </td>
<td> 214.68               </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson          </td>
<td> JSON                   </td>
<td> 147.37               </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json </td>
<td> JSON                   </td>
<td> 147                  </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json   </td>
<td> JSON                   </td>
<td> 80.49                </td>
</tr>
</tbody>
</table>


<p><code>serde::json</code> is doing pretty good! It still has got a ways to go to catch up
to <a href="https://github.com/miloyip/rapidjson">rapidjson</a>, but it&rsquo;s pretty cool it&rsquo;s
beating <a href="https://github.com/golang/protobuf">goprotobuf</a> out of the box :)</p>

<p>Here are the deserialization numbers:</p>

<table>
<thead>
<tr>
<th> language </th>
<th> library         </th>
<th> format                  </th>
<th> deserialization (MB/s) </th>
</tr>
</thead>
<tbody>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (unpacked)  </td>
<td> 2185                   </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto (zero copy) </td>
<td> 1407.95                </td>
</tr>
<tr>
<td> Go       </td>
<td> go-capnproto    </td>
<td> Cap&#8217;n Proto             </td>
<td> 711.77                 </td>
</tr>
<tr>
<td> Rust     </td>
<td> capnproto-rust  </td>
<td> Cap&#8217;n Proto (packed)    </td>
<td> 351                    </td>
</tr>
<tr>
<td> Go       </td>
<td> gogoprotobuf    </td>
<td> Protocol Buffers        </td>
<td> 272.68                 </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON (sax)              </td>
<td> 189                    </td>
</tr>
<tr>
<td> C++      </td>
<td> rapidjson       </td>
<td> JSON (dom)              </td>
<td> 162                    </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-msgpack    </td>
<td> MessagePack             </td>
<td> 138                    </td>
</tr>
<tr>
<td> Rust     </td>
<td> rust-protobuf   </td>
<td> Protocol Buffers        </td>
<td> 129                    </td>
</tr>
<tr>
<td> Go       </td>
<td> ffjson          </td>
<td> JSON                    </td>
<td> 95.06                  </td>
</tr>
<tr>
<td> Rust     </td>
<td> bincode         </td>
<td> Binary                  </td>
<td> 80                     </td>
</tr>
<tr>
<td> Go       </td>
<td> goprotobuf      </td>
<td> Protocol Buffers        </td>
<td> 79.78                  </td>
</tr>
<tr>
<td> <strong>Rust</strong> </td>
<td> <strong>serde::json</strong> </td>
<td> <strong>JSON</strong>                </td>
<td> <strong>67</strong>                 </td>
</tr>
<tr>
<td> Rust     </td>
<td> serialize::json </td>
<td> JSON                    </td>
<td> 24                     </td>
</tr>
<tr>
<td> Go       </td>
<td> encoding/json   </td>
<td> JSON                    </td>
<td> 22.79                  </td>
</tr>
</tbody>
</table>


<p>Well on the plus side, <code>serde::json</code> nearly 3 times faster than
<code>libserialize::json</code>. On the downside rapidjson is nearly 3 times faster than
us in it&rsquo;s SAX style parsing. Even the newly added deserialization support in
<a href="https://github.com/pquerna/ffjson">ffjson</a> is 1.4 times faster than us. So we
got more work cut out for us!</p>

<p>Next time, serde2!</p>

<p>PS: I&rsquo;m definitely getting close to the end of my story, and while I have some
better numbers with serde2, nothing is quite putting me in the rapidjson
range. Anyone want to help optimize
<a href="https://github.com/erickt/rust-serde">serde</a>? I would greatly appreciate the help!</p>

<p>PPS: I&rsquo;ve gotten a number of requests for my
<a href="https://github.com/erickt/rust-serialization-benchmarks">serialization benchmarks</a>
to be ported over to other languages and libraries. Especially a C++ version
of Cap&#8217;n Proto. Unfortunately I don&rsquo;t really have the time to do it myself.
Would anyone be up for helping to implement it?</p>

<p>comments on <a href="https://www.reddit.com/r/rust/comments/2p85za/rewriting_rust_serialization_part_3_introducing/">reddit</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Rust to Make a Safer Interface for Yahoo's Fast MDBM Database]]></title>
    <link href="http://erickt.github.io/blog/2014/12/13/rust-and-mdbm/"/>
    <updated>2014-12-13T11:09:19-08:00</updated>
    <id>http://erickt.github.io/blog/2014/12/13/rust-and-mdbm</id>
    <content type="html"><![CDATA[<p>I&rsquo;m really supposed to be working on my
<a href="http://erickt.github.io/blog/categories/serialization/">serialization series</a>,
 but I just saw a neat new library that was open sourced by Yahoo a couple days
ago called <a href="https://github.com/yahoo/mdbm">MDBM</a> on
<a href="https://news.ycombinator.com/item?id=8732891">Hacker News</a>. I know nothing
about the library, but there are some really neat claims:</p>

<ol>
<li>It&rsquo;s supposed to be fast, and that&rsquo;s always nice.</li>
<li>It&rsquo;s supposed to have a really slim interface.</li>
<li>It&rsquo;s so fast because it&rsquo;s passing around naked pointers to mmapped files,
which is terribly unsafe. Unless you got rust which can prove that those
pointers won&rsquo;t escape :)</li>
</ol>


<p>So I wanted to see how easy it&rsquo;d be to make a Rust binding for the project.
If you want to follow along, first make sure you have
 <a href="http://www.rust-lang.org/install.html">rust installed</a>. Unfortunately it
looks like MDBM only supports Linux and FreeBSD, so I had to build out a Fedora
VM to test this out on. I <em>think</em> this is all you need to build it:</p>

<pre><code>% git clone https://github.com/yahoo/mdbm
% cd mdbm/redhat
% make
% rpm -Uvh ~/rpmbuild/RPMS/x86_64/mdbm-4.11.1-1.fc21.x86_64.rpm
% rpm -Uvh ~/rpmbuild/RPMS/x86_64/mdbm-devel-4.11.1-1.fc21.x86_64.rpm
</code></pre>

<p>Unfortunately it&rsquo;s only for linux, and I got a mac, but it turns out there&rsquo;s
plenty I can do to prep while VirtualBox and Fedora 21 download. Lets start out
by creating our project with <a href="https://crates.io">cargo</a>:</p>

<pre><code>% cargo new mdbm
% cd rust-mdbm
</code></pre>

<p>(Right now there&rsquo;s no way to have the name be different than the path, so edit
<code>Cargo.toml</code> to rename the project to <code>mdbm</code>. I filed
<a href="https://github.com/rust-lang/cargo/issues/1030">#1030</a> to get that
implemented).</p>

<p>By convention, we put bindgen packages into <code>$name-sys</code>, so make that crate as
well:</p>

<pre><code>% cargo new --no-git mdbm-sys
% cd mdbm-sys
</code></pre>

<p>We&rsquo;ve got a really cool tool called
<a href="https://github.com/crabtw/rust-bindgen">bindgen</a>, which uses clang to parse
header files and convert them into an unsafe rust interface. So lets check out
MDBM, and generate a crate to wrap it up in.</p>

<pre><code>% cd ../..
% git clone git@github.com:crabtw/rust-bindgen.git
% cd rust-bindgen
% cargo build
% cd ..
% git clone git@github.com:yahoo/mdbm.git
% cd rust-mdbm/mdbm-sys
% DYLD_LIBRARY_PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib \
  ~/rust/rust-bindgen/target/bindgen \
  -lmdbm \
  -o src/lib.rs \
  mdbm/include/mdbm.h
</code></pre>

<p>Pretty magical. Make sure it builds:</p>

<pre><code>% cargo build
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3:21: 3:25 error: failed to resolve. Maybe a missing `extern crate libc`?
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3 pub type __int8_t = ::libc::c_char;
                                                                  ^~~~
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3:21: 3:35 error: use of undeclared type name `libc::c_char`
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:3 pub type __int8_t = ::libc::c_char;
                                                                  ^~~~~~~~~~~~~~
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:4:22: 4:26 error: failed to resolve. Maybe a missing `extern crate libc`?
/Users/erickt/rust-mdbm/mdbm-sys/src/lib.rs:4 pub type __uint8_t = ::libc::c_uchar;
                                                                   ^~~~
...
</code></pre>

<p>Nope! The problem is that we don&rsquo;t have the <code>libc</code> crate imported. We don&rsquo;t
have a convention yet for this, but I like to do is:</p>

<pre><code>mv src/lib.rs src/ffi.rs
</code></pre>

<p>And create a <code>src/lib.rs</code> that contains:</p>

<pre><code class="rust">#![allow(non_camel_case_types)]
#![allow(non_snake_case)]

extern crate libc;

type __builtin_va_list = libc::c_void;

include!("ffi.rs")
</code></pre>

<p>This lets me run bindgen later on without mucking up the library. This now
compiles. Next up is our high level interface. Add <code>mdbm-sys</code> to our high level
interface by adding this to the <code>rust-mdbm/Cargo.toml</code> file:</p>

<pre><code class="toml">[dependencies.mdbm-sys]
path = "mdbm-sys"
</code></pre>

<p>By now I got my VirtualBox setup working, so now to the actual code! Lets start
with a barebones wrapper around the database:</p>

<pre><code class="rust">pub struct MDBM {
    db: *mut mdbm_sys::MDBM,
}
</code></pre>

<p>Next is the constructor and destructor. I&rsquo;m hardcoding things for now and using
IoError, since MDBM appears to log everything to the ERRNO:</p>

<pre><code>impl MDBM {
    pub fn new(
        path: &amp;Path,
        flags: uint,
        mode: uint,
        psize: uint,
        presize: uint
    ) -&gt; Result&lt;MDBM, IoError&gt; {
        unsafe {
            let path = path.to_c_str();
            let db = mdbm_sys::mdbm_open(
                path.as_ptr(),
                flags as libc::c_int,
                mode as libc::c_int,
                psize as libc::c_int,
                presize as libc::c_int);

            if db.is_null() {
                Err(IoError::last_error())
            } else {
                Ok(MDBM { db: db })
            }
        }
    }

    ...
}

impl Drop for MDBM {
    fn drop(&amp;mut self) {
        unsafe {
            mdbm_sys::mdbm_sync(self.db);
            mdbm_sys::mdbm_close(self.db);
        }
    }
}
</code></pre>

<p>Pretty straightforward translation of the examples with some hardcoded values
to start out. Next up is a wrapper around MDBM&rsquo;s <code>datum</code> type, which is the
type used for both keys and values. <code>datum</code> is just a simple struct containing
a pointer and length, pretty much analogous to our <code>&amp;[u8]</code> slices. However our
slices are much more powerful because our type system can guarantee that in
safe Rust, these slices can never outlive where they are derived from:</p>

<pre><code class="rust">pub struct Datum&lt;'a&gt; {
    bytes: &amp;'a [u8],
}

impl&lt;'a&gt; Datum&lt;'a&gt; {
    pub fn new(bytes: &amp;[u8]) -&gt; Datum {
        Datum { bytes: bytes }
    }
}

fn to_raw_datum(datum: &amp;Datum) -&gt; mdbm_sys::datum {
    mdbm_sys::datum {
        dptr: datum.bytes.as_ptr() as *mut _,
        dsize: datum.bytes.len() as libc::c_int,
    }
}
</code></pre>

<p>And for convenience, lets add a <code>AsDatum</code> conversion method:</p>

<pre><code class="rust">pub trait AsDatum for Sized? {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt;;
}

impl&lt;'a, Sized? T: AsDatum&gt; AsDatum for &amp;'a T {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; { (**self).as_datum() }
}

impl AsDatum for [u8] {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; {
        Datum::new(self)
    }
}

impl AsDatum for str {
    fn as_datum&lt;'a&gt;(&amp;'a self) -&gt; Datum&lt;'a&gt; {
        self.as_bytes().as_datum()
    }
}
</code></pre>

<p>And finally, we got setting and getting a key-value. Setting is pretty
straightforward. The only fun thing is using the <code>AsDatum</code> constraints so we
can do <code>db.set(&amp;"foo", &amp;"bar", 0)</code> instead of
<code>db.set(Datum::new(&amp;"foo".as_slice()), Datum::new("bar".as_slice()), 0)</code>.
we&rsquo;re copying into the database, we don&rsquo;t have to worry about lifetimes yet:</p>

<pre><code class="rust">impl MDBM {
    ...

    /// Set a key.
    pub fn set&lt;K, V&gt;(&amp;self, key: &amp;K, value: &amp;V, flags: int) -&gt; Result&lt;(), IoError&gt; where
        K: AsDatum,
        V: AsDatum,
    {
        unsafe {
            let rc = mdbm_sys::mdbm_store(
                self.db,
                to_raw_datum(&amp;key.as_datum()),
                to_raw_datum(&amp;value.as_datum()),
                flags as libc::c_int);

            if rc == -1 {
                Err(IoError::last_error())
            } else {
                Ok(())
            }
        }
    }

    ...
</code></pre>

<p>MDBM requires the database to be locked in order to get the keys. This os
where things get fun in order to prevent those interior pointers from escaping.
We&rsquo;ll create another wrapper type that manages the lock, and uses RAII to
unlock when we&rsquo;re done. We tie the lifetime of the <code>Lock</code> to the lifetime of
the database and key, which prevents it from outliving either object:</p>

<pre><code class="rust">impl MDBM {
    ...

    /// Lock a key.
    pub fn lock&lt;'a, K&gt;(&amp;'a self, key: &amp;'a K, flags: int) -&gt; Result&lt;Lock&lt;'a&gt;, IoError&gt; where
        K: AsDatum,
    {
        let rc = unsafe {
            mdbm_sys::mdbm_lock_smart(
                self.db,
                &amp;to_raw_datum(&amp;key.as_datum()),
                flags as libc::c_int)
        };

        if rc == 1 {
            Ok(Lock { db: self, key: key.as_datum() })
        } else {
            Err(IoError::last_error())
        }
    }

    ...
}

pub struct Lock&lt;'a&gt; {
    db: &amp;'a MDBM,
    key: Datum&lt;'a&gt;,
}

#[unsafe_destructor]
impl&lt;'a&gt; Drop for Lock&lt;'a&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            let rc = mdbm_sys::mdbm_unlock_smart(
                self.db.db,
                &amp;to_raw_datum(&amp;self.key),
                0);

            assert_eq!(rc, 1);
        }
    }
}
</code></pre>

<p>(Note that I&rsquo;ve heard <code>#[unsafe_destrutor]</code> as used here may become unnecessary
in 1.0).</p>

<p>Finally, let&rsquo;s get our value! Assuming the value exists, we tie the lifetime of
the <code>Lock</code> to the lifetime of the returned <code>&amp;[u8]</code>:</p>

<pre><code class="rust">impl&lt;'a&gt; Lock&lt;'a&gt; {
    /// Fetch a key.
    pub fn get&lt;'a&gt;(&amp;'a self) -&gt; Option&lt;&amp;'a [u8]&gt; {
        unsafe {
            let value = mdbm_sys::mdbm_fetch(
                self.db.db,
                to_raw_datum(&amp;self.key));

            if value.dptr.is_null() {
                None
            } else {
                // we want to constrain the ptr to our lifetime.
                let ptr: &amp;*const u8 = mem::transmute(&amp;value.dptr);
                Some(slice::from_raw_buf(ptr, value.dsize as uint))
            }
        }
    }
}
</code></pre>

<p>Now to verify it works:</p>

<pre><code class="rust">#[test]
fn test() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    db.set(&amp;"hello", &amp;"world", 0).unwrap();

    {
        // key needs to be an lvalue so the lock can hold a reference to
        // it.
        let key = "hello";

        // Lock the key. RAII will unlock it when we exit this scope.
        let value = db.lock(&amp;key, 0).unwrap();

        // Convert the value into a string. The lock is still live at this
        // point.
        let value = str::from_utf8(value.get().unwrap()).unwrap();
        assert_eq!(value, "world");
        println!("hello: {}", value);
    }
}
</code></pre>

<p>Which when run with <code>cargo test</code>, produces:</p>

<pre><code>   Compiling rust-mdbm v0.0.1 (file:///home/erickt/Projects/rust-mdbm)
     Running target/rust-mdbm-98b81ab156dc1e5f

running 1 test
test tests::test_set_get ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured

   Doc-tests rust-mdbm

running 0 tests

test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured
</code></pre>

<p>Next we want to make sure invalid behavior is a compile-time error. First, make
sure we don&rsquo;t leak the keys:</p>

<pre><code class="rust">#[test]
fn test_keys_cannot_escape() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    db.set(&amp;"hello", &amp;"world", 0).unwrap();

    let _ = {
        let key = vec![1];
        db.lock(&amp;key.as_slice(), 0).unwrap()
    };
}
</code></pre>

<p>Which errors with:</p>

<pre><code>src/lib.rs:217:22: 217:25 error: `key` does not live long enough
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
                                    ^~~
src/lib.rs:215:17: 218:10 note: reference must be valid for the expression at 215:16...
src/lib.rs:215         let _ = {
src/lib.rs:216             let key = vec![1];
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
src/lib.rs:218         };
src/lib.rs:215:17: 218:10 note: ...but borrowed value is only valid for the block at 215:16
src/lib.rs:215         let _ = {
src/lib.rs:216             let key = vec![1];
src/lib.rs:217             db.lock(&amp;key.as_slice(), 0).unwrap()
src/lib.rs:218         };
error: aborting due to previous error
</code></pre>

<p>And confirm the value doesn&rsquo;t leak either:</p>

<pre><code class="rust">#[test]
fn test_values_cannot_escape() {
    let db = MDBM::new(
        &amp;Path::new("test.db"),
        super::MDBM_O_RDWR | super::MDBM_O_CREAT,
        0o644,
        0,
        0
    ).unwrap();

    let _ = {
        db.set(&amp;"hello", &amp;"world", 0).unwrap();

        let key = "hello";
        let value = db.lock(&amp;key, 0).unwrap();
        str::from_utf8(value.get().unwrap()).unwrap()
    };
}
</code></pre>

<p>Which errors with:</p>

<pre><code>src/lib.rs:237:34: 237:37 error: `key` does not live long enough
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
                                                ^~~
src/lib.rs:233:17: 239:10 note: reference must be valid for the expression at 233:16...
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:233:17: 239:10 note: ...but borrowed value is only valid for the block at 233:16
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:238:28: 238:33 error: `value` does not live long enough
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
                                          ^~~~~
src/lib.rs:233:17: 239:10 note: reference must be valid for the expression at 233:16...
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
               ...
src/lib.rs:233:17: 239:10 note: ...but borrowed value is only valid for the block at 233:16
src/lib.rs:233         let _ = {
src/lib.rs:234             db.set(&amp;"hello", &amp;"world", 0).unwrap();
src/lib.rs:235
src/lib.rs:236             let key = "hello";
src/lib.rs:237             let value = db.lock(&amp;key, 0).unwrap();
src/lib.rs:238             str::from_utf8(value.get().unwrap()).unwrap()
                                                     ...
</code></pre>

<p>Success! Not too bad for 2 hours of work. Baring bugs, this <code>mdbm</code>
library should perform at roughly the same speed as the C library, but
eliminate many very painful bug opportunities that require tools like Valgrind
to debug.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking Is Confusing in Low Level Rust]]></title>
    <link href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/"/>
    <updated>2014-11-22T12:10:29-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing</id>
    <content type="html"><![CDATA[<p>Edit: you can find all the code in this post
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">here</a>,
and I filed <a href="https://github.com/rust-lang/rust/issues/19281">19281</a> for the
regression I mentioned at the end of the post.</p>

<hr />

<p>Low level benchmarking is confusing and non-intuitive.</p>

<p>The end.</p>

<hr />

<p>Or not. Whatever. So I&rsquo;m trying to get my
implement-<code>Reader</code>-and-<code>Writer</code>-for-<code>&amp;[u8]</code> type PR
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a> landed. But
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63925495">Steven Fackler</a>
obnixously and correctly pointed out that this won&rsquo;t play that nicely with the
new <code>Reader</code> and <code>Writer</code> implementation for <code>Vec&lt;u8&gt;</code>. Grumble grumble. And then
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63927659">Alex Crichton</a>
had the gall to mention that a <code>Writer</code> for <code>mut &amp;[u8]</code> also probably won&rsquo;t be
that common either. Sure, he&rsquo;s write and all, but but I got it working without
needing an index! That means that the <code>&amp;mut [u8]</code> <code>Writer</code> only needs 2
pointers instead of <code>BufWriter</code>&rsquo;s three, so it just has to be faster! Well,
doesn&rsquo;t it?</p>

<p>Stupid benchmarks.</p>

<p>I got to say it&rsquo;s pretty addicting writing micro-benchmarks. It&rsquo;s a lot of fun
seeing how sensitive low-level code can be to just the smallest of tweaks. It&rsquo;s
also really annoying when you write something you think is pretty neat, then
you find it&rsquo;s chock-full of false dependencies between cache lines, or other
mean things CPUs like to impose on poor programmers.</p>

<p>Anyway, to start lets look at what should be the fastest way to write to a
buffer. Completely unsafely with no checks.</p>

<pre><code class="rust">unsafe fn do_copy_nonoverlapping_memory(
  mut dst: *mut u8,
  src: *const u8,
  len: uint,
  batches: uint
) {
    for _ in range(0, batches) {
        ptr::copy_nonoverlapping_memory(dst, src, len);
        dst = dst.offset(len as int);
    }
}

#[test]
fn test_copy_nonoverlapping_memory() {
    let dst = &amp;mut [0_u8, .. BATCHES * SRC_LEN];
    let src = &amp;[1, .. SRC_LEN];

    unsafe {
        do_copy_nonoverlapping_memory(
            dst.as_mut_ptr(),
            src.as_ptr(),
            src.len(),
            BATCHES
        );
    }
    assert!(dst.iter().all(|c| *c == 1));
}
</code></pre>

<p>With <code>SRC_LEN=4</code> and <code>BATCHES=128</code>, we get this. For fun I added the new
<code>libtest</code> from <a href="https://github.com/rust-lang/rust/pull/19233">#19233</a> that will
hopefully land soon. I also added also ran variations that explicitly inlined
and not inlined the inner function:</p>

<pre><code>test bench_copy_nonoverlapping_memory               ... bench: 50 |   [-***#**------]                        | 200:        72 ns/iter (+/- 45)
test bench_copy_nonoverlapping_memory_inline_always ... bench: 50 |       [---***#****************----------]| 100:        65 ns/iter (+/- 39)
test bench_copy_nonoverlapping_memory_inline_never  ... bench: 500 |      [------********#*********-------] | 1000:       747 ns/iter (+/- 393)
</code></pre>

<p>So overall it does quite well. Now lets compare with the code I wrote:</p>

<pre><code class="rust">impl&lt;'a&gt; Writer for &amp;'a mut [u8] {
    #[inline]
    fn write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        if self.is_empty() { return Err(io::standard_error(io::EndOfFile)); }

        let dst_len = self.len();
        let src_len = src.len();

        let write_len = min(dst_len, src_len);

        slice::bytes::copy_memory(*self, src.slice_to(write_len));

        unsafe {
            *self = mem::transmute(raw::Slice {
                data: self.as_ptr().offset(write_len as int),
                len: dst_len - write_len,
            });
        }

        if src_len &gt; dst_len {
            Err(io::standard_error(io::ShortWrite(write_len)))
        } else {
            Ok(())
        }
    }
}
</code></pre>

<p>And. Well. It didn&rsquo;t do that well.</p>

<pre><code>test writer::bench_slice_writer                             ... bench: 500 |   [------**#**--]                      | 2000:       920 ns/iter (+/- 448)
test writer::bench_slice_writer_inline_always               ... bench: 600 | [-**#*****---]                         | 2000:       711 ns/iter (+/- 405)
test writer::bench_slice_writer_inline_never                ... bench: 600 |   [-***#******---]                     | 2000:       838 ns/iter (+/- 474)
</code></pre>

<p>Wow. That&rsquo;s pretty bad compared to the ideal.</p>

<p>Crud. So not only did I add an implementation that&rsquo;s probably going to not work
with <code>write!</code> and now it turns out the performance is pretty terrible. Inlining
isn&rsquo;t helping like it did in the unsafe case. So how&rsquo;s
<a href="https://github.com/rust-lang/rust/blob/master/src/libstd/io/mem.rs#L219">std::io::BufWriter</a>
compare?</p>

<pre><code class="rust">pub struct BufWriter&lt;'a&gt; {
    buf: &amp;'a mut [u8],
    pos: uint
}

impl&lt;'a&gt; Writer for BufWriter&lt;'a&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        // return an error if the entire write does not fit in the buffer
        let cap = if self.pos &gt;= self.buf.len() { 0 } else { self.buf.len() - self.pos };
        if buf.len() &gt; cap {
            return Err(IoError {
                kind: io::OtherIoError,
                desc: "Trying to write past end of buffer",
                detail: None
            })
        }

        slice::bytes::copy_memory(self.buf[mut self.pos..], buf);
        self.pos += buf.len();
        Ok(())
    }
}
</code></pre>

<p>Here&rsquo;s how it does:</p>

<pre><code>test writer::bench_std_buf_writer                           ... bench: 50 |     [------**************#******-------] | 100:        79 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_always             ... bench: 50 |    [-----************#*********-------]  | 100:        75 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_never              ... bench: 600 |   [#****-----]                         | 2000:       705 ns/iter (+/- 337)
</code></pre>

<p>That&rsquo;s just cruel. The optimization gods obviously hate me. So I started
playing with a lot of
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">variations</a>
(it&rsquo;s my yeah yeah it&rsquo;s my serialization benchmark suite, I&rsquo;m planning on
making it more general purpose. Besides it&rsquo;s my suite and I can do whatever I
want with it, so there):</p>

<ul>
<li>(BufWriter0): Turning this <code>Writer</code> into a struct wrapper shouldn&rsquo;t do anything, and it
didn&rsquo;t.</li>
<li>(BufWriter1): There&rsquo;s error handling, does removing it help? Nope!</li>
<li>(BufWriter5): There&rsquo;s an implied branch in <code>let write_len = min(dst_len, src_len)</code>. We can
turn that into the branch-predictor-friendly:</li>
</ul>


<pre><code class="rust">let x = (dst_len &lt; buf_len) as uint;
let write_len = dst_len * x + src_len * (1 - x);
</code></pre>

<p>Doesn&rsquo;t matter, still performs the same.</p>

<ul>
<li>(BufWriter2): Fine then, optimization gods! Lets remove the branch altogether and just
  always advance the slice <code>src.len()</code> bytes! Damn the safety! That, of course,
works. I can hear them giggle.</li>
<li>(BufWriter3): Maybe, just maybe there&rsquo;s something weird going on with
  inlining across crates? Lets copy <code>std::io::BufWriter</code> and make sure that
  it&rsquo;s still nearly optimal. It still is.</li>
<li>(BufWriter6): Technically the <code>min(dst_len, src_len)</code> is a bounds check, so
we could switch from the bounds checked <code>std.slice::bytes::copy_memory</code> to
  the unsafe <code>std::ptr::copy_nonoverlapping_memory</code>, but that also doesn&rsquo;t
  help.</li>
<li>(BufWriter7): Might as well and apply the last trick to <code>std::io::BufWriter</code>,
  and it does shave a couple nanoseconds off. It might be worth pushing it
  upstream:</li>
</ul>


<pre><code>test writer::bench_buf_writer_7                             ... bench: 50 |   [-*#********--------------------------]| 100:        55 ns/iter (+/- 44)
test writer::bench_buf_writer_7_inline_always               ... bench: 50 |     [---------********#*********-------] | 100:        76 ns/iter (+/- 39)
test writer::bench_buf_writer_7_inline_never                ... bench: 600 |   [-***#****----]                      | 2000:       828 ns/iter (+/- 417)
</code></pre>

<ul>
<li>(BufWriter4): While I&rsquo;m using one less <code>uint</code> than <code>std::io::BufWriter</code>, I&rsquo;m
  doing two writes to advance my slice, one to advance the pointer, and one to
  shrink the length. <code>std::io::BufWriter</code> only has to advance it&rsquo;s <code>pos</code> index.
  But in this case if instead of treating the slice as a <code>(ptr, length)</code>, we
  can convert it into a <code>(start_ptr, end_ptr)</code>, where <code>start_ptr=ptr</code>, and
  <code>end_ptr=ptr+length</code>. This works! Ish:</li>
</ul>


<pre><code>test writer::bench_buf_writer_4                             ... bench: 80 |   [--******#*******-----]                | 200:       109 ns/iter (+/- 59)
test writer::bench_buf_writer_4_inline_always               ... bench: 100 |     [------***#******---]               | 200:       133 ns/iter (+/- 44)
test writer::bench_buf_writer_4_inline_never                ... bench: 500 |      [-----***********#***********----]| 1000:       778 ns/iter (+/- 426)
</code></pre>

<p>I know when I&rsquo;m defeated. Oh well. I guess I can at least update
<code>std::io::BufWriter</code> to support the new error handling approach:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter10&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if self.pos == dst_len { return Err(io::standard_error(io::EndOfFile)); }

        let src_len = src.len();
        let cap = dst_len - self.pos;

        let write_len = min(cap, src_len);

        slice::bytes::copy_memory(self.dst[mut self.pos..], src[..write_len]);

        if src_len &gt; dst_len {
            return Err(io::standard_error(io::ShortWrite(write_len)));
        }

        self.pos += write_len;
        Ok(())
    }
}
</code></pre>

<p>How&rsquo;s it do?</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 600 | [----**#***--]                         | 2000:       841 ns/iter (+/- 413)
test writer::bench_buf_writer_10_inline_always              ... bench: 600 |  [----**#***--]                        | 2000:       872 ns/iter (+/- 387)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--******#**---]                     | 2000:       960 ns/iter (+/- 486)
</code></pre>

<p>Grumble grumble. It turns out that if we tweak the <code>copy_memory</code> line to:</p>

<pre><code class="rust">        slice::bytes::copy_memory(self.dst[mut self.pos..], src);
</code></pre>

<p>It shaves 674 nanoseconds off the run:</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 200 |[---***#************------------]        | 400:       230 ns/iter (+/- 147)
test writer::bench_buf_writer_10_inline_always              ... bench: 200 |   [-----********#*******------]         | 400:       280 ns/iter (+/- 128)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--***#****----]                     | 2000:       885 ns/iter (+/- 475)
</code></pre>

<p>But still no where near where we need to be. That suggests though that always
cutting down the <code>src</code>, which triggers another bounds check has some measurable
impact. So maybe I should only shrink the <code>src</code> slice when we know it needs to
be shrunk?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter11&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst = self.dst[mut self.pos..];
        let dst_len = dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);
            }

                self.pos += src_len;

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);
            }

                self.pos += dst_len;

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>Lets see how it failed this time&hellip;</p>

<pre><code>test writer::bench_buf_writer_11                            ... bench: 60 |      [------********#*********-----]     | 100:        79 ns/iter (+/- 28)
test writer::bench_buf_writer_11_inline_always              ... bench: 60 |[-------******#*************-----------]  | 100:        72 ns/iter (+/- 35)
test writer::bench_buf_writer_11_inline_never               ... bench: 600 |  [--***#***----]                       | 2000:       835 ns/iter (+/- 423)
</code></pre>

<p>No way. That actually worked?! That&rsquo;s awesome! I&rsquo;ll carve that out into another
PR. Maybe it&rsquo;ll work for my original version that doesn&rsquo;t use a <code>pos</code>?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter12&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(src_len as int),
                    len: dst_len - src_len,
                });
            }

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(dst_len as int),
                    len: 0,
                });
            }

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>And yep, just as fast!</p>

<pre><code>test writer::bench_buf_writer_12                            ... bench: 50 |[**#*----------------------------------]   | 80:        51 ns/iter (+/- 26)
test writer::bench_buf_writer_12_inline_always              ... bench: 50 |  [--------**********#********------------]| 90:        69 ns/iter (+/- 36)
test writer::bench_buf_writer_12_inline_never               ... bench: 800 |  [---**#***-]                          | 2000:      1000 ns/iter (+/- 263)
</code></pre>

<p>At this point, both solutions are approximately just as fast as the unsafe
<code>ptr::copy_nonoverlapping_memory</code>! So that&rsquo;s awesome. Now would anyone really
care enough about the extra <code>uint</code>?  There may be a few very specialized cases
where that extra <code>uint</code> could cause a problem, but I&rsquo;m not sure if it&rsquo;s worth
it. What do you all think?</p>

<hr />

<p>I thought that was good, but since I&rsquo;m already here, how&rsquo;s the new <code>Vec&lt;u8&gt;</code>
writer doing? Here&rsquo;s the driver:</p>

<pre><code class="rust">impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_std_vec_writer(b: &amp;mut test::Bencher) {
    let mut dst = Vec::with_capacity(BATCHES * SRC_LEN);
    let src = &amp;[1, .. SRC_LEN];

    b.iter(|| {
        dst.clear();

        do_std_writes(&amp;mut dst, src, BATCHES);
    })
}
</code></pre>

<p>And the results:</p>

<pre><code>test writer::bench_std_vec_writer                           ... bench: 1000 | [----*****#*****--------]             | 2000:      1248 ns/iter (+/- 588)
test writer::bench_std_vec_writer_inline_always             ... bench: 900 |   [----*#***--]                        | 2000:      1125 ns/iter (+/- 282)
test writer::bench_std_vec_writer_inline_never              ... bench: 1000 |  [----***#*****--------]              | 2000:      1227 ns/iter (+/- 516)
</code></pre>

<p>Wow. That&rsquo;s pretty terrible. Something weird must be going on with
<code>Vec::push_all</code>. (Maybe that&rsquo;s what caused my serialization benchmarks to slow
1/3?). Lets skip it:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for VecWriter1&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let src_len = src.len();

        self.dst.reserve(src_len);

        let dst = self.dst.as_mut_slice();

        unsafe {
            // we reserved enough room in `dst` to store `src`.
            ptr::copy_nonoverlapping_memory(
                dst.as_mut_ptr(),
                src.as_ptr(),
                src_len);
        }

        Ok(())
    }
}
</code></pre>

<p>And it looks a bit better, but not perfect:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 100 |         [------*********#*****--------] | 200:       160 ns/iter (+/- 68)
test writer::bench_vec_writer_1_inline_always               ... bench: 100 |     [--------****#**--]                 | 300:       182 ns/iter (+/- 79)
test writer::bench_vec_writer_1_inline_never                ... bench: 600 |   [---****#**--]                       | 2000:       952 ns/iter (+/- 399)
</code></pre>

<p>There&rsquo;s even less going on here than before. The only difference is that
reserve call. Commenting it out gets us back to <code>copy_nonoverlapping_memory</code>
territory:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 70 | [----------------*********#******-------]| 100:        89 ns/iter (+/- 27)
test writer::bench_vec_writer_1_inline_always               ... bench: 50 |   [-***#******--]                        | 200:        75 ns/iter (+/- 46)
test writer::bench_vec_writer_1_inline_never                ... bench: 500 |   [--***#***---]                       | 2000:       775 ns/iter (+/- 433)
</code></pre>

<p>Unfortunately it&rsquo;s getting pretty late, so rather than wait until the next time
to dive into this, I&rsquo;ll leave it up to you all. Does anyone know why <code>reserve</code>
is causing so much trouble here?</p>

<hr />

<p>PS: While I was working on this, I saw
<a href="https://github.com/erickt/rust-serialization-benchmarks/pull/2">stevencheg</a>
submitted a patch to speed up the protocol buffer support. But when I ran the
tests, everything was about 40% slower than the last benchmark
<a href="http://erickt.github.io/blog/2014/11/13/benchmarks-2/">post</a>! Something
happened with Rust&rsquo;s performance over these past couple weeks!</p>

<pre><code>test goser::bincode::bench_decoder                          ... bench:      7682 ns/iter (+/- 3680) = 52 MB/s
test goser::bincode::bench_encoder                          ... bench:       516 ns/iter (+/- 265) = 775 MB/s
test goser::bincode::bench_populate                         ... bench:      1504 ns/iter (+/- 324)
test goser::capnp::bench_deserialize                        ... bench:       251 ns/iter (+/- 140) = 1784 MB/s
test goser::capnp::bench_deserialize_packed_unbuffered      ... bench:      1344 ns/iter (+/- 533) = 250 MB/s
test goser::capnp::bench_populate                           ... bench:       663 ns/iter (+/- 236)
test goser::capnp::bench_serialize                          ... bench:       144 ns/iter (+/- 37) = 3111 MB/s
test goser::capnp::bench_serialize_packed_unbuffered        ... bench:       913 ns/iter (+/- 436) = 369 MB/s
test goser::msgpack::bench_decoder                          ... bench:      3411 ns/iter (+/- 1837) = 84 MB/s
test goser::msgpack::bench_encoder                          ... bench:       961 ns/iter (+/- 477) = 298 MB/s
test goser::msgpack::bench_populate                         ... bench:      1564 ns/iter (+/- 453)
test goser::protobuf::bench_decoder                         ... bench:      3116 ns/iter (+/- 1485) = 91 MB/s
test goser::protobuf::bench_encoder                         ... bench:      1220 ns/iter (+/- 482) = 234 MB/s
test goser::protobuf::bench_populate                        ... bench:       942 ns/iter (+/- 836)
test goser::serialize_json::bench_decoder                   ... bench:     31934 ns/iter (+/- 16186) = 18 MB/s
test goser::serialize_json::bench_encoder                   ... bench:      8481 ns/iter (+/- 3392) = 71 MB/s
test goser::serialize_json::bench_populate                  ... bench:      1471 ns/iter (+/- 426)
</code></pre>
]]></content>
  </entry>
  
</feed>
