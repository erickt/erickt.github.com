<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Benchmarking | Chasing Rabbits]]></title>
  <link href="http://erickt.github.io/blog/categories/benchmarking/atom.xml" rel="self"/>
  <link href="http://erickt.github.io/"/>
  <updated>2015-03-31T22:24:42-07:00</updated>
  <id>http://erickt.github.io/</id>
  <author>
    <name><![CDATA[Erick Tryzelaar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking Is Confusing in Low Level Rust]]></title>
    <link href="http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing/"/>
    <updated>2014-11-22T12:10:29-08:00</updated>
    <id>http://erickt.github.io/blog/2014/11/22/benchmarking-is-confusing</id>
    <content type="html"><![CDATA[<p>Edit: you can find all the code in this post
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">here</a>,
and I filed <a href="https://github.com/rust-lang/rust/issues/19281">19281</a> for the
regression I mentioned at the end of the post.</p>

<hr />

<p>Low level benchmarking is confusing and non-intuitive.</p>

<p>The end.</p>

<hr />

<p>Or not. Whatever. So I&rsquo;m trying to get my
implement-<code>Reader</code>-and-<code>Writer</code>-for-<code>&amp;[u8]</code> type PR
<a href="https://github.com/rust-lang/rust/pull/18980">#18980</a> landed. But
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63925495">Steven Fackler</a>
obnixously and correctly pointed out that this won&rsquo;t play that nicely with the
new <code>Reader</code> and <code>Writer</code> implementation for <code>Vec&lt;u8&gt;</code>. Grumble grumble. And then
<a href="https://github.com/rust-lang/rust/pull/18980#issuecomment-63927659">Alex Crichton</a>
had the gall to mention that a <code>Writer</code> for <code>mut &amp;[u8]</code> also probably won&rsquo;t be
that common either. Sure, he&rsquo;s write and all, but but I got it working without
needing an index! That means that the <code>&amp;mut [u8]</code> <code>Writer</code> only needs 2
pointers instead of <code>BufWriter</code>&rsquo;s three, so it just has to be faster! Well,
doesn&rsquo;t it?</p>

<p>Stupid benchmarks.</p>

<p>I got to say it&rsquo;s pretty addicting writing micro-benchmarks. It&rsquo;s a lot of fun
seeing how sensitive low-level code can be to just the smallest of tweaks. It&rsquo;s
also really annoying when you write something you think is pretty neat, then
you find it&rsquo;s chock-full of false dependencies between cache lines, or other
mean things CPUs like to impose on poor programmers.</p>

<p>Anyway, to start lets look at what should be the fastest way to write to a
buffer. Completely unsafely with no checks.</p>

<pre><code class="rust">unsafe fn do_copy_nonoverlapping_memory(
  mut dst: *mut u8,
  src: *const u8,
  len: uint,
  batches: uint
) {
    for _ in range(0, batches) {
        ptr::copy_nonoverlapping_memory(dst, src, len);
        dst = dst.offset(len as int);
    }
}

#[test]
fn test_copy_nonoverlapping_memory() {
    let dst = &amp;mut [0_u8, .. BATCHES * SRC_LEN];
    let src = &amp;[1, .. SRC_LEN];

    unsafe {
        do_copy_nonoverlapping_memory(
            dst.as_mut_ptr(),
            src.as_ptr(),
            src.len(),
            BATCHES
        );
    }
    assert!(dst.iter().all(|c| *c == 1));
}
</code></pre>

<p>With <code>SRC_LEN=4</code> and <code>BATCHES=128</code>, we get this. For fun I added the new
<code>libtest</code> from <a href="https://github.com/rust-lang/rust/pull/19233">#19233</a> that will
hopefully land soon. I also added also ran variations that explicitly inlined
and not inlined the inner function:</p>

<pre><code>test bench_copy_nonoverlapping_memory               ... bench: 50 |   [-***#**------]                        | 200:        72 ns/iter (+/- 45)
test bench_copy_nonoverlapping_memory_inline_always ... bench: 50 |       [---***#****************----------]| 100:        65 ns/iter (+/- 39)
test bench_copy_nonoverlapping_memory_inline_never  ... bench: 500 |      [------********#*********-------] | 1000:       747 ns/iter (+/- 393)
</code></pre>

<p>So overall it does quite well. Now lets compare with the code I wrote:</p>

<pre><code class="rust">impl&lt;'a&gt; Writer for &amp;'a mut [u8] {
    #[inline]
    fn write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        if self.is_empty() { return Err(io::standard_error(io::EndOfFile)); }

        let dst_len = self.len();
        let src_len = src.len();

        let write_len = min(dst_len, src_len);

        slice::bytes::copy_memory(*self, src.slice_to(write_len));

        unsafe {
            *self = mem::transmute(raw::Slice {
                data: self.as_ptr().offset(write_len as int),
                len: dst_len - write_len,
            });
        }

        if src_len &gt; dst_len {
            Err(io::standard_error(io::ShortWrite(write_len)))
        } else {
            Ok(())
        }
    }
}
</code></pre>

<p>And. Well. It didn&rsquo;t do that well.</p>

<pre><code>test writer::bench_slice_writer                             ... bench: 500 |   [------**#**--]                      | 2000:       920 ns/iter (+/- 448)
test writer::bench_slice_writer_inline_always               ... bench: 600 | [-**#*****---]                         | 2000:       711 ns/iter (+/- 405)
test writer::bench_slice_writer_inline_never                ... bench: 600 |   [-***#******---]                     | 2000:       838 ns/iter (+/- 474)
</code></pre>

<p>Wow. That&rsquo;s pretty bad compared to the ideal.</p>

<p>Crud. So not only did I add an implementation that&rsquo;s probably going to not work
with <code>write!</code> and now it turns out the performance is pretty terrible. Inlining
isn&rsquo;t helping like it did in the unsafe case. So how&rsquo;s
<a href="https://github.com/rust-lang/rust/blob/master/src/libstd/io/mem.rs#L219">std::io::BufWriter</a>
compare?</p>

<pre><code class="rust">pub struct BufWriter&lt;'a&gt; {
    buf: &amp;'a mut [u8],
    pos: uint
}

impl&lt;'a&gt; Writer for BufWriter&lt;'a&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        // return an error if the entire write does not fit in the buffer
        let cap = if self.pos &gt;= self.buf.len() { 0 } else { self.buf.len() - self.pos };
        if buf.len() &gt; cap {
            return Err(IoError {
                kind: io::OtherIoError,
                desc: "Trying to write past end of buffer",
                detail: None
            })
        }

        slice::bytes::copy_memory(self.buf[mut self.pos..], buf);
        self.pos += buf.len();
        Ok(())
    }
}
</code></pre>

<p>Here&rsquo;s how it does:</p>

<pre><code>test writer::bench_std_buf_writer                           ... bench: 50 |     [------**************#******-------] | 100:        79 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_always             ... bench: 50 |    [-----************#*********-------]  | 100:        75 ns/iter (+/- 40)
test writer::bench_std_buf_writer_inline_never              ... bench: 600 |   [#****-----]                         | 2000:       705 ns/iter (+/- 337)
</code></pre>

<p>That&rsquo;s just cruel. The optimization gods obviously hate me. So I started
playing with a lot of
<a href="https://github.com/erickt/rust-serialization-benchmarks/blob/master/rust/src/writer.rs">variations</a>
(it&rsquo;s my yeah yeah it&rsquo;s my serialization benchmark suite, I&rsquo;m planning on
making it more general purpose. Besides it&rsquo;s my suite and I can do whatever I
want with it, so there):</p>

<ul>
<li>(BufWriter0): Turning this <code>Writer</code> into a struct wrapper shouldn&rsquo;t do anything, and it
didn&rsquo;t.</li>
<li>(BufWriter1): There&rsquo;s error handling, does removing it help? Nope!</li>
<li>(BufWriter5): There&rsquo;s an implied branch in <code>let write_len = min(dst_len, src_len)</code>. We can
turn that into the branch-predictor-friendly:</li>
</ul>


<pre><code class="rust">let x = (dst_len &lt; buf_len) as uint;
let write_len = dst_len * x + src_len * (1 - x);
</code></pre>

<p>Doesn&rsquo;t matter, still performs the same.</p>

<ul>
<li>(BufWriter2): Fine then, optimization gods! Lets remove the branch altogether and just
  always advance the slice <code>src.len()</code> bytes! Damn the safety! That, of course,
works. I can hear them giggle.</li>
<li>(BufWriter3): Maybe, just maybe there&rsquo;s something weird going on with
  inlining across crates? Lets copy <code>std::io::BufWriter</code> and make sure that
  it&rsquo;s still nearly optimal. It still is.</li>
<li>(BufWriter6): Technically the <code>min(dst_len, src_len)</code> is a bounds check, so
we could switch from the bounds checked <code>std.slice::bytes::copy_memory</code> to
  the unsafe <code>std::ptr::copy_nonoverlapping_memory</code>, but that also doesn&rsquo;t
  help.</li>
<li>(BufWriter7): Might as well and apply the last trick to <code>std::io::BufWriter</code>,
  and it does shave a couple nanoseconds off. It might be worth pushing it
  upstream:</li>
</ul>


<pre><code>test writer::bench_buf_writer_7                             ... bench: 50 |   [-*#********--------------------------]| 100:        55 ns/iter (+/- 44)
test writer::bench_buf_writer_7_inline_always               ... bench: 50 |     [---------********#*********-------] | 100:        76 ns/iter (+/- 39)
test writer::bench_buf_writer_7_inline_never                ... bench: 600 |   [-***#****----]                      | 2000:       828 ns/iter (+/- 417)
</code></pre>

<ul>
<li>(BufWriter4): While I&rsquo;m using one less <code>uint</code> than <code>std::io::BufWriter</code>, I&rsquo;m
  doing two writes to advance my slice, one to advance the pointer, and one to
  shrink the length. <code>std::io::BufWriter</code> only has to advance it&rsquo;s <code>pos</code> index.
  But in this case if instead of treating the slice as a <code>(ptr, length)</code>, we
  can convert it into a <code>(start_ptr, end_ptr)</code>, where <code>start_ptr=ptr</code>, and
  <code>end_ptr=ptr+length</code>. This works! Ish:</li>
</ul>


<pre><code>test writer::bench_buf_writer_4                             ... bench: 80 |   [--******#*******-----]                | 200:       109 ns/iter (+/- 59)
test writer::bench_buf_writer_4_inline_always               ... bench: 100 |     [------***#******---]               | 200:       133 ns/iter (+/- 44)
test writer::bench_buf_writer_4_inline_never                ... bench: 500 |      [-----***********#***********----]| 1000:       778 ns/iter (+/- 426)
</code></pre>

<p>I know when I&rsquo;m defeated. Oh well. I guess I can at least update
<code>std::io::BufWriter</code> to support the new error handling approach:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter10&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if self.pos == dst_len { return Err(io::standard_error(io::EndOfFile)); }

        let src_len = src.len();
        let cap = dst_len - self.pos;

        let write_len = min(cap, src_len);

        slice::bytes::copy_memory(self.dst[mut self.pos..], src[..write_len]);

        if src_len &gt; dst_len {
            return Err(io::standard_error(io::ShortWrite(write_len)));
        }

        self.pos += write_len;
        Ok(())
    }
}
</code></pre>

<p>How&rsquo;s it do?</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 600 | [----**#***--]                         | 2000:       841 ns/iter (+/- 413)
test writer::bench_buf_writer_10_inline_always              ... bench: 600 |  [----**#***--]                        | 2000:       872 ns/iter (+/- 387)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--******#**---]                     | 2000:       960 ns/iter (+/- 486)
</code></pre>

<p>Grumble grumble. It turns out that if we tweak the <code>copy_memory</code> line to:</p>

<pre><code class="rust">        slice::bytes::copy_memory(self.dst[mut self.pos..], src);
</code></pre>

<p>It shaves 674 nanoseconds off the run:</p>

<pre><code>test writer::bench_buf_writer_10                            ... bench: 200 |[---***#************------------]        | 400:       230 ns/iter (+/- 147)
test writer::bench_buf_writer_10_inline_always              ... bench: 200 |   [-----********#*******------]         | 400:       280 ns/iter (+/- 128)
test writer::bench_buf_writer_10_inline_never               ... bench: 600 |   [--***#****----]                     | 2000:       885 ns/iter (+/- 475)
</code></pre>

<p>But still no where near where we need to be. That suggests though that always
cutting down the <code>src</code>, which triggers another bounds check has some measurable
impact. So maybe I should only shrink the <code>src</code> slice when we know it needs to
be shrunk?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter11&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst = self.dst[mut self.pos..];
        let dst_len = dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);
            }

                self.pos += src_len;

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);
            }

                self.pos += dst_len;

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>Lets see how it failed this time&hellip;</p>

<pre><code>test writer::bench_buf_writer_11                            ... bench: 60 |      [------********#*********-----]     | 100:        79 ns/iter (+/- 28)
test writer::bench_buf_writer_11_inline_always              ... bench: 60 |[-------******#*************-----------]  | 100:        72 ns/iter (+/- 35)
test writer::bench_buf_writer_11_inline_never               ... bench: 600 |  [--***#***----]                       | 2000:       835 ns/iter (+/- 423)
</code></pre>

<p>No way. That actually worked?! That&rsquo;s awesome! I&rsquo;ll carve that out into another
PR. Maybe it&rsquo;ll work for my original version that doesn&rsquo;t use a <code>pos</code>?</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for BufWriter12&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let dst_len = self.dst.len();

        if dst_len == 0 {
            return Err(io::standard_error(io::EndOfFile));
        }

        let src_len = src.len();

        if dst_len &gt;= src_len {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    src_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(src_len as int),
                    len: dst_len - src_len,
                });
            }

            Ok(())
        } else {
            unsafe {
                ptr::copy_nonoverlapping_memory(
                    self.dst.as_mut_ptr(),
                    src.as_ptr(),
                    dst_len);

                self.dst = mem::transmute(raw::Slice {
                    data: self.dst.as_ptr().offset(dst_len as int),
                    len: 0,
                });
            }

            Err(io::standard_error(io::ShortWrite(dst_len)))
        }
    }
}
</code></pre>

<p>And yep, just as fast!</p>

<pre><code>test writer::bench_buf_writer_12                            ... bench: 50 |[**#*----------------------------------]   | 80:        51 ns/iter (+/- 26)
test writer::bench_buf_writer_12_inline_always              ... bench: 50 |  [--------**********#********------------]| 90:        69 ns/iter (+/- 36)
test writer::bench_buf_writer_12_inline_never               ... bench: 800 |  [---**#***-]                          | 2000:      1000 ns/iter (+/- 263)
</code></pre>

<p>At this point, both solutions are approximately just as fast as the unsafe
<code>ptr::copy_nonoverlapping_memory</code>! So that&rsquo;s awesome. Now would anyone really
care enough about the extra <code>uint</code>?  There may be a few very specialized cases
where that extra <code>uint</code> could cause a problem, but I&rsquo;m not sure if it&rsquo;s worth
it. What do you all think?</p>

<hr />

<p>I thought that was good, but since I&rsquo;m already here, how&rsquo;s the new <code>Vec&lt;u8&gt;</code>
writer doing? Here&rsquo;s the driver:</p>

<pre><code class="rust">impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

impl Writer for Vec&lt;u8&gt; {
    #[inline]
    fn write(&amp;mut self, buf: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        self.push_all(buf);
        Ok(())
    }
}

#[bench]
fn bench_std_vec_writer(b: &amp;mut test::Bencher) {
    let mut dst = Vec::with_capacity(BATCHES * SRC_LEN);
    let src = &amp;[1, .. SRC_LEN];

    b.iter(|| {
        dst.clear();

        do_std_writes(&amp;mut dst, src, BATCHES);
    })
}
</code></pre>

<p>And the results:</p>

<pre><code>test writer::bench_std_vec_writer                           ... bench: 1000 | [----*****#*****--------]             | 2000:      1248 ns/iter (+/- 588)
test writer::bench_std_vec_writer_inline_always             ... bench: 900 |   [----*#***--]                        | 2000:      1125 ns/iter (+/- 282)
test writer::bench_std_vec_writer_inline_never              ... bench: 1000 |  [----***#*****--------]              | 2000:      1227 ns/iter (+/- 516)
</code></pre>

<p>Wow. That&rsquo;s pretty terrible. Something weird must be going on with
<code>Vec::push_all</code>. (Maybe that&rsquo;s what caused my serialization benchmarks to slow
1/3?). Lets skip it:</p>

<pre><code class="rust">impl&lt;'a&gt; MyWriter for VecWriter1&lt;'a&gt; {
    #[inline]
    fn my_write(&amp;mut self, src: &amp;[u8]) -&gt; IoResult&lt;()&gt; {
        let src_len = src.len();

        self.dst.reserve(src_len);

        let dst = self.dst.as_mut_slice();

        unsafe {
            // we reserved enough room in `dst` to store `src`.
            ptr::copy_nonoverlapping_memory(
                dst.as_mut_ptr(),
                src.as_ptr(),
                src_len);
        }

        Ok(())
    }
}
</code></pre>

<p>And it looks a bit better, but not perfect:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 100 |         [------*********#*****--------] | 200:       160 ns/iter (+/- 68)
test writer::bench_vec_writer_1_inline_always               ... bench: 100 |     [--------****#**--]                 | 300:       182 ns/iter (+/- 79)
test writer::bench_vec_writer_1_inline_never                ... bench: 600 |   [---****#**--]                       | 2000:       952 ns/iter (+/- 399)
</code></pre>

<p>There&rsquo;s even less going on here than before. The only difference is that
reserve call. Commenting it out gets us back to <code>copy_nonoverlapping_memory</code>
territory:</p>

<pre><code>test writer::bench_vec_writer_1                             ... bench: 70 | [----------------*********#******-------]| 100:        89 ns/iter (+/- 27)
test writer::bench_vec_writer_1_inline_always               ... bench: 50 |   [-***#******--]                        | 200:        75 ns/iter (+/- 46)
test writer::bench_vec_writer_1_inline_never                ... bench: 500 |   [--***#***---]                       | 2000:       775 ns/iter (+/- 433)
</code></pre>

<p>Unfortunately it&rsquo;s getting pretty late, so rather than wait until the next time
to dive into this, I&rsquo;ll leave it up to you all. Does anyone know why <code>reserve</code>
is causing so much trouble here?</p>

<hr />

<p>PS: While I was working on this, I saw
<a href="https://github.com/erickt/rust-serialization-benchmarks/pull/2">stevencheg</a>
submitted a patch to speed up the protocol buffer support. But when I ran the
tests, everything was about 40% slower than the last benchmark
<a href="http://erickt.github.io/blog/2014/11/13/benchmarks-2/">post</a>! Something
happened with Rust&rsquo;s performance over these past couple weeks!</p>

<pre><code>test goser::bincode::bench_decoder                          ... bench:      7682 ns/iter (+/- 3680) = 52 MB/s
test goser::bincode::bench_encoder                          ... bench:       516 ns/iter (+/- 265) = 775 MB/s
test goser::bincode::bench_populate                         ... bench:      1504 ns/iter (+/- 324)
test goser::capnp::bench_deserialize                        ... bench:       251 ns/iter (+/- 140) = 1784 MB/s
test goser::capnp::bench_deserialize_packed_unbuffered      ... bench:      1344 ns/iter (+/- 533) = 250 MB/s
test goser::capnp::bench_populate                           ... bench:       663 ns/iter (+/- 236)
test goser::capnp::bench_serialize                          ... bench:       144 ns/iter (+/- 37) = 3111 MB/s
test goser::capnp::bench_serialize_packed_unbuffered        ... bench:       913 ns/iter (+/- 436) = 369 MB/s
test goser::msgpack::bench_decoder                          ... bench:      3411 ns/iter (+/- 1837) = 84 MB/s
test goser::msgpack::bench_encoder                          ... bench:       961 ns/iter (+/- 477) = 298 MB/s
test goser::msgpack::bench_populate                         ... bench:      1564 ns/iter (+/- 453)
test goser::protobuf::bench_decoder                         ... bench:      3116 ns/iter (+/- 1485) = 91 MB/s
test goser::protobuf::bench_encoder                         ... bench:      1220 ns/iter (+/- 482) = 234 MB/s
test goser::protobuf::bench_populate                        ... bench:       942 ns/iter (+/- 836)
test goser::serialize_json::bench_decoder                   ... bench:     31934 ns/iter (+/- 16186) = 18 MB/s
test goser::serialize_json::bench_encoder                   ... bench:      8481 ns/iter (+/- 3392) = 71 MB/s
test goser::serialize_json::bench_populate                  ... bench:      1471 ns/iter (+/- 426)
</code></pre>
]]></content>
  </entry>
  
</feed>
